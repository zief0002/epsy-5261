# Effect Size {#sec-compare-standard}

In the methods introduced for comparing data to a standard and comparing two groups, *t*- and *z*-tests were presented as methods that a researcher can use to examine differences. But, as @Kirk:2001 points out, the group differences question often expands into three questions: (a) Is an observed effect real or should it be attributed to chance? (b) If the effect is real, how large is it? and (c) Is the effect large enough to be useful? Using the inferential methods introduced thus far, only the first of Kirk's three questions can be answered. To answer the other two questions, which are often more meaningful to applied researchers, the results from hypothesis tests need to be supplemented with additional analysis.

Numerous researchers have argued for the use of *effect sizes* to complement or even replace hypothesis tests [e.g., @Cohen:1990; @Cohen:1994; @Kirk:1995; @Kirk:1996; @Thompson:1996; @Thompson:2007]. Effect size is a term used to describe a family of indices that characterize the *extent to which sample results diverge from the expectations specified in the null hypothesis*. These measures help researchers focus on how meaningful the results from hypothesis tests are by providing answers to Kirk's second two questions and also provide a method by which to compare the results between different studies. Effect sizes have been fully embraced by the American Psychological Association. Its Publication Manual [APA:2019, p. 25] states, "For the reader to fully understand the importance of your findings, it is almost always necessary to include some index of effect size or strength of relationship in your Results section." It further states that the "failure to report effect sizes" may be found by editors to be one of the "defects in the design and reporting of results" (p. 5).

<br />


## References



