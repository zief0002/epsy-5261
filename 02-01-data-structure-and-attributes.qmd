# Data Structure and Attribute Classification {#sec-data-structure}


Consider the following data which constitute a random sample of 25 New York Times bestselling harcover books [@Pruett:2021]. Take a minute to look familiarize yourself with these data.

```{r}
#| echo: false
#| message: false
nyt = readr::read_csv("data/nyt-best-sellers.csv")
DT::datatable(nyt)
```

These data have a tabular structure, that is, it is organized into rows and columns. In well-organized tabular data, the rows represent **cases** (also called **units**) and the columns represent **attributes** (also referred to as **variables**). It is important to identify what the cases and attributes are when you initially look at a new data set.

In these data, there are 25 cases, and each case is a NYT best selling book. That is, each row represents a different NYT best selling book. There are eight attributes in these data. Attributes are the information or characteristics that are collected for each of the cases. The attributes in these data are:

- The year the book was published, 
- The title of the book, 
- The author of the book, 
- Whether the author identifies as a female (0 = no, 1 = yes), 
- The total number of weeks the book was on the NYT Best Sellers list, 
- The first week the book appeared on the NYT Best Sellers list, 
- The book's debut rank on the NYT Best Sellers list, and 
- The book's highest rank while it was on the NYT Best Sellers list.

:::protip
Tabular data are required in order to carry out most analyses with computational tools, although not all data are tabular. For example, many websites store data in non-tabular forms such as [XML](https://www.w3.org/standards/xml/core) or [JSON](https://www.json.org/). In this course, all the data you work with will be tabular. 
:::


Aside from identifying the cases and attributes, here are some other questions about collected data that you should answer [@Gould:2013]:

- What are the units of measurement for each quantitative attribute?
- Who collected the data? 
- How and where were the data collected?
- Why were the data collected?

The answers to these questions give us information about the data's context, which is key to drawing reasonable conclusions and interpreting any statistical analyses. If you are the one collecting the data, you should record all of this information so others have access to it. For the NYT bests sellers data here are the answers to these questions:

- **What are the units of measurement for each quantitative attribute?**
  + `total_weeks`: Number of weeks
  + `first_week`: Date given in month/day/year format
  + `debut_rank` and `best_rank`: Rank on the NYT Best Sellers List (smaller values indicate a higher rank) 
- **Who collected the data?** Jordan Pruett for the Post45 Data Collective. The Post45 Data Collective peer reviews and houses post-1945 literary data on an open-access website designed, hosted, and maintained by Emory Universityâ€™s Center for Digital Scholarship.
- **How and where were the data collected?** These data were scraped from Hawes Publications, an online repository that publishes a PDF transcript of the list for every year of the last going back to 1931, using the open-source Python library pdfminer.
- **Why were the data collected?** As per the [curatorial statement](https://data.post45.org/new-york-times-hardcover-fiction-bestsellers-1931-2020-curatorial-statement/), "[t]his dataset provides valuable metadata for researchers of 20th century American literature working in fields such as cultural analytics, book and publishing history, and the sociology of literature."

:::protip
Often these types of metadata (data about the data) are included in a **data codebook**. If you collect data as part of a study or project, you should create codebook for the data.
:::


<br />


## Classifying Attributes

Our ultimate goal is often to analyze the data we have to learn from it. For example, in our NYT Best Seller data, we may be interested in the proportion of authors that identify as female. Or, we may want to. know how many weeks a book stays on the Best Sellers list. The type of analyses we can do, however, depend on the type of attributes we have.

We typically classify attributes as either **categorical attributes** or **quantitative attributes**. These classifications are based on the type of information (data) in the attribute. A categorical attribute has values that represent categorical (or qualitative) differences between the cases, whereas a quantitative attribute represents numerical (or quantitative) differences between cases. For example, in the NYT Best Seller data, title and author are categorical variables, whereas year, and total number of weeks the book was on the NYT Best Sellers list are quantitative attributes. 

Typically attributes that have numerical values are quantitative, but not always. In our data, consider the attribute that indicates whether the author identifies as a female. Although the values in the data are numeric, these numbers actually represent different categories: 0 = no (not female) and 1 = yes (female). Therefore, this attribute is actually a categorical attribute, not a quantitative attribute.

One check of whether anattribute is actually quantitative is whether numeric computations, such as finding an average of the attribute, can be carried out *and* the result makes conceptual sense. For example, we cannot compute the mean author value (it is thus a categorical attribute). If we compute the mean of the `female` attribute we get a result, but it does not indicate anything about the gender identity of a NYT best selling author. The mean does not make conceptual sense and thus we classify `female` as a categorical attribute.

:::exercises
#### Your Turn {.unnumbered}

Classify the `best_rank` attribute (the book's highest rank while it was on the NYT Best Sellers list) as either categorical or quantitative. Explain.

<button class="solution-btn solution-btn-default" onclick="toggle_visibility('exercise_01');">Show/Hide Solution</button>

<div id="exercise_01" style="display:none; margin: -40px 0 40px 40px;">
The attribute `best_rank` is a quantitative attribute. The data in this attribute are numeric values, and it makes conceptual sense to compute summaries such as the mean for this attribute.
</div>
:::


<br />


### Further Classifications of Attributes

While categorizing each attribute as categorical or quantitative is a good first step, statisticians and data analysts often go a step further and classify attributes based on their **scale of measurement**. This classification is based on how attributes were measured (i.e., how we assign numerals to objects of events) and what this implies about the empirical statements we can make about the constructs measured on that particular scale. The most common taxonomy for this was described by @Stevens:1946 who classified four scales of measurement: nominal, ordinal, interval, or ratio (NOIR). Below we describe each of these scales of measurement:

**Nominal Scale of Measurement:** In nominal scales of measurement, any numerals assigned to different values would only be useful as labels, for distinguishing or organizing values. Most categorical attributes have this scale of measurement. For example, in the NYT bests sellers data, the numerical values in the gender identity attribute (`female`) are only useful as labels and for distinguishing authors who identify as female and authors who don't. Because of this, only the following type of statement would be meaningful:

- In the NYT bestseller data, more authors identify as female (15) than do not identify as female (10).

The only type of empirical statements we can make are comparisons of the number of cases between different labels (e.g., counts, percentages). 

**Ordinal Scale of Measurement:** Data measured using an ordinal scales of measurement, is still categorical. It has all the features of nominal measured data (e.g., labeling, distinguishing, organizing), but we can also rank order the values in a meaningful way. A classic example of the ordinal scale of measurement is in the 5-star review rating used on sites like Amazon. All of these statements would be meaningful:

- There are more 4-star reviews than 5-star reviews (comparison of counts).
- A review of 3 stars is better than a review of 2 stars (rank ordering). 


With this scale of measurement, it is reasonable to not only provide counts of the different values (e.g., the number of 5-star reviews), but now because there is a rank ordering, we can also make empirical statements related to the rank ordering of the measured construct based on the numeral values. In nominal level data, these latter types of statements are not appropriate since the values for the labels are arbitrary. For example, authors with a gender identity of female were assigned a 1 and others were assigned a 0. This is arbitrary in that authors identifying as female could just as easily have been assigned a value of 0 and those that didn't identify as female a value of 1. This implies that even though the numeral 1 is greater than the numeral 0, the attribute values associated with these numerals (identifying as female or not) do not have any meaningful rank order. Because of this, saying something about one identity being greater than or less than another is inappropriate.


**Interval Scale of Measurement:** In interval level data, the rank order of the numbers assigned to attribute values is meaningful, similar to ordinal data. Moreover, the difference between consecutive values represents the same amount of difference on the underlying attribute. For example, consider the Fahrenheit temperature scale. All of these statements would be meaningful:

- There are more 30 degrees F days than 0 degrees F days (comparison of counts).
- A day that is 10 degrees F is warmer than a day that is 9 degrees F (rank ordering).
- The difference in temperature when you are comparing days that are 9 and 10 degrees F is the same as when you are comparing days that are 0 and 1 degrees F. (interval comparison).

The critical component is that on an interval scale, the difference in consecutive numerals has the same level of difference in the construct being measured, regardless of scale location. Again consider our 5-star rating system. The difference between a 1- and 2-star review is not the same as the difference between a 4- and 5-star review. While the numbers themselves have a constant difference, the difference in the amount of the underlying construct (satisfaction, happiness with the product, etc.) does not. With interval level scales we can compute summaries like means, standard deviations, and correlations.

**Ratio Scale of Measurement:** With attributes that have the ratio scale of measurement, the rank ordering of the numbers assigned to attribute values is meaningful, the differences between consecutive numerals indicates the same amount of difference in the underlying construct being measured, and ratio type statements about these differences are also meaningful. For example, the amount of snowfall is an attribute on the ratio scale of measurement. All of these statements would be meaningful:

- There are more days with 0 inches of snowfall than days with 10 inches of snowfall (comparison of counts).
- A day with 8 inches of snowfall got more snow than a day with 4 inches of snowfall (rank ordering).
- The difference in snowfall when you are comparing days with 8 inches of snowfall and 7 inches of snowfall is the same as when you are comparing days with 15 and 16 inches of snow (interval comparison).
- A day with 8 inches of snow got twice the amount of snow as a day with 4 inches of snow (ratio comparison).

Going back to our temperature scale, we cannot make these ratio type statements. For example, a day that is 60 degrees F is not twice as warm as a day that is 30 degrees F. This is because the Fahrenheit scale does not have a "true" zero value. (Zero degrees F does not indicate absence of temperature.) Whereas, in our snowfall attribute, a day with 0 inches of snow does indeed indicate no snow fell on that day.

Aside from the type of empirical statements we can make, the level of measurement also puts limits on the type of statistical analyses that are appropriate. 

```{r}
#| label: tbl-measurement
#| tbl-cap: "The four measurement scales and the types of empirical statements and statistical summaries that are appropriate for each scale."
#| echo: false
library(gt)

tab_measurement = data.frame(
  scale = c("Nominal", "Ordinal", "Interval", "Ratio"),
  statement = c("Comparison of counts", 
                "Comparison of counts <br /> Rank ordering",
                "Comparison of counts <br /> Rank ordering <br /> Interval comparisons",
                "Comparison of counts <br /> Rank ordering <br /> Interval comparisons <br /> Ratio comparisons"),
  stat = c("Counts, percentages", 
           "Counts, percentages <br /> Median, percentiles",
           "Counts, percentages <br /> Median, percentiles <br /> Mean, standard deviation",
           "Counts, percentages <br /> Median, percentiles <br /> Mean, standard deviation <br /> Coefficient of variation")
)

tab_measurement |>
  gt() |>
  cols_label(
    scale = md("*Scale*"),
    statement = md("Empirical Statement"),
    stat = md("Statistical Summaries")
  ) |>
  cols_align(
    columns = c(scale, statement, stat),
    align = "left"
  ) |>
  fmt_markdown(columns = everything())

```

<br />


## How Were the Data Generated?

Another question that has direct implications on the methods used in data analysis is: How were the data generated or collected? Were they collected from a survey? Were they mined from the web? Were they generated as part of an experimental study? Knowing the answer to these questions also is important for the degree to which we can draw conclusions from the analysis. 

Understanding how the data were generated allows us to determine whether the data we have constitute a **sample** of cases or the entire **population** of cases we are interested in learning about. Importantly, whether you have a sample or the entire population depends on how you define the cases/observations you are interested in drawing conclusions about. 

:::fyi
A **population** includes *all* cases/observations of interest, whereas a. **sample** includes a subset of cases from the population.
:::

For example, consider a child psychologist who wants to draw conclusions about all students at a particular school in Minnesota. To do this, she collects data from every student in that school. Since her data includes every case (student) she is interest in drawing conclusions for, her data would be a population, Now consider a second child psychologist who is interested in drawing conclusions about all students in Minnesota. This psychologist also collects data from every student in the same school as the first psychologist. This second psychologist's data would be considered a sample since the cases they included in their data are only a subset of the cases they want to draw conclusions about.

:::exercises
#### Your Turn {.unnumbered}

Is the New York Time best sellers data a population or a sample? Explain. 

<button class="solution-btn solution-btn-default" onclick="toggle_visibility('exercise_02');">Show/Hide Solution</button>

<div id="exercise_02" style="display:none; margin: -40px 0 40px 40px;">
The New York Time best sellers data is a sample since it is only a subset of all the New York Times best selling books. 
</div>
:::

<br />


### Drawing Conclusions from a Sample

In practice, we rarely have data collected from an entire population, but we still want to use the data we have in our sample to draw conclusions about that population. Drawing conclusions about an entire population when you only have data from a subset cases is what statisticians call **statistical inference**. 

```{r}
#| label: fig-pop-sample
#| fig-cap: A sample of data is drawn from the population. Information from the sample is then analyzed and used to make a statistical inference about the population.
#| echo: false
knitr::include_graphics("figs/03-01-population-to-sample.png")
```


This can be a very tricky thing to do since the sample does not give us complete information about the population. As an example, consider if you wanted to figure out the average monthly living expenses for all graduate students at the University of Minnesota. To do this you collect data on the monthly living expenses for the students in your EPSY 5261 class and compute the average monthly living expense based on the data you collected and use that value as a guess for the average monthly living expenses for all graduate students at the University of Minnesota. (Note that the cases in your data (students in your EPSY 5261 class) are a subset of the population you want to draw conclusions about (all graduate students at the University of Minnesota) and thus are a sample.)

:::fyi
Summaries computed from the population are referred to as **parameters** and summaries computed from a sample are referred to as **statistics**. 
:::

In statistical inference the statistics we compute from a sample are estimates for the population parameters that we are interested in. The word "estimate" may have clued you in that the value of a statistic is generally not equal to the value of the parameter. In our example, the average monthly living expenses we computed based on your sample of cases is probably not the same as the average monthly living expenses for all graduate students at the University of Minnesota. This is because our sample only includes data for some (not all) of the cases. 

We don't expect the value of the statistic to be the same as that for the parameter we are trying to estimate, but a key question is: Is the value of the statistic a *reasonable* estimate of the parameter? The answer to this question can sometimes be difficult to answer. What do we mean by reasonable? In statistical analysis, there are a few ways that we consider reasonableness of an estimate. We will explore these below.

<br />


#### Sampling Error: Quantifying the Amount of Uncertainty in our Sample Estimate

One way we consider whether an answer is reasonable is how much uncertainty we have in the estimate from our sample. Imagine if you repeated the study, but this time, you collected data on the monthly living expenses in a different section of EPSY 5261. The average computed from these data would likely be different from the average you computed from your section of EPSY 5261, and therefore your guess for the average monthly living expenses for all graduate students at the University of Minnesota would be different. This is because you would have different cases in your sample. 

:::fyi
When using a sample to infer about a population, our guesses or estimates vary depending on the cases in our sample. This means that when we make inferences there is always some degree of uncertainty in our estimates.
:::

The idea that estimates from samples vary depending on the cases in your sample is well known and is referred to as **sampling error**. In carrying out statistical inference, we not only acknowledge that we have uncertainty in our guess from the sample data, but we also try and quantify how much uncertainty there is in that estimate. For example, do we think that the average monthly living expenses for all graduate students at the University of Minnesota is within a few dollars of our sample estimate? Or do we think that it is within a few hundred dollars of our sample estimate? By providing this estimate of the uncertainty, it lets other people know "how reasonable" our guess might be.

```{r}
#| label: fig-sampling-error
#| fig-cap: Estimates for the mean living expense for all graduate students at the University of Minnesota will vary from sample to sample because of sampling error. In statistical inference this is expected and quantifying the amount of sampling error gives us an indication of how much uncertainty we have in our estimate.
#| echo: false
knitr::include_graphics("figs/03-02-sampling-error.png")
```

<br />


#### Sampling Bias: Does the Sample Represent the Population?

A second way we consider whether an answer is reasonable is to consider whether our sample of cases is representative of the population as a whole. In our example, we are asking the question of whether the students in your section of EPSY 5261 are representative of all graduate students at the University of Minnesota when it comes to living expenses. This is a really difficult question to answer, but generally (unless you have selected your sample randomly from the population), your sample is not representative. The key here is that the sampling method (how you chose your cases) matters!

:::fyi
When a sample is not randomly selected from the population we say that the sampling method is biased.
:::

A biased sampling method leads to systematically wrong answers. For example, again say you were interested in determining the average monthly living expenses for all graduate students at the University of Minnesota. This time, your sampling method is to collect data about the monthly living expenses from all the graduate students who live in a particular apartment building in Downtown Minneapolis. Would these students' living expenses be representative of all graduate students at the University of Minnesota?

Again, probably not. The living expenses in Downtown Minneapolis are different (generally higher) than the living expenses of students who live in Dinkytown or one of the suburbs. Because the cases in your sample all come from the same apartment building in Downtown Minneapolis, their average monthly living expense will be systematically higher than the average  monthly living expenses for all graduate students at the University of Minnesota. 

What about our original sampling method of collecting data from each of the graduate students in your EPSY 5261 section? While these students might live in different areas, and seem more representative, this sampling method is likely still biased. Even if we have a hard time identifying how, the estimate for the average monthly living expenses based on students in EPSY 5261 is likely systematically different than the average  monthly living expenses for all graduate students at the University of Minnesota. (It may be systematically too low, or too high.)

:::fyi
The only sampling method that is guaranteed to be unbiased (and therefore representative) is to select your sample randomly from the population. 
:::

<br />


### Random Sampling

There are many methods for randomly selecting a sample from the population. The simplest method that incorporates randomness into the sampling process is **Simple Random Sampling**. In simple random sampling each case in the population has an equal probability of being selected into the sample.^[Technically the definition of a simple random sample is all potential samples of size *n* have the same probability of being selected, which implies that each case in the population has an equal probability of being selected into the sample. Conceptually, however, it is easier to think about the probability of each case rather than of the probability of groups of size *n*.]

:::protip
In the discipline of statistics, there are words that we use that have very different meanings from their use in colloquial English. "Random" is one of those words. In our everyday language "random" might mean happenstance or unexpected. For example: It was so random that I saw Ferris Bueller at the 31 Flavors last night. In statistics, "random" does not mean happenstance at all. Random sampling is quite formal in ensuring that cases have a specified probability of being selected into the sample. 
:::

One of the most compelling and useful results in statistics is that a simple random sample is representative of the population, and moreover that even small samples that are selected with this method can be representative of very large populations. This is powerful! 

But, it can sometimes be very difficult to draw a simple random sample in practice. For one thing, it requires that you have a list of all the cases in the population (called a **sampling frame**). This allows you to make sure that everyone in the population has the same probability of being selected. While it might be possible to obtain a list of all graduate students enrolled at the University of Minnesota, it is another thing to obtain a list of all people living in Minnesota. Or even a list of people living in Dinkytown. Depending on your population of interest you may not be able to get a simple random sample.^[In this case there are other more complex methods of random sampling that you could use (e.g., stratified random sampling, cluster random sampling.]

:::exercises
#### Your Turn {.unnumbered}

What is the sampling method for the New York Time best sellers data. Based on this method, are the estimates of the population parameters we compute from these data going to be biased or unbiased? 

<button class="solution-btn solution-btn-default" onclick="toggle_visibility('exercise_03');">Show/Hide Solution</button>

<div id="exercise_03" style="display:none; margin: -40px 0 40px 40px;">
The New York Time best sellers data was sampled using simple random sampling. Because the sampling method employed randomness, the estimates we compute from the data are unbiased estimates of the population parameters. 
</div>
:::


<br />


## Summary

Every time you encounter data, you should identify the cases and attributes in the data. Understanding the cases, especially in relation to the cases you want to draw conclusions about, helps you identify whether you have a sample, or the entire population. Classifying the attributes helps you think about the type of analysis you can undertake. If your data are a sample (rather than a population), you also need to ask how they were collected. Were they collected using randomness in the sampling method? Or is the sampling method used to collect the data biased? 

<br />


## References



<!-- ## Experiments vs Observations -->

<!-- The type of analysis that is relevant for a given problem depends on the experimental design. More generally, experimental design refers to how the data were collected. How the data were collected has implications for the type of conclusions that can be drawn from the data (i.e., you may have heard the phrase, "correlation does not imply causation") and the subsequent analysis. A extremely sophisticated analysis does not overcome limitations in how the data were collected.  -->

<!-- Given that how the data were collected greatly impacts the types of conclusions that can be drawn from the analysis, what are design features are important to consider? This topic is large, nuanced, and an entire series of courses have been created on this topic. The goal here is to think about some concepts that are particularly helpful to consider for any analysis.  -->

<!-- The discussion will be framed around articulating whether the data collected were part of an experiment or were simply observed. The simplest definition for observational data are those that were collected without strong consideration about who is or how the data are collected. One example of observational data could be collected information about the shoes that people wear as they are walking in a busy part of town. Given this type of data collected, what could be some limitations about this type of data?  -->

<!-- To contrast observational data, experimental data are those in which care was taken to how the data were collected. Within experimental data, it is common for there to be two or more conditions that are being explored. For example, in a clinical trial that is testing whether a vaccine or drug is effective and safe, the new vaccine or drug is often tested against a placebo. The placebo is a harmless substance that does not produce any change to the body, for example, the placebo could be a sugar pill that looks just like a new drug to be tested in every other way. The placebo would simply not contain the active ingredients of the new drug.  -->

<!-- When conducting these experiments with two or more groups that are of interest to be compared, those who are participating in the study are randomly assigned or selected to be in one of the groups. Using the placebo vs new drug example, this would mean that each participant is randomly assigned to either receive the placebo or the new drug, but the participant does not know which one they are receiving. Often, those administering the treatment also do not know whether the placebo or active drug is being given as well.  -->

<!-- <br /> -->

<!-- ### Explore Random Assignment -->

<!-- How does the random assignment of individuals to treatment conditions change the design from an observational to an experimental design? The random process is really the primary difference between being an observational vs an experiment. The random assignment to each condition in the study has the ability to, on average, even out differences across the two groups. Since the inclusion of being in one of the two groups is random, if the study has enough participants, it is more likely for the two groups to be as equal as possible across all characteristics for the study participants.  -->

<!-- <!-- What could random assignment look like in practice? More to come ... . -->

<!-- <br /> -->

<!-- ### Example: Natural Experiments -->

<!-- <br /> -->

<!-- ## Data Structure -->

<!-- Data are often stored in tabular form for ease of use with common statistical programs, however data need not be in this structure. Data can come from anywhere and could consist of text, numbers representing some quantity, text labels representing groups, or many other formats. This section aims to give an introduction to the form and format of data, both common and uncommon. -->

<!-- <br /> -->

<!-- ### Tabular Data -->

<!-- Tabular data are those that are most commonly used in statistics courses. Tabular data are such that rows indicate unique cases of data and the columns represent different attributes for those cases. Table @tbl-tabular-data shows the an example of tabular data.  -->

<!-- ```{r} -->
<!-- #| label: tbl-tabular-data -->
<!-- #| tbl-cap: "Example of tabular data." -->
<!-- #| echo: false -->
<!-- #| message: false -->
<!-- #| warning: false -->
<!-- knitr::kable( -->
<!--   data.frame(name = c('He-Man', 'She-Ra', 'Voltron'), -->
<!--     human = c('Yes', 'Yes', 'No'), -->
<!--     fictional = c('Yes', 'Yes', 'Yes'), -->
<!--     height = c(83, 96, 3936) -->
<!--     ) -->
<!--   ) -->
<!-- ``` -->

<!-- In this table, each row represents a unique person and the first column would represent the identifying attribute for each unique row. For example, the first row represents characteristics for He-Man and each subsequent column represents specific attributes about them. For example, the second and third columns are Yes/No attributes indicating if the person is human or fictional or not. What are the primary differences between the second and third columns? Notice that the third column, the fictional attribute, is one in which all elements in the current tabular data are the same value. This would be an attribute that does not vary across the different rows in the data. These attributes are not helpful from a statistical perspective in this small data set, but if more data were added in which some elements were not fictional, then this attribute would contain useful information. In statistics terminology, this type of attribute would be called a **constant attribute**. In statistics, we are interested in attributes that **vary** across our units in the data. Attributes that vary are often referred to as **variables** in statistics terminology. Throughout this textbook, the term attribute will be used instead of variable. An attribute will refer to a column of data that carries information about the units in the data.  -->

<!-- The final column of data in Table @tbl-tabular-data is one that represents the approximate height of each character, in inches ^[This is based on a search for the character name plus height, for example, "He-Man height."]. This attribute is different from the rest of the attributes in that it is a numeric quantity for each unit. Numeric quantities usually carry more information about magnitude differences compared to the text attributes described earlier. More explicitly, differences in the height attributes can be quantified and in this case, with the attribute being represented as the height in inches, each unit represents the same distance across the entire scale. That is, a one inch difference across the entire height scale is the same no matter where that inch occurs on the scale. These types of attributes will be used extensively in this text, most commonly these types of attributes will be used as the attribute of most interest in our analyses. For example, from the data above, it could be asked if there are differences in height of the characters based on if they are human or not. For this question, differences in height for those that are human (i.e., human attribute = "Yes") compared to those that are not human (ie., human attribute = "No") would be used as the primary comparison. This text will explore questions like this from a descriptive and inferential framework later.  -->

<!-- :::exercises -->
<!-- For these data, there are only three rows, but you could imagine adding more rows to these elements for other characters. For example, imagine adding a new row to this data for a rabbit. Take a few minutes to think about what a new row with the rabbit would look like for each column of Table @tbl-tabular-data. Would any of the columns in the table change from a constant to now being a variable? -->
<!-- ::: -->

<!-- <br /> -->

<!-- ### Non-tabular Data -->

<!-- Data can come in many different formats, this book will not extensively cover data that are not in a tabular format. However, non-tabular data are very common in practical problems. In these situations, these data are often wrangled into a more structured format to conduct a statistical analysis. Some common non-tabular data formats include data coming from text, video, audio, graphics, images, sensors, and even more.  -->

