# One-Sample z-Test: Evaluating Proportions Against a Standard {#sec-one-sample-test-assumptions}

In this chapter you will learn about how to use a one-sample *z*-test to statistically compare a proportion computed in a sample of data to a standard by accounting for the sampling uncertainty. You will also learn about the assumptions that we need to make in order for the results of a one-sample *z*-test to be statistically valid.  

<br />


## Case Study: Lead Levels in Minnesota Children 

Lead exposure has been shown to have deleterious effects on peoples' health and well-being, especially children. The Center for Disease Control collects blood lead surveillance data from all 50 states in the United States. In 2012, the proportion of children tested under 6 years of age that had lead levels in their blood above the CDC reference level for high blood lead levels (5Âµg) was .029. The data in [mn-lead.csv](https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/mn-lead.csv) contains the measured lead levels in the blood for all Minnesota children tested under 6 years of age in 2018. There is also an attribute (`above_cdc`) that indicates whether the lead level is above the CDC reference level for high blood lead levels.

```{r}
#| label: load-packages
#| message: false
library(educate)
library(ggformula)
library(mosaic)
library(mosaicCore)
library(tidyverse)


# Import data
mn_lead <- read_csv("https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/mn-lead.csv")

# View data
mn_lead
```

One question health officials might ask is: Is the proportion of all Minnesota children under 6 years of age who are above the CDC reference level in 2018 lower than the proportion in 2012? That is, they might wish to examine the following hypotheses:

$$
\begin{split}
H_0: \pi = .029 \\[1ex]
H_A: \pi < .029
\end{split}
$$
where $\pi$ (the Greek letter equivalent of "p") is the proportion of all Minnesota children under 6 years of age who are above the CDC reference level. To answer this question, we can compute the proportion of Minnesota children under 6 years of age (who were tested) who have above blood lead levels above the CDC reference level in 2018. We can then carry out a one-sample *z*-test to see if any differences between the 2012 and 2018 proportions are just due to sampling error. 

<br />


### Summarizing the Sample Data

We will start the analysis by summarizing the `above_cdc` attribute to determine the proportion of Minnesota children in 2018 under 6 years of age (who were tested) who have above blood lead levels above the CDC reference level.


```{r}
# Syntax to compute the proportion for each category in the control attribute
df_stats(~above_cdc, data = mn_lead, props)
```

These summaries indicates that in 2018, the proportion of Minnesota children under 6 years of age (who were tested) who have above blood lead levels above the CDC reference level was .0139. The notation we use to denote a sample proportion is $\hat{p}$.

$$
\hat{p} = .0139
$$

This is a lower proportion than was found in the 2012 data by .0151. The next question we would want to tackle is whether this difference is more than we expect because of sampling error. To determine this, we need to carry out a hypothesis test.

<br />


### Testing Proportions Using the One-Sample z-Test

The hypothesis test we use to compare a sample against a known standard is the *one-sample z-test*. The process we use is very similar to that for the one-sample *t*-test, which was:

- Use the sample proportion to compute a *t*-value;
- Locate the observed *z*-value in the *t*-distribution; and
- Determine the *p*-value by computing the area under the curve in the *t*-distribution that is at least as extreme as the observed *t*-value based on the alternative hypothesis.

For the one-sample *z*-test, the process is:

- Use the sample proportion to compute a *z*-value;
- Locate the observed *z*-value in the *z*-distribution; and
- Determine the *p*-value by computing the area under the curve in the *z*-distribution that is at least as extreme as the observed *z*-value based on the alternative hypothesis.

To compute the *z*-value, we use:

$$
z = \frac{\hat{p} - \pi}{SE_{\hat{p}}}
$$

where $\hat{p}$ is the sample proportion, $\pi$ is the value hypothesized in the null hypothesis, and $SE_{\hat{p}}$ is the standard error for the proportion. This SE is computed as:

$$
SE_{\hat{p}} = \sqrt{\frac{\pi(1 - \pi)}{n}}
$$
where again, $\pi$ is the hypothesized proportion in the null hypothesis and $n$ is the sample size. In our example, the *z*-value is:

$$
\begin{split}
z &= \frac{.01391403 - .029}{\sqrt{.029(1 - .029)/91706}} \\[1ex]
&= \frac{-.0151}{.0006} \\[1ex]
&= -27.22
\end{split}
$$

Similar to a *t*-value, a *z*-value indicates how many standard errors the sample mean is from the hypothesized value, In our case, the sample proportion we computed in the data of .0139 is 27.22 standard errors below the hypothesized value of .029. We can evaluate this in the *z*-distribution.

Unlike the *t*-distribution which is different depending on the *df*, there is only one *z*-distribution. The *z*-distribution is a normal distribution that has a mean of 0 and a standard deviation of 1. The *z*-distribution is shown in @fig-z-dist.

```{r}
#| label: fig-z-dist
#| fig-cap: "Density plot of the *z*-distribution."
#| fig-width: 6
#| fig-height: 4
#| out-width: "80%"
#| echo: FALSE
ggplot(data = data.frame(x = c(-3, 3)), aes(x = x)) +
    stat_function(
      fun = dnorm,
      geom = "line"
    ) +
    theme_bw() +
    scale_x_continuous(
      name = "z",
      breaks = -3:3
      ) +
    ylab("Density")
```

To find the *p*-value associated with the alternative hypothesis that $\pi<.029$, we will include the observed *z*-value of -27.22 into the *z*-distribution and shade the area under the curve that is less than the observed *z*-value. A sketch of this is shown in @fig-z-dist2.

```{r}
#| label: fig-z-dist2
#| fig-cap: "Sketch of the density plot of the *z*-distribution with the observed *z*-value of -27.22 also included. The shaded area to the left of -27.22 constitutes the *p*-value associate with the null hypotheis that the population proportion is less than .029."
#| fig-width: 6
#| fig-height: 4
#| out-width: "80%"
#| echo: FALSE
knitr::include_graphics("figs/04-08-z-dist-sketch.jpg")
```

The *p*-value here is going to be quite small since the area under the curve to the left of $-37.75$ is quite small relative to the area under the whole curve. 

In practice, we will use the `prop_test()` function from the `{mosaic}` package to carry out the one-sample *z*-test and compute the *p*-value. This function takes: 

- A formula using the tilde (`~`), similar to the `gf_` and `df_stats` functions, that specifies the attribute to carry out the one-sample *z*-test on. We also need to specify the level of the attribute we want to compute the sample proportion for using `==` and then giving the exact name for that level inside quotation marks.
- `data=` specifying the name of the data object,
- `p=` indicating the value of the proportion in the null hypothesis,
- `alternative=` indicating one of three potential alternative hypotheses: `"less"`, `"greater"`, or `"two.sided"` (not equal). Note that these need to be enclosed in quotation marks.
- `correct=FALSE` indicating that we want to do the calculation of the *z*-value without a correction for continuity which will mimic the formula.

To carry out the one-sample *z*-test, we will use the following syntax. Note that in the formula we also indicate that we want to compute the sample proportion for the `"Yes"` values. We assign the results of this *z*-test to an object (in this case, I called it `my_z`). Then we can use the `z_results()` and `plot_z_dist()` functions (both from the `{educate}` package) to show the results of the *z*-test and plot the *z*-distribution along with the observed *z*-value and shaded area associated with the *p*-value.

```{r}
# One-sample z-test
my_z <- prop_test(
  ~above_cdc == "Yes", 
  data = mn_lead, 
  p = .029, 
  alternative = "less",
  correct = FALSE
  )

# Plot z-distribution, observed z-value and shaded p-value
plot_z_dist(my_z)

# Show z-test results
z_results(my_z)
```

Based on the *p*-value, and using an $\alpha$-value of .05, we would reject the null hypothesis. This suggests *it is likely* that the proportion of all Minnesota children under 6 years of age who are above the CDC reference level in 2018 is lower than the proportion in 2012. In this interpretation we call attention to the words "it is likely" to remind you that it is possible we may have made a Type I error in which case the proportion of all Minnesota children under 6 years of age who are above the CDC reference level in 2018 is NOT lower than the proportion in 2012.


<br />


## Assumptions for the One-Sample z-Test

Whether or not the *p*-value we obtain from the *z*-test is accurate depends on the following set of statistical assumptions: 

1. The values in the population follow a binomial distribution. This is true so long as there are only two values the attribute can take on (e.g., "Yes" or "No").
2. The values in the population are *independent* from each other.
3. The quantities $n(\hat{p})$ and $n(1-\hat{p})$ are both greater than 10, where *n* is the sample size and $\hat{p}$ is the sample proportion value. 


To evaluate the first assumption that the distribution of values in the population follow a binomial distribution, we only need to confirm that the population only has two values. In our example, this is true; the only two values a case can have is "Yes" (blood lead level is above the CDC reference) or "No" (blood lead level is not above the CDC reference). 

We will evaluate the independence assumption the same way we did for the one-sample *t*-test, by referring to the study design. In our example, the cases in the data do not constitute a random sample of all Minnesota children. Does knowing that one Minnesota child's blood lead level is above (or below) the CDC reference level give us any information about whether any other Minnesota child's blood lead level is above (or below) the CDC reference level? Without additional data it is difficult to know, so we could argue that the independence assumption seems tenable.^[Just remember other scholars might argue that the population values are not independent.]

Lastly we compute the quantities in the third assumption and check that they are both greater than 10.

$$
\begin{split}
n(\hat{p}) &= 91706(.01391403) \\[1ex]
&= 1276 \\[3ex]
n(1-\hat{p}) &= 91706(1 - .01391403) \\[1ex]
&= 90430
\end{split}
$$


<br />


### Case Study: Test Responses

The [Rotten Tomatoes](https://www.rottentomatoes.com/) website includes both audiences' and critics' ratings for different movies. The ratings are then classified into one of two categories: "Fresh" (which indicates a positive review) or "Rotten" (which indicates a negative review). The data in [fast-x-reviews.csv](https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/fast-x-reviews.csv) includes the critic ratings (as of May 23, 2023) for the film Fast X (the 10th installment of the *Fast &amp; the Furious* franchise).

  


claims that Republican Party will win in the next Senate elections, especially in Florida State. Statistical data reported that 23% voted for Republican Party in the last election. To test the claim a researcher surveyed 80 people and found 22 said they voted for Republican Party in the last election. Is there enough evidence at Î±=0.05 to support this claim?


:::exercises
#### Your Turn {.unnumbered}

In the continuous assessment case study from @sec-one-sample-test-r, we evaluated whether, on average, Ethiopian primary school teachers agree/disagree with the statement that they assess students' prior knowledge. The histogram of the 30 teachers' responses is shown below. Based on this plot and the sample size, do you believe the normality assumption is tenable?

```{r}
#| label: fig-prior-knowledge
#| fig-cap: "Histogram of teachers responses to the survey item: I always assess students' prior knowledge before starting new lesson. "
#| warning: false
#| echo: false
library(ggformula)
continuous_assessment <- read_csv("https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/continuous-assessment.csv")

# Create density plot
gf_histogram(
  ~prior_knowledge, data = continuous_assessment,
  bins = 4,
  color = "black", 
  fill = "skyblue",
  xlab = "Level of Agreement",
  ylab = "Count"
  )
```

<button class="solution-btn solution-btn-default" onclick="toggle_visibility('exercise_04-05-05');">Show/Hide Solution</button>

<div id="exercise_04-05-05" style="display:none; margin: -40px 0 40px 40px;">
Yes, the normality assumption seems tenable. While the distribution of the sample responses is not normally distributed, it is not egregiously skewed. Moreover, since the sample size is 30, the CLT would suggest that we can assume that the *p*-value is not going to be impacted by any deviation in the population distribution from normality.
</div>

Based on the data collection process described in the [data codebook](https://zief0002.github.io/epsy-5261/codebooks/continuous-assessment.html), do you believe the independence assumption is tenable?

<button class="solution-btn solution-btn-default" onclick="toggle_visibility('exercise_04-05-06');">Show/Hide Solution</button>

<div id="exercise_04-05-06" style="display:none; margin: -40px 0 40px 40px;">
Yes, the independence assumption is tenable. The use of random chance to select the sample data (random sampling) guarantees independence.
</div>
:::

<br />


### Evaluating Assumptions in the House Prices Case Study

:::exercises
#### Your Turn {.unnumbered}

In the house prices case study from @sec-one-sample-test-r, we evaluated whether, on average, houses near the University of Minnesota campus are more expensive than \$322.46k (the average price of a single-family house in Minneapolis as of May 2023). The histogram of the 15 sample house prices is shown below. Based on this plot and the sample size, do you believe the normality assumption is tenable?

```{r}
#| label: fig-house-prices
#| fig-cap: "Histogram of the asking price for 15 houses in neighborhoods adjacent to the UMN campus. "
#| warning: false
#| echo: false
zillow <- read_csv("https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/zillow.csv")

# Create histogram
gf_histogram(
  ~price, data = zillow,
  binwidth = 75,
  color = "black", 
  fill = "skyblue",
  xlab = "House Price",
  ylab = "Count"
  )
```

<button class="solution-btn solution-btn-default" onclick="toggle_visibility('exercise_04-05-05');">Show/Hide Solution</button>

<div id="exercise_04-05-05" style="display:none; margin: -40px 0 40px 40px;">
The normality assumption may not be tenable. The distribution of the sample responses is not normally distributed, and is clearly left-skewed. Moreover, since the sample size is less than 30, we can not rely on the CLT.
</div>

Based on the data collection process described in the [data codebook](https://zief0002.github.io/epsy-5261/codebooks/zillow.html), do you believe the independence assumption is tenable?

<button class="solution-btn solution-btn-default" onclick="toggle_visibility('exercise_04-05-06');">Show/Hide Solution</button>

<div id="exercise_04-05-06" style="display:none; margin: -40px 0 40px 40px;">
It is unclear that the independence assumption is tenable. The data collection did not use random chance to select the sample data. Moreover, it is likely that knowing the sale price for one house would give us information about the sale price for other houses since houses in the same neighborhood are often sold for similar prices.
</div>
:::

## References

