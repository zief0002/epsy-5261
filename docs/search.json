[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Data Analysis and Randomization Methods",
    "section": "",
    "text": "Foreword\nThis work in progress is ultimately going to be the primarily textbook resource for EPSY 5261 students. (Note: If you want to contribute to this, create a Pull Request or send me an email.) Also, feel free to offer criticism, suggestion, and feedback. You can either open an issue on the book‚Äôs github page or send me an email directly."
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "An Introduction to Data Analysis and Randomization Methods",
    "section": "Colophon",
    "text": "Colophon\nArtwork by @allison_horst\nIcon and note ideas and prototypes by Desir√©e De Leon.\nThe book is typeset using Crimson Text for the body font, Raleway for the headings and Sue Ellen Francisco for the title. The color palette was generated using coolors.co.\nStatistical Computing\n\nLaptop icon made by Tomas Knop from www.flaticon.com\nDirectory icon made by Darius Dan from www.flaticon.com\nBrain icon made by Aranagraphics from www.flaticon.com\nInternet icon made by Freepik from www.flaticon.com"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "An Introduction to Data Analysis and Randomization Methods",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "01-statistical-computation.html",
    "href": "01-statistical-computation.html",
    "title": "Statistical Computation",
    "section": "",
    "text": "The first set of tools we will discuss will be related to statistical computation. Although there are many computational tools for statistical analysis, the first tools we will add to your computational toolkit is R. R is a free software environment for statistical computing and graphics. It can be installed on a variety of operating systems, including the MacOS, Windows, and UNIX platforms. To really make use of the computational power of R, we are also going to introduce you to RStudio, an open-source front-end1 to R.\nThe initial chapters of this document will address:\n\nInstalling R and RStudio;\nGetting started with R‚Äôs computational syntax;\nWrangling data using functions from the dplyr package; and\nVisualizing data using functions from the ggplot2 package.\n\n\n\n\n\n\n\nSpecifically, RStudio is branded as an ‚Äúintegrated development environment (IDE) [that] includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.‚Äù‚Ü©Ô∏é"
  },
  {
    "objectID": "02-r-and-rstudio-installation.html#installing-r",
    "href": "02-r-and-rstudio-installation.html#installing-r",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.1 Installing R",
    "text": "1.1 Installing R\nTo install R, navigate your web browser to:\n\nhttps://www.r-project.org/\n\nThen,\n\nClick the CRAN link under Download on the left-hand side of the page.\nSelect a mirror site. These should all be the same, but I tend to choose the Iowa State University link under USA.1\nIn the Download and Install R box, choose the binary that matches the operating system (OS) for your computer.\n\nThis is where the installation directions diverge depending on your OS.\nMac Instructions\nSo long as you are running MacOS 10.13 or higher just click the first link for the PKG, which will download the installer for the most current version of R (4.1.1 as of August 16, 2021). Once the download completes, open the installer and follow the directions to install R on your computer.\nIf you are running an older version of MacOS, you will have to install an older version of R. You can find these links under the Binaries for legacy OS X systems heading further down the install page. Click the appropriate PKG link for R your version of MacOS. Once the download completes, open the installer and follow the directions to install R on your computer.\nIf you are unsure which version of the MacOS is running on your computer, select About this Mac from the Apple menu in your toolbar.\nWindows Instructions\nClick the link that says Install R for the first time (or click base; they go to the same place). Then click the Download R 4.1.1 for Windows link, which will download the installer for the most current version of R (4.0.2 as of July 24, 2020). Once the download completes, open the installer and follow the directions to install R on your computer.\nLinux Instructions\nIf you are running Linux, you should know how to install things on your computer. üôã"
  },
  {
    "objectID": "02-r-and-rstudio-installation.html#installing-rstudio-desktop",
    "href": "02-r-and-rstudio-installation.html#installing-rstudio-desktop",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.2 Installing RStudio Desktop",
    "text": "1.2 Installing RStudio Desktop\nAfter you have installed R, you next need to install RStudio Desktop. To do this, navigate your web browser to:\n\nhttps://rstudio.com/products/rstudio/download/\n\nThen,\n\nSelect the blue Download button under the free, open-source version of RStudio Desktop.\nSelect the installer associated with your computer‚Äôs OS.\nOnce the download completes, open the installer and follow the directions to install RStudio Desktop on your computer."
  },
  {
    "objectID": "02-r-and-rstudio-installation.html#checking-that-things-worked",
    "href": "02-r-and-rstudio-installation.html#checking-that-things-worked",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.3 Checking that Things Worked",
    "text": "1.3 Checking that Things Worked\nFrom your Applications or Programs folder, open RStudio. If you have successfully downloaded both programs, this should open the application and you should see a message indicating that you are using ‚ÄúR version 4.1.1‚Äù (or whichever version of R you installed) in the console pane.\n\n\n\n\n\nOnce you open RStudio, you should see a message indicating that you are using R version 4.1.1 (or whichever version of R you installed) in the console pane. Here the console pane is on the left-side, but it may be in a different location for you. Your RStudio may also have a white background rather than the black background seen here."
  },
  {
    "objectID": "02-r-and-rstudio-installation.html#customizing-rstudio",
    "href": "02-r-and-rstudio-installation.html#customizing-rstudio",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.4 Customizing RStudio",
    "text": "1.4 Customizing RStudio\nWhile the information in this section is not crucial for making things work, it is useful to get RStudio looking good and setting some default settings. Open the Tools > Options menu (Windows) or RStudio > Preferences (Mac).\n\n\n\n\n\nThe RStudio options/preferences menu has many settings to customize RStudio.\n\n\n\n\n\nIn the General > Basic settings, change the option on Save workspace to .Rdata on exit to be ‚ÄúNever‚Äù. Click the ‚ÄúApply‚Äù button.\nIn the Appearance settings, customize the look of RStudio to something aesthetically appealing to you. When you are finished, click the ‚ÄúApply‚Äù button.\nThere are also options you can set in the Accessibility settings if you use a screen reader. If you change anything, don‚Äôt forget to click the ‚ÄúApply‚Äù button.\n\nWhen you are finished customizing RStudio, click the ‚ÄúOK‚Äù button."
  },
  {
    "objectID": "02-r-and-rstudio-installation.html#install-rtoolscommand-line-tools",
    "href": "02-r-and-rstudio-installation.html#install-rtoolscommand-line-tools",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.5 Install Rtools/Command Line Tools",
    "text": "1.5 Install Rtools/Command Line Tools\nYou may need to install some additional functionality to your system in order to get certain packages to install or load properly. On a Windows machine, you might need to install Rtools. Mac users might need to add the Command Line Tools. These tools also allow you to write and compile your own R packages. RStudio has well written instructions for adding these tools at: https://support.rstudio.com/hc/en-us/articles/200486498-Package-Development-Prerequisites."
  },
  {
    "objectID": "04-data-structures-in-r.html#vectors",
    "href": "04-data-structures-in-r.html#vectors",
    "title": "2¬† Data Structures in R",
    "section": "2.1 Vectors",
    "text": "2.1 Vectors\nVectors (the single-column bookcases in our metaphor) are perhaps the most common data structure you will encounter in R. In fact, even the data frame is composed of vectors; each column is a vector. There are many ways to create a vector in R, in fact you have already been introduced to a couple of them: seq() and rep(). These are useful to create sequences of values and vectors with repeated values, respectively. But what if you wanted to create the vector of the Spice Girls‚Äô ages when the band was formed in 1994 (the values in the second column in the picture above)?\nTo create a vector of these ages, we can use the c() function to input each of the five ages. Within this function, each age is separated by a comma‚Äîeach input is a separate argument to the c() function. We will also assign this to an object called age.\n\n# Create age vector\nage = c(19, 20, 18, 22, 20)\n\n# View vector\nage\n\n[1] 19 20 18 22 20\n\n\nNote that once we assign create age it shows up in our global environment. In the technical language of R, each age is an element of the vector. All of the elements in the age vector are numeric values. This is the vector‚Äôs type.3 Lastly, there are five elements in the vector.\nOnce you have created a numeric vector, you can compute on it. For example in the syntax below we compute the mean age, the standard deviation of the ages, and count the elements in the vector.\n\n# Compute mean\nmean(age)\n\n[1] 19.8\n\n# Compute standard deviation\nsd(age)\n\n[1] 1.48324\n\n# Count elements\nlength(age)\n\n[1] 5\n\n\n\n\n2.1.1 Logical Vectors\nAnother common vector type you will encounter is the logical vector. Each element in a logical vector is either TRUE or FALSE (all uppercase letters). You could use the c() function to create a logical vector. For example, to create the original_member vector we could use the following syntax:\n\n# Create logical vector\noriginal_member = c(TRUE, TRUE, FALSE, TRUE, FALSE)\n\n# View vector\noriginal_member\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE\n\n\nIt is more common to create logical vectors through computation using logical operators. For example we might ask which elements of the age object are greater than 20.\n\n# Which elements of age > 20\nage > 20\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\nThe result of using the logical operator > is a logical vector. There are several logical operators in addition to >:\n\n# greater than or equal to 20\nage >= 20\n\n[1] FALSE  TRUE FALSE  TRUE  TRUE\n\n# less than 20\nage < 20\n\n[1]  TRUE FALSE  TRUE FALSE FALSE\n\n# less than or equal to 20\nage <= 20\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE\n\n# equal to 20\nage == 20\n\n[1] FALSE  TRUE FALSE FALSE  TRUE\n\n# not equal to 20\nage != 20\n\n[1]  TRUE FALSE  TRUE  TRUE FALSE\n\n\nNote that the logical operator for ‚Äúequal to‚Äù is two equals signs. This is because = (one equal sign) is what we use for assignment . If you wrote age=20 you would be assigning the value 20 to age, not asking whether the elements in age are equal to 20!\nLogical elements have numeric values associated with them, namely,\n\nFALSE = 0; and\nTRUE = 1.\n\nThis means we can apply computations to a logical vector. For example, we could count the number of Spice Girls that were original members by summing the logical values in the original_members object. (Since all FALSE values are 0, this amounts to counting the number of TRUE values.)\n\n# Count original members\nsum(original_member)\n\n[1] 3\n\n\nWe could also count the number of Spice Girls who are under the age of 20.\n\n# Count members with age < 20\nsum(age < 20)\n\n[1] 2\n\n\n\n\n\n2.1.2 Character Vectors\nA third type of vector you will work with is a character vector. Character vectors (a.k.a., strings, literals) are vectors in which each element is a string of characters delimited by quotation marks. For example, the Spice names column is a character vector. We can again create this vector using the c() function.\n\n# Create character vector\nspice = c(\"Scary\", \"Sporty\", \"Baby\", \"Ginger\", \"Posh\")\n\n# View vector\nspice\n\n[1] \"Scary\"  \"Sporty\" \"Baby\"   \"Ginger\" \"Posh\"  \n\n\nMany computations that worked on numeric vectors do not work on character vectors. These will often return an error or unexpected result. In the syntax below, for example, we are told that the mean() function expects a numeric or logical vector, and since what we used was not either of those, the result returned was NA.\n\n# Find mean name\nmean(spice)\n\nWarning in mean.default(spice): argument is not numeric or logical: returning\nNA\n\n\n[1] NA\n\n\nSome computations work the same way.\n\n# Count the number of elements\nlength(spice)\n\n[1] 5"
  },
  {
    "objectID": "04-data-structures-in-r.html#data-frames",
    "href": "04-data-structures-in-r.html#data-frames",
    "title": "2¬† Data Structures in R",
    "section": "2.2 Data Frames",
    "text": "2.2 Data Frames\nData frames (the multi-column bookcases in our metaphor) are a more complex data structures than vectors. There are again, multiple ways to create a data frame in R. We will examine two methods for creating a data frame: using the data.frame() function and importing data from a spreadsheet or CSV file.\nTo create a data frame from scratch, using R, we can use the data.frame() function. Each argument to this function is a named vector that will correspond to a column within the data frame, and each argument (vector) is separated by commas. For example, to create the Spice Girls data frame from our example, we could use the following syntax:\n\n# Create data frame\nspice_girls = data.frame(\n  spice = c(\"Scary\", \"Sporty\", \"Baby\", \"Ginger\", \"Posh\"),\n  age = c(19, 20, 18, 22, 20),\n  original_member = c(TRUE, TRUE, FALSE, TRUE, FALSE),\n  solo_nominations = c(4, 26, 14, 13, 12),\n  real_name = c(\"Mel B\", \"Mel C\", \"Emma\", \"Geri\", \"Victoria\")\n)\n\n# View data frame\nspice_girls\n\n\n\n  \n\n\n\nNote that we also assigned the data frame to an object called spice_girls so we can compute on it. You will learn how to compute on data frames in the chapters Data Wrangling with dplyr and Plotting with ggplot2."
  },
  {
    "objectID": "04-data-structures-in-r.html#importing-data-from-a-csv-file",
    "href": "04-data-structures-in-r.html#importing-data-from-a-csv-file",
    "title": "2¬† Data Structures in R",
    "section": "2.3 Importing Data From a CSV File",
    "text": "2.3 Importing Data From a CSV File\nIn professional practice, you will often enter data into a spreadsheet and rather than typing it into R. When you save this work, many spreadsheet programs use a proprietary format for saving the information (e.g., Excel saves as a XLSX file; Google Sheets saves as a GSHEET file). These often include extraneous information (e.g., formatting) that is irrelevant to the raw data. While R includes libraries and functions that can import data in the XLSX and GSHEETS formats, it is generally easier to save or export your data to a CSV (comma separated value) file from within your spreadsheet program prior to importing it into R.\n\n\n\n\n\n\nHere are some tips for entering data into a spreadsheet:\n\nThe first row should be the variable names. Do not use spaces in variable names.\nCharacter strings should be entered without quotation marks in a spreadsheet.\nIf you have missing data, leave the cell blank.\n\nFor more tips on entering data, see Broman & Woo (2018).\n\nOnce your data are saved as a CSV file, it can be easily imported into R. To do so,\n\nClick the Import Dataset button under the Environment tab in RStudio and choose ‚ÄúFrom Text (readr)‚Äù.\nIf the CSV file is a file stored on your computer, click the Browse button and navigate to where you saved your CSV file, select the file, and click ‚ÄúOpen‚Äù. If the CSV file is hosted on the web, type the URL into the ‚ÄúFile/URL‚Äù text box and click ‚ÄúUpdate‚Äù.\n\n\n\n2.3.1 Importing the Spice Girls Data\nThe file spice-girls.csv is accessible at https://raw.githubusercontent.com/zief0002/toolkit/master/data/spice_girls.csv.\n\nCopy and paste that URL into the ‚ÄúFile/URL‚Äù text box.\nClick the ‚ÄúUpdate‚Äù button.\n\nClicking ‚ÄúUpdate‚Äù will open a preview of your data. Check to be sure the variable names are correct and that the data looks like what you entered into your spreadsheet.\n\nChange the text in the name box to correspond to the object name you want to use in R.\nFinally, click the Import button to import your data.\n\nAfter importing the data you should see the object in your global environment.\n\n\n\n\n\n\n\n\n2.3.2 Importing Data Using a Script File\nEven though you used the Import button‚Äîa point-and-click feature in RStudio‚Äîto import the data, behind the scenes, syntax was generated that was actually used to import the data into R. When we selected ‚ÄúFrom Text (readr)‚Äù, the read_csv() function from the {readr} package was used to import the data. You can see the syntax generated in the Code Preview window after you selected your CSV file.\n\n\n\n\n\nIn the first line of syntax, the {readr} package is loaded using the library() function. The data is imported in the second line of syntax and assigned to an object, in this case spice_girls. The read_csv() function includes an unnamed argument providing the URL for the CSV file.4 The View() function in the third line of syntax simply opens the spice_girls object in a view tab in RStudio.\nIt is a good idea to copy the first two lines of syntax from the Code Preview window into your script file. It will be faster to import the data in the future by running it from a script file rather than trying to reproduce all the steps to import your data. The third line of syntax, using View(), is not essential to importing your data..\n\nSince there are better ways to actually ‚Äúsee‚Äù the contents of the data object (e.g., print()), you should not include the View() syntax line in your script file.\n\nBelow are the two lines I would include in the script file. I would also comment them.\n\n# Load readr library\nlibrary(readr)\n\n# Import data\nspice_girls <- read_csv(\"https://raw.githubusercontent.com/zief0002/toolkit/master/data/spice_girls.csv\")\n\n\nThe syntax <- is another way to write the assignment operator. You can use either = or <- for the assignment operator. Whichever you choose, be consistent!"
  },
  {
    "objectID": "04-data-structures-in-r.html#validity-check-on-imported-data",
    "href": "04-data-structures-in-r.html#validity-check-on-imported-data",
    "title": "2¬† Data Structures in R",
    "section": "2.4 Validity Check on Imported Data",
    "text": "2.4 Validity Check on Imported Data\nOnce you import data, you should always perform a validity check to ensure that the entire dataset was imported and that things look OK. There are several functions that are useful for this examination. Three that I use regularly are print(), glimpse() and summary().\nThe print() function gives us a quick look at the data.\n\n# View data\nprint(spice_girls)\n\n# A tibble: 5 √ó 5\n  spice_name   age original_member solo_nominations real_name\n  <chr>      <dbl> <lgl>                      <dbl> <chr>    \n1 Scary         19 TRUE                           4 Mel B    \n2 Sporty        20 TRUE                          26 Mel C    \n3 Baby          18 FALSE                         14 Emma     \n4 Ginger        22 TRUE                          13 Geri     \n5 Posh          20 FALSE                         12 Victoria \n\n\nNote that from this output we can see that the read_csv() function actually imports the data as a tibble. Tibbles are essentially the same data structure as data frames. The only difference is that when you use print() (and some other functions) to examine the data object, what is printed to the screen is slightly different.5 For tibbles,\n\nThe number of rows and columns is displayed;\nThe first 10 rows are shown;\nOnly the columns that fit on screen are printed; and\nEach column type is reported\n\nHere the size of the data object is 5 x 5, which indicates that there are five rows (first value) and five columns (second value). We are also informed which columns are numeric, which are logical, and which are character.6\nThe summary() function computes summary statistics for each column in the data object. Different measures are computed depending on the column type. For character columns, only the length of the column is computed. The count of TRUE and FALSE values are computed for logical columns, and several measures are computed for numeric columns.\n\n# Compute summary measures for each column\nsummary(spice_girls)\n\n  spice_name             age       original_member solo_nominations\n Length:5           Min.   :18.0   Mode :logical   Min.   : 4.0    \n Class :character   1st Qu.:19.0   FALSE:2         1st Qu.:12.0    \n Mode  :character   Median :20.0   TRUE :3         Median :13.0    \n                    Mean   :19.8                   Mean   :13.8    \n                    3rd Qu.:20.0                   3rd Qu.:14.0    \n                    Max.   :22.0                   Max.   :26.0    \n  real_name        \n Length:5          \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nThis is another good validity check to ensure that numeric columns have appropriate minimum and maximum values, etc. Here we see that the the age and solo_nominations columns have reasonable values. In practice you would undertake many more validity checks, but for now this is a good start."
  },
  {
    "objectID": "04-data-structures-in-r.html#references",
    "href": "04-data-structures-in-r.html#references",
    "title": "2¬† Data Structures in R",
    "section": "References",
    "text": "References\n\n\n\n\nBroman, K. W., & Woo, K. H. (2018). Data organization in spreadsheets. The American Statistician, 72(1), 2‚Äì10. https://doi.org/10.1080/00031305.2017.1375989"
  },
  {
    "objectID": "05-data.html#experiments-vs-observations",
    "href": "05-data.html#experiments-vs-observations",
    "title": "3¬† Data: Where Does it Come From?",
    "section": "3.1 Experiments vs Observations",
    "text": "3.1 Experiments vs Observations\nThe type of analysis that is relevant for a given problem depends on the experimental design. More generally, experimental design refers to how the data were collected. How the data were collected has implications for the type of conclusions that can be drawn from the data (i.e., you may have heard the phrase, ‚Äúcorrelation does not imply causation‚Äù) and the subsequent analysis. A extremely sophisticated analysis does not overcome limitations in how the data were collected.\nGiven that how the data were collected greatly impacts the types of conclusions that can be drawn from the analysis, what are design features are important to consider? This topic is large, nuanced, and an entire series of courses have been created on this topic. The goal here is to think about some concepts that are particularly helpful to consider for any analysis.\nThe discussion will be framed around articulating whether the data collected were part of an experiment or were simply observed. The simplest definition for observational data are those that were collected without strong consideration about who is or how the data are collected. One example of observational data could be collected information about the shoes that people wear as they are walking in a busy part of town. Given this type of data collected, what could be some limitations about this type of data?\nTo contrast observational data, experimental data are those in which care was taken to how the data were collected. Within experimental data, it is common for there to be two or more conditions that are being explored. For example, in a clinical trial that is testing whether a vaccine or drug is effective and safe, the new vaccine or drug is often tested against a placebo. The placebo is a harmless substance that does not produce any change to the body, for example, the placebo could be a sugar pill that looks just like a new drug to be tested in every other way. The placebo would simply not contain the active ingredients of the new drug.\nWhen conducting these experiments with two or more groups that are of interest to be compared, those who are participating in the study are randomly assigned or selected to be in one of the groups. Using the placebo vs new drug example, this would mean that each participant is randomly assigned to either receive the placebo or the new drug, but the participant does not know which one they are receiving. Often, those administering the treatment also do not know whether the placebo or active drug is being given as well.\n\n\n3.1.1 Explore Random Assignment\nHow does the random assignment of individuals to treatment conditions change the design from an observational to an experimental design? The random process is really the primary difference between being an observational vs an experiment. The random assignment to each condition in the study has the ability to, on average, even out differences across the two groups. Since the inclusion of being in one of the two groups is random, if the study has enough participants, it is more likely for the two groups to be as equal as possible across all characteristics for the study participants.\n\n\n\n\n3.1.2 Example: Natural Experiments"
  },
  {
    "objectID": "05-data.html#data-structure",
    "href": "05-data.html#data-structure",
    "title": "3¬† Data: Where Does it Come From?",
    "section": "3.2 Data Structure",
    "text": "3.2 Data Structure\nData are often stored in tabular form for ease of use with common statistical programs, however data need not be in this structure. Data can come from anywhere and could consist of text, numbers representing some quantity, text labels representing groups, or many other formats. This section aims to give an introduction to the form and format of data, both common and uncommon.\n\n\n3.2.1 Tabular Data\nTabular data are those that are most commonly used in statistics courses. Tabular data are such that rows indicate unique cases of data and the columns represent different attributes for those cases. Table Table¬†3.1 shows the an example of tabular data.\n\n\n\n\nTable¬†3.1: Example of tabular data.\n\n\nname\nhuman\nfictional\nheight\n\n\n\n\nHe-Man\nYes\nYes\n83\n\n\nShe-Ra\nYes\nYes\n96\n\n\nVoltron\nNo\nYes\n3936\n\n\n\n\n\n\nIn this table, each row represents a unique person and the first column would represent the identifying attribute for each unique row. For example, the first row represents characteristics for He-Man and each subsequent column represents specific attributes about them. For example, the second and third columns are Yes/No attributes indicating if the person is human or fictional or not. What are the primary differences between the second and third columns? Notice that the third column, the fictional attribute, is one in which all elements in the current tabular data are the same value. This would be an attribute that does not vary across the different rows in the data. These attributes are not helpful from a statistical perspective in this small data set, but if more data were added in which some elements were not fictional, then this attribute would contain useful information. In statistics terminology, this type of attribute would be called a constant attribute. In statistics, we are interested in attributes that vary across our units in the data. Attributes that vary are often referred to as variables in statistics terminology. Throughout this textbook, the term attribute will be used instead of variable. An attribute will refer to a column of data that carries information about the units in the data.\nThe final column of data in Table Table¬†3.1 is one that represents the approximate height of each character, in inches 1. This attribute is different from the rest of the attributes in that it is a numeric quantity for each unit. Numeric quantities usually carry more information about magnitude differences compared to the text attributes described earlier. More explicitly, differences in the height attributes can be quantified and in this case, with the attribute being represented as the height in inches, each unit represents the same distance across the entire scale. That is, a one inch difference across the entire height scale is the same no matter where that inch occurs on the scale. These types of attributes will be used extensively in this text, most commonly these types of attributes will be used as the attribute of most interest in our analyses. For example, from the data above, it could be asked if there are differences in height of the characters based on if they are human or not. For this question, differences in height for those that are human (i.e., human attribute = ‚ÄúYes‚Äù) compared to those that are not human (ie., human attribute = ‚ÄúNo‚Äù) would be used as the primary comparison. This text will explore questions like this from a descriptive and inferential framework later.\n\nFor these data, there are only three rows, but you could imagine adding more rows to these elements for other characters. For example, imagine adding a new row to this data for a rabbit. Take a few minutes to think about what a new row with the rabbit would look like for each column of Table Table¬†3.1. Would any of the columns in the table change from a constant to now being a variable?\n\n\n\n\n3.2.2 Non-tabular Data\nData can come in many different formats, this book will not extensively cover data that are not in a tabular format. However, non-tabular data are very common in practical problems. In these situations, these data are often wrangled into a more structured format to conduct a statistical analysis. Some common non-tabular data formats include data coming from text, video, audio, graphics, images, sensors, and even more."
  },
  {
    "objectID": "06-visualization.html",
    "href": "06-visualization.html",
    "title": "4¬† Visualization",
    "section": "",
    "text": "5 College Scorecard Data\nThe U.S. Department of Education publishes data on institutions of higher education in their College Scorecard (https://collegescorecard.ed.gov/) to facilitate transparency and provide information for interested stakeholders (e.g., parents, students, educators). A subset of this data is provided in the file College-scorecard-clean.csv. To illustrate some of the common methods statisticians use to visualize data, we will examine admissions rates for 2,019 institutions of higher education.\nBefore we begin the analysis, we will load two packages, the tidyverse package and the ggformula package. These packages include many useful functions that we will use in this chapter.\nThere are many functions in R to import data. We will use the function read_csv() since the data file we are importing (College-scorecard-clean.csv) is a comma separated value (CSV) file..2 CSV files are a common format for storing data. Since they are encoded as text files they generally do not take up a lot of space nor computer memory. They get their name from the fact that in the text file, each data attribute (i.e.¬†column in the data) is separated by a comma within each row. Each row represents a unique case or observation from the data. The syntax to import the college scorecard data is as follows:\nIn this syntax we have passed two arguments to the read_csv() function. The first argument, file=, indicates the path to the data file. The data file here is stored on GitHub, so the path is specified as a URL. The second argument, guess_max=, helps ensure that the data are read in appropriately. This argument will be described in more detail later.\nThe syntax to the left of the read_csv() function, namely colleges <-, takes the output of the function and stores it, or in the language of R, assigns it to an object named colleges. In data analysis, it is often useful to use results in later computations, so rather than continually re-running syntax to obtain these results, we can instead store those results in an object and then compute on the object. Here for example, we would like to use the data that was read by the read_csv() function to explore it. When we want to assign computational results to an object, we use the assignment operator, <- . (Note that the assignment operator looks like a left-pointing arrow; it is taking the computational result produced on the right side and storing it in the object to the left side.)"
  },
  {
    "objectID": "06-visualization.html#view-the-data",
    "href": "06-visualization.html#view-the-data",
    "title": "4¬† Visualization",
    "section": "5.1 View the Data",
    "text": "5.1 View the Data\nOnce we have imported and assigned the data to an object, it is quite useful to ensure that it was read in appropriately. The head() function will give us a quick snapshot of the data by printing the first six rows of data.\n\nhead(colleges)\n\n\n\n  \n\n\n\nWe can also include an interactive version for viewing the book on the web using the DT package.\n\nDT::datatable(colleges)"
  },
  {
    "objectID": "06-visualization.html#exploring-attributes",
    "href": "06-visualization.html#exploring-attributes",
    "title": "4¬† Visualization",
    "section": "5.2 Exploring Attributes",
    "text": "5.2 Exploring Attributes\nData scientists and statisticians often start analyses by exploring attributes (i.e., variables) that are of interest to them. For example, suppose we are interested in exploring the admission rates of the institutions in the college scorecard data to determine how selective the different institutions are. We will begin our exploration of admission rates by examining different visualizations of the admissions rate attribute. There is not one perfect visualization for exploring the data. Each visualization has pros and cons; it may highlight some features of the attribute and mask others. It is often necessary to look at many different visualizations of the data in the exploratory phase.\nOne of the primary goals of any data visualization, especially those in this chapter, are to summarize (think, simplify) the data so that it can be more easily processed to understand key components of the attribute being explored. To be more explicit, it would be possible to explore all 2019 of the raw data to see the exact admission rate for each institution. However, if the goal is to know overall trends for the admission rates of institutions, knowing the exact values for each institution from the table would be too unwieldy. Instead, the goal of the data visualization to simplify the attribute to understand better the key components of the admission rate attribute. This is a trade-off, as there is a loss of information, but this loss of information is useful in this context as it allows for the summarization of the attribute.\n\n\n\n5.2.1 Histograms\nThe first visualization we will examine is a histogram. We can create a histogram of the admission rates using the gf_histogram() function. (This function is part of the ggformula package which needs to be loaded prior to using the gf_histogram() function.) This function requires two arguments. The first argument is a formula that identifies the variables to be plotted and the second argument, data =, specifies the data object that was assigned on data import. For example, earlier we used the read_csv() function to import the college scorecard data and we assigned this to the name, colleges. The syntax used to create a histogram of the admission rates is:\n\ngf_histogram(~ adm_rate, data = colleges)\n\n\n\n\n\n\nFigure¬†5.1: Histogram of college admission rates.\n\n\n\nThe formula we provide in the first argument is based on the following general structure:\n~ attribute name\nwhere the attribute name identified to the right of the ~ is the exact name of one of the columns in the colleges data object.\n\n\n\n5.2.2 Interpretting Histograms\nHistograms are created by collapsing the data into bins and then counting the number of observations that fall into each bin. To show this more clearly in the figure created previously, we can color the bin lines to highlight the different bins. To do this we include an additional argument, color =, in the gf_histogram() function. We can also set the color for the bins themselves using the fill = argument. Here we color the bin lines black and set the bin color to yellow.3\n\ngf_histogram(~ adm_rate, data = colleges, color = 'black', fill = 'yellow')\n\n\n\n\nFigure¬†5.2: Histogram of college admission rates. Here the color of the bin lines to black and fill in the bars with yellow.\n\n\n\n\nWhen looking at a single bar, for example the bar that is at 0.50, shows the number of institutions with admissions rates between about 0.48 and 0.52. In this case, there are about 80 institutions that have admissions rates between 0.48 and 0.52 (i.e., 48% to 52% admission rates). Similar interpretations are found for all of the other bars as well.\nOne common assumption made with a histogram is for the width of the bars be the same width on the attribute of interest. For example, the single bar interpreted in the preceding paragraph, the width of the bar was about 4% on the admission rate scale. Therefore, with the assumption that all bars have the same width, this would mean that all of the 30 bars in the histogram would each range by about 4%.\nRather than focusing on any one bin, we typically want to describe the distribution as a whole. For example, it appears as though most institutions admit a high proportion of applicants since the bins to the right of 0.5 have higher counts than the bins that are below 0.5. There are, however, some institutions that are quite selective, only admitting fewer than 25% of the students who apply.\n\n\n5.2.2.1 Adjusting Number of Bins\nInterpretation of the distribution is sometimes influenced by the width or number of bins. It is often useful to change the number of bins to explore the impact this may have on your interpretation. This can be accomplished by either (1) changing the width of the bins via thebinwidth = argument in the gf_histogram() function, or (2) changing the number of bins using the bins = argument.\nThe binwidth in the histogram refers to the range or width of each bin. A larger binwidth would mean there are fewer bins as it would take fewer bins to span the entire range of the attribute of interest. In contrast, a smaller binwidth would require more bins to span the entire range of the attribute of interest.\nIn contrast, the number of bins can be specified directly, for example 10 or 20 bins. The default within the R graphics package used is 30. Within this framework, each bin will have the same width or binwidth.\nThe relationship between the number of bins and binwidth could be shown with the following equation:\n\\[\nbinwidth = \\frac{attribute\\ range}{\\#\\ of\\ bins}\n\\]\nTo be more explicit, suppose that we wanted there to be 25 bins, using algebra we could compute the new binwidth given that we know we want 25 bins and knowing the range of the original attribute. The admission rates attribute have values as small as 0 and as large as 1. Therefore, the total range would be 1 (1 - 0 = 1). The binwidth could then be computed as:\n\\[\nbindwidth = \\frac{1}{25} = .04\n\\]\nIn contrast, if we wanted to specify the binwidth instead of the number of bins, we could do a little bit of algebra in the equation above to compute the number of bins needed to span the range of the attribute given the specified binwidth. For example, if we wanted the binwidth to be .025, 2.5%, we could compute this as follows:\n\\[\n\\#\\ of\\ bins = \\frac{1}{.025} = 40\n\\]\nWe will take the approach of letting the software compute these, but the equations above shows the general process that is used by the software in selecting the binwidth.\nMore bins/smaller binwidth can give a slightly more nuanced interpretation of the attribute of interest, whereas fewer bins/large binwidth will do more summarization. Having too few bins or too many bins can make the figure more difficult to interpret by missing key features of the attribute or including too many unique features of the attribute. For this reason, it is often of interest to adjust the binwidth or number of bins to explore the impact on the interpretation.\nThe code below changes the binwidth to specify it as .01 via the binwidth = .01 argument with the figure shown in Figure¬†5.3.\n\ngf_histogram(~ adm_rate, data = colleges, color = 'black', fill = 'yellow', binwidth = .01)\n\n\n\n\nFigure¬†5.3: Histogram of college admission rates. Here the binwidth has been changed to .01.\n\n\n\n\nThe code below specifies 10 bins via the bins = 10 argument with the figure shown in Figure¬†5.4.\n\ngf_histogram(~ adm_rate, data = colleges, color = 'black', fill = 'yellow', bins = 10)\n\n\n\n\nFigure¬†5.4: Histogram of college admission rates. Here the number of bins has been changed to 10.\n\n\n\n\nIn general, our interpretation remains the same across all the different binwidth/bins combinations, namely that most institutions admit a high proportion of applicants. When we used a bin width of 0.01, however, we were able to see that several institutions admit 100% of applicants. This was obscured in the other histograms we examined. As a data scientist these institutions might warrant a more nuanced examination."
  },
  {
    "objectID": "06-visualization.html#plot-customization",
    "href": "06-visualization.html#plot-customization",
    "title": "4¬† Visualization",
    "section": "5.3 Plot Customization",
    "text": "5.3 Plot Customization\nThere are many ways to further customize the plot we produced to make it more appealing. For example, you would likely want to change the label on the x-axis from adm_rate to something more informative. Or, you may want to add a descriptive title to your plot. These customizations can be specified using the gf_labs() function. Specific examples are given below.\n\n\n5.3.1 Axes labels\nTo change the labels on the x- and y-axes, we can use the arguments x = and y = in the gf_labs() function. These arguments take the text for the label you want to add to each axis, respectively. Here we change the text on the x-axis to ‚ÄúAdmission Rate‚Äù and the text on the y-axis to ‚ÄúFrequency‚Äù. The gf_labs() function is connected to the histogram by linking the gf_histogram() and gf_labs() functions with the pipe operator (|>).4\n\ngf_histogram(~ adm_rate, data = colleges, color = 'black', fill = 'yellow', bins = 25) |>\n  gf_labs(\n    x = 'Admission Rate',\n    y = 'Frequency'\n    )\n\n\n\n\nFigure¬†5.5: Histogram of college admission rates. Here we add custom x- and y-axis labels.\n\n\n\n\n\n\n\n5.3.2 Plot title and subtitle\nWe can also add a title and subtitle to our plot. Similar to changing the axis labels, these are added using gf_labs(), but using the title = and subtitle = arguments.\n\ngf_histogram(~ adm_rate, data = colleges, color = 'black', fill = 'yellow', bins = 25) |>\n  gf_labs(\n    x = 'Admission Rate',\n    y = 'Frequency',\n    title = 'Distribution of admission rates for 2,019 institutions of higher education.',\n    subtitle = 'Data Source: U.S. Department of Education College Scorecard'\n    )\n\n\n\n\nFigure¬†5.6: Histogram of college admission rates. Here we add a title and subtitle.\n\n\n\n\nPlot titles and subtitles are helpful to used to provide context to the figure and describe the overall purpose for the figure. For example, the subtitle in Figure @ref(fig:subtitle) describes the source for the data plotted.\n\n\n\n5.3.3 Plot theme\nBy default, the plot has a grey background and white grid lines. This can be modified to using the gf_theme() function. For example, in the syntax below we change the plot theme to a white background with no grid lines using theme_classic(). Again, the gf_theme() is linked to the histogram with the pipe operator.\n\ngf_histogram(~ adm_rate, data = colleges, color = 'black', fill = 'yellow', bins = 25) |>\n  gf_labs(\n    x = 'Admission Rate',\n    y = 'Frequency',\n    title = 'Distribution of admission rates for 2,019 institutions of higher education.',\n    subtitle = 'Data Source: U.S. Department of Education College Scorecard'\n    ) |>\n  gf_theme(theme_classic())\n\n\n\n\nFigure¬†5.7: Histogram of college admission rates. Here we change the plot theme."
  },
  {
    "objectID": "06-visualization.html#density-plots",
    "href": "06-visualization.html#density-plots",
    "title": "4¬† Visualization",
    "section": "5.4 Density plots",
    "text": "5.4 Density plots\nAnother plot that is useful for exploring attributes is the density plot. This plot usually highlights similar distributional features as the histogram, but the visualization does not have the same dependency on the specification of bins. Density plots can be created with the gf_density() function which takes similar arguments as gf_histogram(), namely a formula identifying the attribute to be plotted and the data object.5 If you compare the code specified for the very first histogram, notice that only the function name changed.\n\ngf_density(~ adm_rate, data = colleges)\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\n\nFigure¬†5.8: Density plot of college admission rates.\n\n\n\n\n\n\n5.4.1 Interpreting Density Plots\nDensity plots are interpreted similarly to a histogram in that areas of the density curve that are higher indicate more data in those areas of the attribute of interest. Places where the density curve are lower indicate areas where data occur infrequently. The density metric on the y-axis is not the same as the histogram, but the relative magnitude can be interpreted similarly. That is, higher indicates more data in that region of the attribute.\nJust like the histogram, the attribute being depicted in the density curve is on the x-axis. Therefore, important features for the attribute of interest can be found by looking at the y-axis, but then the place where high or low prevalence occurs are depicted by looking back to the x-axis. For example, when looking at the density curve in Figure¬†5.9, the density curve has a peak on the y-axis density scale of just under 2.0, the peak of this density curve occurs around a 0.75 as shown on the x-axis.\nOur interpretation remains that most institutions admit a high proportion of applicants. In fact, colleges that admit around 75% of their applicants have the highest probability density, indicating this is where most of the institutions are found in the distribution. Additionally, there are just a few institutions that are have an admission rate 25% or less.\nThe axis labels, title, subtitle can be customized with gf_labs() in the same manner as with the histogram. The color = and fill = arguments in gf_density() will color the density curve and area under the density curve, respectively.\n\ngf_density(~ adm_rate, data = colleges, color = 'black', fill = 'yellow') |>\n  gf_labs(\n    x = 'Admission Rate',\n    y = 'Probability density',\n    title = 'Distribution of admission rates for 2,019 institutions of higher education.',\n    subtitle = 'Data Source: U.S. Department of Education College Scorecard'\n    )\n\n\n\n\nFigure¬†5.9: Density plot of college admission rates."
  }
]