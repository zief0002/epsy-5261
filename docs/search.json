[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Data Analysis and Randomization Methods",
    "section": "",
    "text": "Foreword\nThis work in progress is ultimately going to be the primarily textbook resource for EPSY 5261 students. (Note: If you want to contribute to this, create a Pull Request or send me an email.) Also, feel free to offer criticism, suggestion, and feedback. You can either open an issue on the book‚Äôs github page or send me an email directly."
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "An Introduction to Data Analysis and Randomization Methods",
    "section": "Colophon",
    "text": "Colophon\nArtwork by @allison_horst\nIcon and note ideas and prototypes by Desir√©e De Leon.\nThe book is typeset using Crimson Text for the body font, Raleway for the headings and Sue Ellen Francisco for the title. The color palette was generated using coolors.co.\nStatistical Computing\n\nLaptop icon made by Tomas Knop from www.flaticon.com\nDirectory icon made by Darius Dan from www.flaticon.com\nBrain icon made by Aranagraphics from www.flaticon.com\nInternet icon made by Freepik from www.flaticon.com"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "An Introduction to Data Analysis and Randomization Methods",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "01-statistical-computation.html",
    "href": "01-statistical-computation.html",
    "title": "Statistical Computation",
    "section": "",
    "text": "The first set of tools we will discuss will be related to statistical computation. Although there are many computational tools for statistical analysis, the first tools we will add to your computational toolkit is R. R is a free software environment for statistical computing and graphics. It can be installed on a variety of operating systems, including the MacOS, Windows, and UNIX platforms. To really make use of the computational power of R, we are also going to introduce you to RStudio, an open-source front-end1 to R.\nThe initial chapters of this document will address:\n\nInstalling R and RStudio;\nGetting started with R‚Äôs computational syntax;\nWrangling data using functions from the dplyr package; and\nVisualizing data using functions from the ggplot2 package.\n\n\n\n\n\n\n\nSpecifically, RStudio is branded as an ‚Äúintegrated development environment (IDE) [that] includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.‚Äù‚Ü©Ô∏é"
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#installing-r",
    "href": "01-01-r-and-rstudio-installation.html#installing-r",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.1 Installing R",
    "text": "1.1 Installing R\nTo install R, navigate your web browser to:\n\nhttps://www.r-project.org/\n\nThen,\n\nClick the CRAN link under Download on the left-hand side of the page.\nSelect a mirror site. These should all be the same, but I tend to choose the Iowa State University link under USA.1\nIn the Download and Install R box, choose the binary that matches the operating system (OS) for your computer.\n\nThis is where the installation directions diverge depending on your OS.\nMac Instructions\nSo long as you are running MacOS 10.13 or higher just click the first link for the PKG, which will download the installer for the most current version of R (4.1.1 as of August 16, 2021). Once the download completes, open the installer and follow the directions to install R on your computer.\nIf you are running an older version of MacOS, you will have to install an older version of R. You can find these links under the Binaries for legacy OS X systems heading further down the install page. Click the appropriate PKG link for R your version of MacOS. Once the download completes, open the installer and follow the directions to install R on your computer.\nIf you are unsure which version of the MacOS is running on your computer, select About this Mac from the Apple menu in your toolbar.\nWindows Instructions\nClick the link that says Install R for the first time (or click base; they go to the same place). Then click the Download R 4.1.1 for Windows link, which will download the installer for the most current version of R (4.0.2 as of July 24, 2020). Once the download completes, open the installer and follow the directions to install R on your computer.\nLinux Instructions\nIf you are running Linux, you should know how to install things on your computer. üôã"
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#installing-rstudio-desktop",
    "href": "01-01-r-and-rstudio-installation.html#installing-rstudio-desktop",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.2 Installing RStudio Desktop",
    "text": "1.2 Installing RStudio Desktop\nAfter you have installed R, you next need to install RStudio Desktop. To do this, navigate your web browser to:\n\nhttps://rstudio.com/products/rstudio/download/\n\nThen,\n\nSelect the blue Download button under the free, open-source version of RStudio Desktop.\nSelect the installer associated with your computer‚Äôs OS.\nOnce the download completes, open the installer and follow the directions to install RStudio Desktop on your computer."
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#checking-that-things-worked",
    "href": "01-01-r-and-rstudio-installation.html#checking-that-things-worked",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.3 Checking that Things Worked",
    "text": "1.3 Checking that Things Worked\nFrom your Applications or Programs folder, open RStudio. If you have successfully downloaded both programs, this should open the application and you should see a message indicating that you are using ‚ÄúR version 4.1.1‚Äù (or whichever version of R you installed) in the console pane.\n\n\n\n\n\nOnce you open RStudio, you should see a message indicating that you are using R version 4.1.1 (or whichever version of R you installed) in the console pane. Here the console pane is on the left-side, but it may be in a different location for you. Your RStudio may also have a white background rather than the black background seen here."
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#customizing-rstudio",
    "href": "01-01-r-and-rstudio-installation.html#customizing-rstudio",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.4 Customizing RStudio",
    "text": "1.4 Customizing RStudio\nWhile the information in this section is not crucial for making things work, it is useful to get RStudio looking good and setting some default settings. Open the Tools > Options menu (Windows) or RStudio > Preferences (Mac).\n\n\n\n\n\nThe RStudio options/preferences menu has many settings to customize RStudio.\n\n\n\n\n\nIn the General > Basic settings, change the option on Save workspace to .Rdata on exit to be ‚ÄúNever‚Äù. Click the ‚ÄúApply‚Äù button.\nIn the Appearance settings, customize the look of RStudio to something aesthetically appealing to you. When you are finished, click the ‚ÄúApply‚Äù button.\nThere are also options you can set in the Accessibility settings if you use a screen reader. If you change anything, don‚Äôt forget to click the ‚ÄúApply‚Äù button.\n\nWhen you are finished customizing RStudio, click the ‚ÄúOK‚Äù button."
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#install-rtoolscommand-line-tools",
    "href": "01-01-r-and-rstudio-installation.html#install-rtoolscommand-line-tools",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.5 Install Rtools/Command Line Tools",
    "text": "1.5 Install Rtools/Command Line Tools\nYou may need to install some additional functionality to your system in order to get certain packages to install or load properly. On a Windows machine, you might need to install Rtools. Mac users might need to add the Command Line Tools. These tools also allow you to write and compile your own R packages. RStudio has well written instructions for adding these tools at: https://support.rstudio.com/hc/en-us/articles/200486498-Package-Development-Prerequisites."
  },
  {
    "objectID": "02-data.html",
    "href": "02-data.html",
    "title": "Data",
    "section": "",
    "text": "The American Statistical Association defines statistics as, ‚Äúthe science of learning from data, and of measuring, controlling and communicating uncertainty‚Äù (American Statistical Association, 2023). The methods you learn throughout this textbook, and the EPSY 5261 course, will help you to learn from data and to measure, control, and communicate about uncertainty.\nAn important learning goal of statistics is therefore to understand the vocabulary and ideas related to data. To this end, in Chapter¬†2 you will learn about the structure of data and attribute classification. You will also learn how to judge the quality of data including questions you should ask about the data. ?sec-importing-data will introduce the syntax we use to import data into R. Finally, ?sec-design will introduce how the collection of the data impacts the type of inferences and conclusions we can draw.\n\n\n\n\nAmerican Statistical Association. (2023). ASA newsroom. Website. https://www.amstat.org/asa-newsroom"
  },
  {
    "objectID": "02-01-data-structure-and-attributes.html#classifying-attributes",
    "href": "02-01-data-structure-and-attributes.html#classifying-attributes",
    "title": "2¬† Data Structure and Attribute Classification",
    "section": "2.1 Classifying Attributes",
    "text": "2.1 Classifying Attributes\nOur ultimate goal is often to analyze the data we have to learn from it. For example, in our NYT Best Seller data, we may be interested in the proportion of authors that identify as female. Or, we may want to. know how many weeks a book stays on the Best Sellers list. The type of analyses we can do, however, depend on the type of attributes we have.\nWe typically classify attributes as either categorical attributes or quantitative attributes. These classifications are based on the type of information (data) in the attribute. A categorical attribute has values that represent categorical (or qualitative) differences between the cases, whereas a quantitative attribute represents numerical (or quantitative) differences between cases. For example, in the NYT Best Seller data, title and author are categorical variables, whereas year, and total number of weeks the book was on the NYT Best Sellers list are quantitative attributes.\nTypically attributes that have numerical values are quantitative, but not always. In our data, consider the attribute that indicates whether the author identifies as a female. Although the values in the data are numeric, these numbers actually represent different categories: 0 = no (not female) and 1 = yes (female). Therefore, this attribute is actually a categorical attribute, not a quantitative attribute.\nOne check of whether anattribute is actually quantitative is whether numeric computations, such as finding an average of the attribute, can be carried out and the result makes conceptual sense. For example, we cannot compute the mean author value (it is thus a categorical attribute). If we compute the mean of the female attribute we get a result, but it does not indicate anything about the gender identity of a NYT best selling author. The mean does not make conceptual sense and thus we classify female as a categorical attribute.\n\nYour Turn\nClassify the best_rank attribute (the book‚Äôs highest rank while it was on the NYT Best Sellers list) as either categorical or quantitative. Explain.\n\nShow/Hide Solution\n\n\nThe attribute best_rank is a quantitative attribute. The data in this attribute are numeric values, and it makes conceptual sense to compute summaries such as the mean for this attribute.\n\n\n\n\n2.1.1 Further Classifications of Attributes\nWhile categorizing each attribute as categorical or quantitative is a good first step, statisticians and data analysts often go a step further and classify attributes based on their scale of measurement. This classification is based on how attributes were measured (i.e., how we assign numerals to objects of events) and what this implies about the empirical statements we can make about the constructs measured on that particular scale. The most common taxonomy for this was described by Stevens (1946) who classified four scales of measurement: nominal, ordinal, interval, or ratio (NOIR). Below we describe each of these scales of measurement:\nNominal Scale of Measurement: In nominal scales of measurement, any numerals assigned to different values would only be useful as labels, for distinguishing or organizing values. Most categorical attributes have this scale of measurement. For example, in the NYT bests sellers data, the numerical values in the gender identity attribute (female) are only useful as labels and for distinguishing authors who identify as female and authors who don‚Äôt. Because of this, only the following type of statement would be meaningful:\n\nIn the NYT bestseller data, more authors identify as female (15) than do not identify as female (10).\n\nThe only type of empirical statements we can make are comparisons of the number of cases between different labels (e.g., counts, percentages).\nOrdinal Scale of Measurement: Data measured using an ordinal scales of measurement, is still categorical. It has all the features of nominal measured data (e.g., labeling, distinguishing, organizing), but we can also rank order the values in a meaningful way. A classic example of the ordinal scale of measurement is in the 5-star review rating used on sites like Amazon. All of these statements would be meaningful:\n\nThere are more 4-star reviews than 5-star reviews (comparison of counts).\nA review of 3 stars is better than a review of 2 stars (rank ordering).\n\nWith this scale of measurement, it is reasonable to not only provide counts of the different values (e.g., the number of 5-star reviews), but now because there is a rank ordering, we can also make empirical statements related to the rank ordering of the measured construct based on the numeral values. In nominal level data, these latter types of statements are not appropriate since the values for the labels are arbitrary. For example, authors with a gender identity of female were assigned a 1 and others were assigned a 0. This is arbitrary in that authors identifying as female could just as easily have been assigned a value of 0 and those that didn‚Äôt identify as female a value of 1. This implies that even though the numeral 1 is greater than the numeral 0, the attribute values associated with these numerals (identifying as female or not) do not have any meaningful rank order. Because of this, saying something about one identity being greater than or less than another is inappropriate.\nInterval Scale of Measurement: In interval level data, the rank order of the numbers assigned to attribute values is meaningful, similar to ordinal data. Moreover, the difference between consecutive values represents the same amount of difference on the underlying attribute. For example, consider the Fahrenheit temperature scale. All of these statements would be meaningful:\n\nThere are more 30 degrees F days than 0 degrees F days (comparison of counts).\nA day that is 10 degrees F is warmer than a day that is 9 degrees F (rank ordering).\nThe difference in temperature when you are comparing days that are 9 and 10 degrees F is the same as when you are comparing days that are 0 and 1 degrees F. (interval comparison).\n\nThe critical component is that on an interval scale, the difference in consecutive numerals has the same level of difference in the construct being measured, regardless of scale location. Again consider our 5-star rating system. The difference between a 1- and 2-star review is not the same as the difference between a 4- and 5-star review. While the numbers themselves have a constant difference, the difference in the amount of the underlying construct (satisfaction, happiness with the product, etc.) does not. With interval level scales we can compute summaries like means, standard deviations, and correlations.\nRatio Scale of Measurement: With attributes that have the ratio scale of measurement, the rank ordering of the numbers assigned to attribute values is meaningful, the differences between consecutive numerals indicates the same amount of difference in the underlying construct being measured, and ratio type statements about these differences are also meaningful. For example, the amount of snowfall is an attribute on the ratio scale of measurement. All of these statements would be meaningful:\n\nThere are more days with 0 inches of snowfall than days with 10 inches of snowfall (comparison of counts).\nA day with 8 inches of snowfall got more snow than a day with 4 inches of snowfall (rank ordering).\nThe difference in snowfall when you are comparing days with 8 inches of snowfall and 7 inches of snowfall is the same as when you are comparing days with 15 and 16 inches of snow (interval comparison).\nA day with 8 inches of snow got twice the amount of snow as a day with 4 inches of snow (ratio comparison).\n\nGoing back to our temperature scale, we cannot make these ratio type statements. For example, a day that is 60 degrees F is not twice as warm as a day that is 30 degrees F. This is because the Fahrenheit scale does not have a ‚Äútrue‚Äù zero value. (Zero degrees F does not indicate absence of temperature.) Whereas, in our snowfall attribute, a day with 0 inches of snow does indeed indicate no snow fell on that day.\nAside from the type of empirical statements we can make, the level of measurement also puts limits on the type of statistical analyses that are appropriate.\n\n\n\n\n\n\nTable¬†2.1:  The four measurement scales and the types of empirical statements and statistical summaries that are appropriate for each scale. \n  \n  \n    \n      Scale\n      Empirical Statement\n      Statistical Summaries\n    \n  \n  \n    Nominal\n\nComparison of counts\n\nCounts, percentages\n\n    Ordinal\n\nComparison of counts  Rank ordering\n\nCounts, percentages  Median, percentiles\n\n    Interval\n\nComparison of counts  Rank ordering  Interval comparisons\n\nCounts, percentages  Median, percentiles  Mean, standard deviation\n\n    Ratio\n\nComparison of counts  Rank ordering  Interval comparisons  Ratio comparisons\n\nCounts, percentages  Median, percentiles  Mean, standard deviation  Coefficient of variation"
  },
  {
    "objectID": "02-01-data-structure-and-attributes.html#how-were-the-data-generated",
    "href": "02-01-data-structure-and-attributes.html#how-were-the-data-generated",
    "title": "2¬† Data Structure and Attribute Classification",
    "section": "2.2 How Were the Data Generated?",
    "text": "2.2 How Were the Data Generated?\nAnother question that has direct implications on the methods used in data analysis is: How were the data generated or collected? Were they collected from a survey? Were they mined from the web? Were they generated as part of an experimental study? Knowing the answer to these questions also is important for the degree to which we can draw conclusions from the analysis.\nUnderstanding how the data were generated allows us to determine whether the data we have constitute a sample of cases or the entire population of cases we are interested in learning about. Importantly, whether you have a sample or the entire population depends on how you define the cases/observations you are interested in drawing conclusions about.\n\nA population includes all cases/observations of interest, whereas a. sample includes a subset of cases from the population.\n\nFor example, consider a child psychologist who wants to draw conclusions about all students at a particular school in Minnesota. To do this, she collects data from every student in that school. Since her data includes every case (student) she is interest in drawing conclusions for, her data would be a population, Now consider a second child psychologist who is interested in drawing conclusions about all students in Minnesota. This psychologist also collects data from every student in the same school as the first psychologist. This second psychologist‚Äôs data would be considered a sample since the cases they included in their data are only a subset of the cases they want to draw conclusions about.\n\nYour Turn\nIs the New York Time best sellers data a population or a sample? Explain.\n\nShow/Hide Solution\n\n\nThe New York Time best sellers data is a sample since it is only a subset of all the New York Times best selling books.\n\n\n\n\n2.2.1 Drawing Conclusions from a Sample\nIn practice, we rarely have data collected from an entire population, but we still want to use the data we have in our sample to draw conclusions about that population. Drawing conclusions about an entire population when you only have data from a subset cases is what statisticians call statistical inference.\n\n\n\n\n\nFigure¬†2.1: A sample of data is drawn from the population. Information from the sample is then analyzed and used to make a statistical inference about the population.\n\n\n\n\nThis can be a very tricky thing to do since the sample does not give us complete information about the population. As an example, consider if you wanted to figure out the average monthly living expenses for all graduate students at the University of Minnesota. To do this you collect data on the monthly living expenses for the students in your EPSY 5261 class and compute the average monthly living expense based on the data you collected and use that value as a guess for the average monthly living expenses for all graduate students at the University of Minnesota. (Note that the cases in your data (students in your EPSY 5261 class) are a subset of the population you want to draw conclusions about (all graduate students at the University of Minnesota) and thus are a sample.)\n\nSummaries computed from the population are referred to as parameters and summaries computed from a sample are referred to as statistics.\n\nIn statistical inference the statistics we compute from a sample are estimates for the population parameters that we are interested in. The word ‚Äúestimate‚Äù may have clued you in that the value of a statistic is generally not equal to the value of the parameter. In our example, the average monthly living expenses we computed based on your sample of cases is probably not the same as the average monthly living expenses for all graduate students at the University of Minnesota. This is because our sample only includes data for some (not all) of the cases.\nWe don‚Äôt expect the value of the statistic to be the same as that for the parameter we are trying to estimate, but a key question is: Is the value of the statistic a reasonable estimate of the parameter? The answer to this question can sometimes be difficult to answer. What do we mean by reasonable? In statistical analysis, there are a few ways that we consider reasonableness of an estimate. We will explore these below.\n\n\n2.2.1.1 Sampling Error: Quantifying the Amount of Uncertainty in our Sample Estimate\nOne way we consider whether an answer is reasonable is how much uncertainty we have in the estimate from our sample. Imagine if you repeated the study, but this time, you collected data on the monthly living expenses in a different section of EPSY 5261. The average computed from these data would likely be different from the average you computed from your section of EPSY 5261, and therefore your guess for the average monthly living expenses for all graduate students at the University of Minnesota would be different. This is because you would have different cases in your sample.\n\nWhen using a sample to infer about a population, our guesses or estimates vary depending on the cases in our sample. This means that when we make inferences there is always some degree of uncertainty in our estimates.\n\nThe idea that estimates from samples vary depending on the cases in your sample is well known and is referred to as sampling error. In carrying out statistical inference, we not only acknowledge that we have uncertainty in our guess from the sample data, but we also try and quantify how much uncertainty there is in that estimate. For example, do we think that the average monthly living expenses for all graduate students at the University of Minnesota is within a few dollars of our sample estimate? Or do we think that it is within a few hundred dollars of our sample estimate? By providing this estimate of the uncertainty, it lets other people know ‚Äúhow reasonable‚Äù our guess might be.\n\n\n\n\n\nFigure¬†2.2: Estimates for the mean living expense for all graduate students at the University of Minnesota will vary from sample to sample because of sampling error. In statistical inference this is expected and quantifying the amount of sampling error gives us an indication of how much uncertainty we have in our estimate.\n\n\n\n\n\n\n\n2.2.1.2 Sampling Bias: Does the Sample Represent the Population?\nA second way we consider whether an answer is reasonable is to consider whether our sample of cases is representative of the population as a whole. In our example, we are asking the question of whether the students in your section of EPSY 5261 are representative of all graduate students at the University of Minnesota when it comes to living expenses. This is a really difficult question to answer, but generally (unless you have selected your sample randomly from the population), your sample is not representative. The key here is that the sampling method (how you chose your cases) matters!\n\nWhen a sample is not randomly selected from the population we say that the sampling method is biased.\n\nA biased sampling method leads to systematically wrong answers. For example, again say you were interested in determining the average monthly living expenses for all graduate students at the University of Minnesota. This time, your sampling method is to collect data about the monthly living expenses from all the graduate students who live in a particular apartment building in Downtown Minneapolis. Would these students‚Äô living expenses be representative of all graduate students at the University of Minnesota?\nAgain, probably not. The living expenses in Downtown Minneapolis are different (generally higher) than the living expenses of students who live in Dinkytown or one of the suburbs. Because the cases in your sample all come from the same apartment building in Downtown Minneapolis, their average monthly living expense will be systematically higher than the average monthly living expenses for all graduate students at the University of Minnesota.\nWhat about our original sampling method of collecting data from each of the graduate students in your EPSY 5261 section? While these students might live in different areas, and seem more representative, this sampling method is likely still biased. Even if we have a hard time identifying how, the estimate for the average monthly living expenses based on students in EPSY 5261 is likely systematically different than the average monthly living expenses for all graduate students at the University of Minnesota. (It may be systematically too low, or too high.)\n\nThe only sampling method that is guaranteed to be unbiased (and therefore representative) is to select your sample randomly from the population.\n\n\n\n\n\n2.2.2 Random Sampling\nThere are many methods for randomly selecting a sample from the population. The simplest method that incorporates randomness into the sampling process is Simple Random Sampling. In simple random sampling each case in the population has an equal probability of being selected into the sample.1\n\nIn the discipline of statistics, there are words that we use that have very different meanings from their use in colloquial English. ‚ÄúRandom‚Äù is one of those words. In our everyday language ‚Äúrandom‚Äù might mean happenstance or unexpected. For example: It was so random that I saw Ferris Bueller at the 31 Flavors last night. In statistics, ‚Äúrandom‚Äù does not mean happenstance at all. Random sampling is quite formal in ensuring that cases have a specified probability of being selected into the sample.\n\nOne of the most compelling and useful results in statistics is that a simple random sample is representative of the population, and moreover that even small samples that are selected with this method can be representative of very large populations. This is powerful!\nBut, it can sometimes be very difficult to draw a simple random sample in practice. For one thing, it requires that you have a list of all the cases in the population (called a sampling frame). This allows you to make sure that everyone in the population has the same probability of being selected. While it might be possible to obtain a list of all graduate students enrolled at the University of Minnesota, it is another thing to obtain a list of all people living in Minnesota. Or even a list of people living in Dinkytown. Depending on your population of interest you may not be able to get a simple random sample.2\n\nYour Turn\nWhat is the sampling method for the New York Time best sellers data. Based on this method, are the estimates of the population parameters we compute from these data going to be biased or unbiased?\n\nShow/Hide Solution\n\n\nThe New York Time best sellers data was sampled using simple random sampling. Because the sampling method employed randomness, the estimates we compute from the data are unbiased estimates of the population parameters."
  },
  {
    "objectID": "02-01-data-structure-and-attributes.html#summary",
    "href": "02-01-data-structure-and-attributes.html#summary",
    "title": "2¬† Data Structure and Attribute Classification",
    "section": "2.3 Summary",
    "text": "2.3 Summary\nEvery time you encounter data, you should identify the cases and attributes in the data. Understanding the cases, especially in relation to the cases you want to draw conclusions about, helps you identify whether you have a sample, or the entire population. Classifying the attributes helps you think about the type of analysis you can undertake. If your data are a sample (rather than a population), you also need to ask how they were collected. Were they collected using randomness in the sampling method? Or is the sampling method used to collect the data biased?"
  },
  {
    "objectID": "02-01-data-structure-and-attributes.html#references",
    "href": "02-01-data-structure-and-attributes.html#references",
    "title": "2¬† Data Structure and Attribute Classification",
    "section": "2.4 References",
    "text": "2.4 References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGould, R., & Ryan, C. (2013). Introductory statistics: Exploring the world through data. Pearson.\n\n\nPruett, J. (2021). NYT hardcover fiction bestsellers. Post45 Data Collective, V1. https://doi.org/https://doi.org/10.18737/CNJV1733p4520220211\n\n\nStevens, S. S. (1946). On the theory of scales of measurement. Science, 103(2684), 677‚Äì680."
  },
  {
    "objectID": "03-exploring-and-describing-data.html#goals-for-summarization-and-visualization",
    "href": "03-exploring-and-describing-data.html#goals-for-summarization-and-visualization",
    "title": "Summarizing and Visualizing Data",
    "section": "Goals for Summarization and Visualization",
    "text": "Goals for Summarization and Visualization\nData scientists and statisticians visualize data and compute numerical summaries to explore and understand data. In addition to visualizing distributions of data, it is common to also summarize certain feature of the data using numbers. (For example, the mean is one summarization of a distribution of quantitative data.) Together visualizing and summarizing data can help analysts identify features in the data such as typical or extreme observations, and also describe and explore the variation in the data. Data exploration is an important first step in any statistical analysis."
  },
  {
    "objectID": "03-exploring-and-describing-data.html#college-scorecard-data",
    "href": "03-exploring-and-describing-data.html#college-scorecard-data",
    "title": "Summarizing and Visualizing Data",
    "section": "College Scorecard Data",
    "text": "College Scorecard Data\nThroughout the chapters in this section we will use the College Scorecard data to illustrate the methods of data exploration. These data were collected and made available by the U.S. Department of Education (DOE). The DOE publishes data on institutions of higher education in their College Scorecard to facilitate transparency and provide information for interested stakeholders (e.g., parents, students, educators). A subset of this data is provided in the file college-scorecard.csv.\n\nCSV File\nData Codebook"
  },
  {
    "objectID": "03-01-categorical-attributes.html#hypothetical-example-pet-ownership",
    "href": "03-01-categorical-attributes.html#hypothetical-example-pet-ownership",
    "title": "3¬† Summarizing and Visualizing Categorical Attributes",
    "section": "3.1 Hypothetical Example: Pet Ownership",
    "text": "3.1 Hypothetical Example: Pet Ownership\nImagine you have surveyed 10 pet owners about the type of pet they own.1 The data you collected is shown in Figure¬†3.1.\n\n\n\n\n\nFigure¬†3.1: Data collected from 10 pet owners about the type of pet the own.\n\n\n\n\nOur goal in exploratory analysis is to describe the data. One way of describing the data is to list all of the values. For example here we could say the sample included a turtle, a fish, a cat, a dog, another dog, another fish, another cat, another cat, another cat, and another dog. While this is an accurate description, it isn‚Äôt very generalizable. (Imagine trying to describe the data from 1000 pet owners or 10,000 pet owners!)\n\n\n3.1.1 Numerically Summarizing Categorical Attributes\nA more natural way to describe these data is to summarize them by providing counts of each pet type. For example, describing our sample data using counts:\n\n1 of the pet owners sampled owned a turtle,\n2 of the pet pet owners sampled owned a fish,\n3 of the pet pet owners sampled owned a dog, and\n4 of the pet pet owners sampled owned a cat.\n\nSummarizing each type of pet owned by reporting counts of them is a much more natural way of describing the data. (This is also useful when the sample size is much larger.)\nAnother summary that could be used to describe this sample is to give the proportion of each type of pet owned. To compute the proportion, we take the count of each type of pet owned, and divide it by the total sample size.\n\\[\n\\mathrm{Proportion} = \\frac{\\mathrm{Count~of~Pet~Type}}{\\mathrm{Total~Sample~Size}}\n\\]\nFor example, to compute the proportion of pet owners in our sample that owned a dog, we use:\n\\[\n\\mathrm{Proportion~of~Dogs} = \\frac{3}{10} = 0.30\n\\]\n\nProportions will always be between 0 and 1. If you add all of the proportions of each category together you will get 1, so long as values can only belong to one category.\n\nDescribing our sample data using proportions:\n\n0.10 of the pet owners sampled owned a turtle,\n0.20 of the pet pet owners sampled owned a fish,\n0.30 of the pet pet owners sampled owned a dog, and\n0.40 of the pet pet owners sampled owned a cat.\n\nThe count and proportion values are often reported in a table, especially if there are more than a couple values in the categorical attribute. Table¬†3.1 is an example table indicating the counts and proportions of values in our hypothetical pet example.\n\n\n\n\n\n  \n  Table¬†3.1:  Counts and proportions of pet owners who own each type of pet. \n  \n  \n    \n      Pet\n      Count\n      Proportion\n    \n  \n  \n    \n3\n0.30\n    \n4\n0.40\n    \n2\n0.20\n    \n1\n0.10\n  \n  \n  \n\n\n\n\n\n\n\n\n3.1.2 Visualizing Categorical Attributes\nTo visualize categorical attributes we typically use a bar chart. Figure¬†3.2 shows a bar chart of the pet data.\n\n\n\n\n\nFigure¬†3.2: Bar chart indicating the counts of each type of pet owned.\n\n\n\n\nA bar chart (also known as a bar graph) shows a bar for each category of the categorical attribute. In our example, we have four bars, one for each pet type. The height of the bar indicates the count of each category. For example, the bar for cats has a height of four on the y-axis.\nSometimes the axes in the bar chart are transposed; categories are placed on the y-axis and counts on the x axis. Also, you might see a bar chart indicating proportions rather than counts. Figure¬†3.3 shows a transposed bar chart indicating the proportion of each pet type.\n\n\n\n\n\nFigure¬†3.3: Bar chart indicating the counts of each type of pet owned. In this plot the categories are placed on the y-axis and the scale on the the x axis indicates proportions rather than counts.\n\n\n\n\n\nWhen proportions are used in a bar chart, it is coventional to extend that axis from 0 to 1 (the range of potential proportions).\n\n\n\n3.1.2.1 Bar Chart Variations\nThere are several variations on the bar chart that you may see in practice. For example, the segmented bar chart is a variation of the bar chart. This variation of the plot, which always uses proportions rather than counts, has a single bar that is split into segments‚Äîone for each category. A segmented bar chart summarizing the pet data is shown in Figure¬†3.4.\n\n\n\n\n\nFigure¬†3.4: Segmented bar chart indicating the proportion of each type of pet owned.\n\n\n\n\nAnother variation of the bar chart is the donut chart. A donut chart is simply a segmented bar chart that is presented in a circular layout. Figure¬†3.5 presents a donut chart summarizing the pet data. Because there is no axis to indicate the proportion of each category in a donut chart, it is conventional to indicate the percentages of each category in the plot. Here percentages are used rather than proportions.\n\n\n\n\n\nFigure¬†3.5: Donut chart indicating the percentage of each type of pet owned. A donut chart is simply a segmented bar chart presented in a circular layout.\n\n\n\n\n\n\n\n3.1.2.2 Pie Charts\nOne last plot used to visualize summaries of categorical dat is the pie chart. Figure¬†3.6 shows a pie chart summarizing the pet data. Unlike any of the bar charts that we have looked at, a pie chart uses the proportional area of a circle to indicate the proportion or percentage of each pet type rather than the length of a bar.\n\n\n\n\n\nFigure¬†3.6: Pie chart indicating the percentage of each type of pet owned. A pie chart uses the proportional area of a circle to indicate the proportion or percentage of each pet type.\n\n\n\n\n\nIn a pie chart, each section of the pie is determined by proportionally dividing the 360¬∞ of the circle based on the data, and then making each section have the computed angle. For example the proportion of pet owners who have a cat is 0.4, and 0.4 of 360¬∞ is 144¬∞‚Äîso the cats section has an angle of 144¬∞. While these computations and the drawing of the pie chart would be done by the computer, it has implications for interpretations. Namely, research has suggested that humans may not be as adept at making accurate comparisons involving angles and the areas of sections based on those angles. Because of this, the recommendation from the data visualization community is to use bar charts rather than pie charts when displaying summaries of categorical data."
  },
  {
    "objectID": "03-01-categorical-attributes.html#using-r-to-numerically-summarize-categorical-attributes",
    "href": "03-01-categorical-attributes.html#using-r-to-numerically-summarize-categorical-attributes",
    "title": "3¬† Summarizing and Visualizing Categorical Attributes",
    "section": "3.2 Using R to Numerically Summarize Categorical Attributes",
    "text": "3.2 Using R to Numerically Summarize Categorical Attributes\nTo illustrate how we can summarize and visualize categorical attributes using R, we will use the college-scorecard.csv data. As a reminder, we will start by loading three libraries: {tidyverse}, {ggformula}, and {mosaicCore}. Then we will import the college scorecard data using the read_csv() function and assign the data into an object called colleges. Finally, we view the data to make sure it read in properly.\n\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaicCore)\n\n# Import data\ncolleges <- read_csv(\"https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/college-scorecard.csv\")\n\n# View data\ncolleges\n\n\n\n  \n\n\n\nRecall that in these data each case in these data is an institution of higher education and there are 234 institutions in our sample. This dataset has several categorical attributes, including: state, region, type of institution, and control.\nThe first attribute we will summarize and visualize is the control variable. Looking at the data codebook we find that this attribute can take on three different values: Public, Private nonprofit, and Private for-profit. Our first task is to get institution counts for each category of control. To do this, we will use the df_stats() function. The general syntax to obtain these counts is shown below.\n\n# General syntax to counts the categories in a categorical attribute\ndf_stats( ~ attribute_name, data = data_name, counts)\n\nWe first need to identify the name of the categorical attribute. To tell df_stats() that this is an attribute, we place a tilde (~) in front of the attribute name. Then we use the data= argument to identify the data object that includes our categorical attribute. Finally, we use counts to indicate that we want to compute the category counts.\n\nIt is a good idea to learn how to read R syntax. The tilde can be read as ‚Äúmodel‚Äù. So the general syntax above is read as, ‚Äúmodel the attribute_name found in the data_name data by counting the categories in the attribute‚Äù.\n\nPutting this into practice to count the categories in the control attribute which is in our colleges data:\n\n# Syntax to count the categories in the control attribute\ndf_stats(~control, data = colleges, counts)\n\n\n\n  \n\n\n\nIf we were to read this syntax, ‚Äúmodel the control attribute found in the colleges data by counting the categories in the attribute‚Äù.\nBased on the counts, we find that most of the institutions of higher learning in our sample are private nonprofit institutions (n = 146). There are also several public institutions of higher learning in our sample (n = 71). Lastly, there are also a few private for-profit institutions of higher learning in our sample (n = 13).\n\nIt is common to use n or N to denote sample size. Some textbooks and authors will use N to indicate the overall sample size (e.g., in the college scorecard data, N = 234) and n to indicate the sample size of subgroups within the sample (e.g., n = 71 for public institutions). Other authors might use n to define the overall sample size (e.g., n = 234) and then use subscripts on n to denote the sample size of different groups (e.g., \\(n_{\\mathrm{Public}}=71\\)). There is not a single unified agreed upon way to denote these things.\n\nWe also might want to compute the proportions for each category of control. To do this, we can again use the function df_stats(), but instead of providing counts we will provide props.\n\n# Syntax to compute the proportion for each category in the control attribute\ndf_stats(~control, data = colleges, props)\n\n\n\n  \n\n\n\nThese proportions (which can also be turned into percentages2) tell a similar story to what the counts did. Most of the institutions in our sample are private nonprofit (63.5%) and public (30.9%) institutions. There is a smaller percentage of institutions that are private for-profit (5.7%).3 As we did with the numerical summaries of the pet data, we can include these values in a table.\n\n\n\n\n\n  \n  Table¬†3.2:  Counts and proportions of institutions of higher learning by control. \n  \n  \n    \n      Control\n      Count\n      Proportion\n    \n  \n  \n    Public\n71\n0.309\n    Private Nonprofit\n146\n0.635\n    Private For-Profit\n13\n0.057\n  \n  \n  \n\n\n\n\n\n\n\nYour Turn\nWrite the syntax to compute the counts of the region attribute.\n\nShow/Hide Solution\n\n\n\ndf_stats(~ region, data = colleges, counts)\n\n\n\n  \n\n\n\n\nWrite the syntax to compute the proportions of the region attribute.\n\nShow/Hide Solution\n\n\n\ndf_stats(~ region, data = colleges, counts)"
  },
  {
    "objectID": "03-01-categorical-attributes.html#creating-a-bar-chart-to-summarize-counts-using-r",
    "href": "03-01-categorical-attributes.html#creating-a-bar-chart-to-summarize-counts-using-r",
    "title": "3¬† Summarizing and Visualizing Categorical Attributes",
    "section": "3.3 Creating a Bar Chart to Summarize Counts using R",
    "text": "3.3 Creating a Bar Chart to Summarize Counts using R\nTo create a bar chart, we will use the gf_counts() function. This general syntax for gf_counts() is\n\n# General syntax to create a bar chart\ngf_counts(~ attribute_name, data = data_name)\n\nIn this function we indicate the name of the attribute we want to create a bar chart for with a tilde (~) precedeing the attribute name. We also give the name of the data object in the data= argument. For example, the syntax to create a bar chart summarizing the counts of the control attribute is:\n\n# Syntax to create a bar chart for the control attribute\ngf_counts(~ control, data = colleges)\n\n\n\n\nFigure¬†3.7: Bar chart summarizing the counts of the control attribute.\n\n\n\n\n\nReading the syntax: Create a bar chart by modeling the counts of the control attribute in the colleges data object.\n\nWe can make this plot nicer by changing the axes labels. For example, we might change the y-axis label to ‚ÄúCount‚Äù and the x-axis label to ‚ÄúType of Institution‚Äù. To do this we include the xlab= and ylab= arguments in the gf_counts() function. The labels we want depicted are given as text inside of quotation marks. Remember that each argument needs to be separated by a comma!\n\nAs you include additional arguments in the function, it can be useful to include each argument on different lines. This will help you troubleshoot syntax that doesn‚Äôt work.\n\n\n# Syntax to create a bar chart for the control attribute\ngf_counts( \n  ~ control, data = colleges,\n  xlab = \"Type of Institution\",\n  ylab = \"Count\"\n  )\n\n\n\n\nFigure¬†3.8: Bar chart summarizing the counts of institutions of higher learning by type.\n\n\n\n\n\nYour Turn\nWrite the syntax to create a bar chart that summarizes the counts of the region attribute.\n\nShow/Hide Solution\n\n\n\ngf_counts(~ region, data = colleges)\n\n\n\n\n\nAdd better labels to the x- and y-axis of the bar chart you just created.\n\nShow/Hide Solution\n\n\n\ngf_counts(\n  ~ region, data = colleges,\n  xlab = \"Region of the United States\",\n  ylab = \"Count\"\n  )"
  },
  {
    "objectID": "03-01-categorical-attributes.html#creating-a-bar-chart-to-summarize-proportions-using-r",
    "href": "03-01-categorical-attributes.html#creating-a-bar-chart-to-summarize-proportions-using-r",
    "title": "3¬† Summarizing and Visualizing Categorical Attributes",
    "section": "3.4 Creating a Bar Chart to Summarize Proportions using R",
    "text": "3.4 Creating a Bar Chart to Summarize Proportions using R\nTo create a bar chart that summarizes the proportion of each category (rather than counts) we can use the gf_props() function. The syntax for this function is identical to that of gf_counts(). The syntax to create a bar chart summarizing the proportion of the control attribute is:\n\n# Syntax to create a bar chart for the control attribute\ngf_props(\n  ~control, data = colleges,\n  xlab = \"Type of Institution\",\n  ylab = \"Count\"\n  )\n\n\n\n\nFigure¬†3.9: Bar chart summarizing the proportions of institutions of higher learning by type.\n\n\n\n\nYou can also create a bar chart that summarizes the percentage of each category using the gf_percents() function.\n\n# Syntax to create a bar chart for the control attribute\ngf_percents(\n  ~control, data = colleges,\n  xlab = \"Type of Institution\",\n  ylab = \"Percent\"\n  )\n\n\n\n\nFigure¬†3.10: Bar chart summarizing the percentage of institutions of higher learning by type.\n\n\n\n\n\nYour Turn\nWrite the syntax to create a bar chart that summarizes the proportions of the region attribute. Also change the axis labels on both the x- and y-axes.\n\nShow/Hide Solution\n\n\n\ngf_props(\n  ~ region, data = colleges,\n  xlab = \"Region of the United States\",\n  ylab = \"Proportion\"\n  )"
  },
  {
    "objectID": "03-01-categorical-attributes.html#creating-horizontal-bar-charts-using-r",
    "href": "03-01-categorical-attributes.html#creating-horizontal-bar-charts-using-r",
    "title": "3¬† Summarizing and Visualizing Categorical Attributes",
    "section": "3.5 Creating Horizontal Bar Charts Using R",
    "text": "3.5 Creating Horizontal Bar Charts Using R\nTo create horizontal bar charts we can use gf_countsh(), gf_propsh(), or gf_percentsh(). These functions again take the same syntax as their vertical counterparts. For example, to create a horizontal bar chart summarizing the counts of each type of institution we can use the following syntax:\n\n# Syntax to create a hirizontal bar chart for the control attribute\ngf_countsh(\n  ~control, data = colleges,\n  xlab = \"Count\",\n  ylab = \"Type of Institution\"\n  )\n\n\n\n\nFigure¬†3.11: Horizontal bar chart summarizing the number of institutions of higher learning by type.\n\n\n\n\n\nYour Turn\nWrite the syntax to create a horizontal bar chart that summarizes the percent of the region attribute. Also change the axis labels on both the x- and y-axes.\n\nShow/Hide Solution\n\n\n\ngf_percentsh(\n  ~ region, data = colleges,\n  xlab = \"Percent\",\n  ylab = \"Region of the United States\"\n  )"
  },
  {
    "objectID": "03-01-categorical-attributes.html#summary",
    "href": "03-01-categorical-attributes.html#summary",
    "title": "3¬† Summarizing and Visualizing Categorical Attributes",
    "section": "3.6 Summary",
    "text": "3.6 Summary\nThere are several R functions that are useful for summarizing categorical attributes. Table¬†3.3 shows the functions (and their descriptions) you will use to summarize and visualize categorical attributes. Note that they all have very parallel syntax.\n\n\n\n\n\n  \n  Table¬†3.3:  The functions and their descriptions to summarize and visualize categorical attributes. \n  \n  \n    \n      Function\n      Description\n    \n  \n  \n    \n      Summarize\n    \n    df_stats(~attribute, data = data_object, counts)\n\nCompute counts of a categorical attribute\n    df_stats(~attribute, data = data_object, props)\n\nCompute proportions of a categorical attribute\n    df_stats(~attribute, data = data_object, percs)\n\nCompute percentages of a categorical attribute\n    \n      Visualize\n    \n    gf_counts(~attribute, data = data_object)\n\nCreate bar chart of counts\n    gf_props(~attribute, data = data_object)\n\nCreate bar chart of proportions\n    gf_percs(~attribute, data = data_object)\n\nCreate bar chart of percentages\n    gf_countsh(~attribute, data = data_object)\n\nCreate horizontal bar chart of counts\n    gf_propsh(~attribute, data = data_object)\n\nCreate horizontal bar chart of proportions\n    gf_percsh(~attribute, data = data_object)\n\nCreate horizontal bar chart of percentages\n  \n  \n  \n\n\n\n\n\nLastly, there are several optional arguments you can include in the visualization (gf_) functions to improve the aesthetic quality of your plot. Some of these are listed in Table¬†3.4.\n\n\n\n\n\n  \n  Table¬†3.4:  Optional argumnents that can be included in the visualization (gf_) functions. \n  \n  \n    \n      Argument\n    \n  \n  \n    \n      Labels\n    \n    xlab = \"x-axis label\"\n\n    ylab = \"y-axis label\"\n\n    title = \"Title for your plot.\"\n\n    subtitle = \"Subtitle for your plot.\"\n\n    caption = \"Caption for your plot.\"\n\n    \n      Color\n    \n    fill = \"color name for bar color\"\n\n    color = \"color name for bar outlines\"\n\n  \n  \n  \n\n\n\n\n\nRemember to separate each argument with a comma if you include multiple arguments in the function."
  }
]