[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Data Analysis: The End",
    "section": "",
    "text": "Foreword\nThis work in progress is ultimately going to be the primarily textbook resource for EPSY 5261 students. (Note: If you want to contribute to this, create a Pull Request or send me an email.) Also, feel free to offer criticism, suggestion, and feedback. You can either open an issue on the book‚Äôs github page or send me an email directly."
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "An Introduction to Data Analysis: The End",
    "section": "Colophon",
    "text": "Colophon\nArtwork by @allison_horst\nIcon and note ideas and prototypes by Desir√©e De Leon.\nThe book is typeset using Crimson Text for the body font, Raleway for the headings and Sue Ellen Francisco for the title. The color palette was generated using coolors.co.\nStatistical Computing\n\nLaptop icon made by Tomas Knop from www.flaticon.com\nDirectory icon made by Darius Dan from www.flaticon.com\nBrain icon made by Aranagraphics from www.flaticon.com\nInternet icon made by Freepik from www.flaticon.com"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "An Introduction to Data Analysis: The End",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "01-00-statistical-computation.html#footnotes",
    "href": "01-00-statistical-computation.html#footnotes",
    "title": "Statistical Computation",
    "section": "",
    "text": "Specifically, RStudio is branded as an ‚Äúintegrated development environment (IDE) [that] includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.‚Äù‚Ü©Ô∏é"
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#installing-r",
    "href": "01-01-r-and-rstudio-installation.html#installing-r",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.1 Installing R",
    "text": "1.1 Installing R\nTo install R, navigate your web browser to:\n\nhttps://www.r-project.org/\n\nThen,\n\nClick the CRAN link under Download on the left-hand side of the page.\nSelect a mirror site. These should all be the same, but I tend to choose the Iowa State University link under USA.1\nIn the Download and Install R box, choose the binary that matches the operating system (OS) for your computer.\n\nThis is where the installation directions diverge depending on your OS.\nMac Instructions\nSo long as you are running MacOS 10.13 or higher just click the first link for the PKG, which will download the installer for the most current version of R (4.1.1 as of August 16, 2021). Once the download completes, open the installer and follow the directions to install R on your computer.\nIf you are running an older version of MacOS, you will have to install an older version of R. You can find these links under the Binaries for legacy OS X systems heading further down the install page. Click the appropriate PKG link for R your version of MacOS. Once the download completes, open the installer and follow the directions to install R on your computer.\nIf you are unsure which version of the MacOS is running on your computer, select About this Mac from the Apple menu in your toolbar.\nWindows Instructions\nClick the link that says Install R for the first time (or click base; they go to the same place). Then click the Download R 4.1.1 for Windows link, which will download the installer for the most current version of R (4.0.2 as of July 24, 2020). Once the download completes, open the installer and follow the directions to install R on your computer.\nLinux Instructions\nIf you are running Linux, you should know how to install things on your computer. üòÄ"
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#installing-rstudio-desktop",
    "href": "01-01-r-and-rstudio-installation.html#installing-rstudio-desktop",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.2 Installing RStudio Desktop",
    "text": "1.2 Installing RStudio Desktop\nAfter you have installed R, you next need to install RStudio Desktop. To do this, navigate your web browser to:\n\nhttps://rstudio.com/products/rstudio/download/\n\nThen,\n\nSelect the blue Download button under the free, open-source version of RStudio Desktop.\nSelect the installer associated with your computer‚Äôs OS.\nOnce the download completes, open the installer and follow the directions to install RStudio Desktop on your computer."
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#checking-that-things-worked",
    "href": "01-01-r-and-rstudio-installation.html#checking-that-things-worked",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.3 Checking that Things Worked",
    "text": "1.3 Checking that Things Worked\nFrom your Applications or Programs folder, open RStudio. If you have successfully downloaded both programs, this should open the application and you should see a message indicating that you are using ‚ÄúR version 4.1.1‚Äù (or whichever version of R you installed) in the console pane.\n\n\n\n\n\nOnce you open RStudio, you should see a message indicating that you are using R version 4.1.1 (or whichever version of R you installed) in the console pane. Here the console pane is on the left-side, but it may be in a different location for you. Your RStudio may also have a white background rather than the black background seen here."
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#customizing-rstudio",
    "href": "01-01-r-and-rstudio-installation.html#customizing-rstudio",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.4 Customizing RStudio",
    "text": "1.4 Customizing RStudio\nWhile the information in this section is not crucial for making things work, it is useful to get RStudio looking good and setting some default settings. Open the Tools &gt; Options menu (Windows) or RStudio &gt; Preferences (Mac).\n\n\n\n\n\nThe RStudio options/preferences menu has many settings to customize RStudio.\n\n\n\n\n\nIn the General &gt; Basic settings, change the option on Save workspace to .Rdata on exit to be ‚ÄúNever‚Äù. Click the ‚ÄúApply‚Äù button.\nIn the Appearance settings, customize the look of RStudio to something aesthetically appealing to you. When you are finished, click the ‚ÄúApply‚Äù button.\nThere are also options you can set in the Accessibility settings if you use a screen reader. If you change anything, don‚Äôt forget to click the ‚ÄúApply‚Äù button.\n\nWhen you are finished customizing RStudio, click the ‚ÄúOK‚Äù button."
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#install-rtoolscommand-line-tools",
    "href": "01-01-r-and-rstudio-installation.html#install-rtoolscommand-line-tools",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.5 Install Rtools/Command Line Tools",
    "text": "1.5 Install Rtools/Command Line Tools\nYou may need to install some additional functionality to your system in order to get certain packages to install or load properly. On a Windows machine, you might need to install Rtools. Mac users might need to add the Command Line Tools. These tools also allow you to write and compile your own R packages. RStudio has well written instructions for adding these tools at: https://support.rstudio.com/hc/en-us/articles/200486498-Package-Development-Prerequisites."
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#footnotes",
    "href": "01-01-r-and-rstudio-installation.html#footnotes",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "",
    "text": "When internet used to be dial-up (i.e., super slow), you wanted to choose a mirror site that was closest in proximity to your location as it sped up the download. This is less of a concern now that internet download speeds are much faster.‚Ü©Ô∏é"
  },
  {
    "objectID": "02-00-data.html",
    "href": "02-00-data.html",
    "title": "Data",
    "section": "",
    "text": "The American Statistical Association defines statistics as, ‚Äúthe science of learning from data, and of measuring, controlling and communicating uncertainty‚Äù (American Statistical Association, 2023). The methods you learn throughout this textbook, and the EPSY 5261 course, will help you to learn from data and to measure, control, and communicate about uncertainty.\nAn important learning goal of statistics is therefore to understand the vocabulary and ideas related to data. To this end, in Chapter¬†2 you will learn about the structure of data and attribute classification. You will also learn how to judge the quality of data including questions you should ask about the data. ?sec-importing-data will introduce the syntax we use to import data into R. Finally, ?sec-design will introduce how the collection of the data impacts the type of inferences and conclusions we can draw.\n\n\n\n\nAmerican Statistical Association. (2023). ASA newsroom. Website. https://www.amstat.org/asa-newsroom"
  },
  {
    "objectID": "02-01-data-structure-and-attributes.html#classifying-attributes",
    "href": "02-01-data-structure-and-attributes.html#classifying-attributes",
    "title": "2¬† Data Structure and Attribute Classification",
    "section": "2.1 Classifying Attributes",
    "text": "2.1 Classifying Attributes\nOur ultimate goal is often to analyze the data we have to learn from it. For example, in our NYT Best Seller data, we may be interested in the proportion of authors that identify as female. Or, we may want to. know how many weeks a book stays on the Best Sellers list. The type of analyses we can do, however, depend on the type of attributes we have.\nWe typically classify attributes as either categorical attributes or quantitative attributes. These classifications are based on the type of information (data) in the attribute. A categorical attribute has values that represent categorical (or qualitative) differences between the cases, whereas a quantitative attribute represents numerical (or quantitative) differences between cases. For example, in the NYT Best Seller data, title and author are categorical variables, whereas year, and total number of weeks the book was on the NYT Best Sellers list are quantitative attributes.\nTypically attributes that have numerical values are quantitative, but not always. In our data, consider the attribute that indicates whether the author identifies as a female. Although the values in the data are numeric, these numbers actually represent different categories: 0 = no (not female) and 1 = yes (female). Therefore, this attribute is actually a categorical attribute, not a quantitative attribute.\nOne check of whether anattribute is actually quantitative is whether numeric computations, such as finding an average of the attribute, can be carried out and the result makes conceptual sense. For example, we cannot compute the mean author value (it is thus a categorical attribute). If we compute the mean of the female attribute we get a result, but it does not indicate anything about the gender identity of a NYT best selling author. The mean does not make conceptual sense and thus we classify female as a categorical attribute.\n\nYour Turn\nClassify the best_rank attribute (the book‚Äôs highest rank while it was on the NYT Best Sellers list) as either categorical or quantitative. Explain.\n\nShow/Hide Solution\n\n\nThe attribute best_rank is a quantitative attribute. The data in this attribute are numeric values, and it makes conceptual sense to compute summaries such as the mean for this attribute.\n\n\n\n\n2.1.1 Further Classifications of Attributes\nWhile categorizing each attribute as categorical or quantitative is a good first step, statisticians and data analysts often go a step further and classify attributes based on their scale of measurement. This classification is based on how attributes were measured (i.e., how we assign numerals to objects of events) and what this implies about the empirical statements we can make about the constructs measured on that particular scale. The most common taxonomy for this was described by Stevens (1946) who classified four scales of measurement: nominal, ordinal, interval, or ratio (NOIR). Below we describe each of these scales of measurement:\nNominal Scale of Measurement: In nominal scales of measurement, any numerals assigned to different values would only be useful as labels, for distinguishing or organizing values. Most categorical attributes have this scale of measurement. For example, in the NYT bests sellers data, the numerical values in the gender identity attribute (female) are only useful as labels and for distinguishing authors who identify as female and authors who don‚Äôt. Because of this, only the following type of statement would be meaningful:\n\nIn the NYT bestseller data, more authors identify as female (15) than do not identify as female (10).\n\nThe only type of empirical statements we can make are comparisons of the number of cases between different labels (e.g., counts, percentages).\nOrdinal Scale of Measurement: Data measured using an ordinal scales of measurement, is still categorical. It has all the features of nominal measured data (e.g., labeling, distinguishing, organizing), but we can also rank order the values in a meaningful way. A classic example of the ordinal scale of measurement is in the 5-star review rating used on sites like Amazon. All of these statements would be meaningful:\n\nThere are more 4-star reviews than 5-star reviews (comparison of counts).\nA review of 3 stars is better than a review of 2 stars (rank ordering).\n\nWith this scale of measurement, it is reasonable to not only provide counts of the different values (e.g., the number of 5-star reviews), but now because there is a rank ordering, we can also make empirical statements related to the rank ordering of the measured construct based on the numeral values. In nominal level data, these latter types of statements are not appropriate since the values for the labels are arbitrary. For example, authors with a gender identity of female were assigned a 1 and others were assigned a 0. This is arbitrary in that authors identifying as female could just as easily have been assigned a value of 0 and those that didn‚Äôt identify as female a value of 1. This implies that even though the numeral 1 is greater than the numeral 0, the attribute values associated with these numerals (identifying as female or not) do not have any meaningful rank order. Because of this, saying something about one identity being greater than or less than another is inappropriate.\nInterval Scale of Measurement: In interval level data, the rank order of the numbers assigned to attribute values is meaningful, similar to ordinal data. Moreover, the difference between consecutive values represents the same amount of difference on the underlying attribute. For example, consider the Fahrenheit temperature scale. All of these statements would be meaningful:\n\nThere are more 30 degrees F days than 0 degrees F days (comparison of counts).\nA day that is 10 degrees F is warmer than a day that is 9 degrees F (rank ordering).\nThe difference in temperature when you are comparing days that are 9 and 10 degrees F is the same as when you are comparing days that are 0 and 1 degrees F. (interval comparison).\n\nThe critical component is that on an interval scale, the difference in consecutive numerals has the same level of difference in the construct being measured, regardless of scale location. Again consider our 5-star rating system. The difference between a 1- and 2-star review is not the same as the difference between a 4- and 5-star review. While the numbers themselves have a constant difference, the difference in the amount of the underlying construct (satisfaction, happiness with the product, etc.) does not. With interval level scales we can compute summaries like means, standard deviations, and correlations.\nRatio Scale of Measurement: With attributes that have the ratio scale of measurement, the rank ordering of the numbers assigned to attribute values is meaningful, the differences between consecutive numerals indicates the same amount of difference in the underlying construct being measured, and ratio type statements about these differences are also meaningful. For example, the amount of snowfall is an attribute on the ratio scale of measurement. All of these statements would be meaningful:\n\nThere are more days with 0 inches of snowfall than days with 10 inches of snowfall (comparison of counts).\nA day with 8 inches of snowfall got more snow than a day with 4 inches of snowfall (rank ordering).\nThe difference in snowfall when you are comparing days with 8 inches of snowfall and 7 inches of snowfall is the same as when you are comparing days with 15 and 16 inches of snow (interval comparison).\nA day with 8 inches of snow got twice the amount of snow as a day with 4 inches of snow (ratio comparison).\n\nGoing back to our temperature scale, we cannot make these ratio type statements. For example, a day that is 60 degrees F is not twice as warm as a day that is 30 degrees F. This is because the Fahrenheit scale does not have a ‚Äútrue‚Äù zero value. (Zero degrees F does not indicate absence of temperature.) Whereas, in our snowfall attribute, a day with 0 inches of snow does indeed indicate no snow fell on that day.\nAside from the type of empirical statements we can make, the level of measurement also puts limits on the type of statistical analyses that are appropriate.\n\n\n\n\n\n\nTable¬†2.1:  The four measurement scales and the types of empirical statements and\nstatistical summaries that are appropriate for each scale. \n  \n    \n    \n      Scale\n      Empirical Statement\n      Statistical Summaries\n    \n  \n  \n    Nominal\n\nComparison of counts\n\nCounts, percentages\n\n    Ordinal\n\nComparison of counts  Rank ordering\n\nCounts, percentages  Median, percentiles\n\n    Interval\n\nComparison of counts  Rank ordering  Interval comparisons\n\nCounts, percentages  Median, percentiles  Mean, standard deviation\n\n    Ratio\n\nComparison of counts  Rank ordering  Interval comparisons  Ratio comparisons\n\nCounts, percentages  Median, percentiles  Mean, standard deviation  Coefficient of variation"
  },
  {
    "objectID": "02-01-data-structure-and-attributes.html#how-were-the-data-generated",
    "href": "02-01-data-structure-and-attributes.html#how-were-the-data-generated",
    "title": "2¬† Data Structure and Attribute Classification",
    "section": "2.2 How Were the Data Generated?",
    "text": "2.2 How Were the Data Generated?\nAnother question that has direct implications on the methods used in data analysis is: How were the data generated or collected? Were they collected from a survey? Were they mined from the web? Were they generated as part of an experimental study? Knowing the answer to these questions also is important for the degree to which we can draw conclusions from the analysis.\nUnderstanding how the data were generated allows us to determine whether the data we have constitute a sample of cases or the entire population of cases we are interested in learning about. Importantly, whether you have a sample or the entire population depends on how you define the cases/observations you are interested in drawing conclusions about.\n\nA population includes all cases/observations of interest, whereas a. sample includes a subset of cases from the population.\n\nFor example, consider a child psychologist who wants to draw conclusions about all students at a particular school in Minnesota. To do this, she collects data from every student in that school. Since her data includes every case (student) she is interest in drawing conclusions for, her data would be a population, Now consider a second child psychologist who is interested in drawing conclusions about all students in Minnesota. This psychologist also collects data from every student in the same school as the first psychologist. This second psychologist‚Äôs data would be considered a sample since the cases they included in their data are only a subset of the cases they want to draw conclusions about.\n\nYour Turn\nIs the New York Time best sellers data a population or a sample? Explain.\n\nShow/Hide Solution\n\n\nThe New York Time best sellers data is a sample since it is only a subset of all the New York Times best selling books.\n\n\n\n\n2.2.1 Drawing Conclusions from a Sample\nIn practice, we rarely have data collected from an entire population, but we still want to use the data we have in our sample to draw conclusions about that population. Drawing conclusions about an entire population when you only have data from a subset cases is what statisticians call statistical inference.\n\n\n\n\n\nFigure¬†2.1: A sample of data is drawn from the population. Information from the sample is then analyzed and used to make a statistical inference about the population.\n\n\n\n\nThis can be a very tricky thing to do since the sample does not give us complete information about the population. As an example, consider if you wanted to figure out the average monthly living expenses for all graduate students at the University of Minnesota. To do this you collect data on the monthly living expenses for the students in your EPSY 5261 class and compute the average monthly living expense based on the data you collected and use that value as a guess for the average monthly living expenses for all graduate students at the University of Minnesota. (Note that the cases in your data (students in your EPSY 5261 class) are a subset of the population you want to draw conclusions about (all graduate students at the University of Minnesota) and thus are a sample.)\n\nSummaries computed from the population are referred to as parameters and summaries computed from a sample are referred to as statistics.\n\nIn statistical inference the statistics we compute from a sample are estimates for the population parameters that we are interested in. The word ‚Äúestimate‚Äù may have clued you in that the value of a statistic is generally not equal to the value of the parameter. In our example, the average monthly living expenses we computed based on your sample of cases is probably not the same as the average monthly living expenses for all graduate students at the University of Minnesota. This is because our sample only includes data for some (not all) of the cases.\nWe don‚Äôt expect the value of the statistic to be the same as that for the parameter we are trying to estimate, but a key question is: Is the value of the statistic a reasonable estimate of the parameter? The answer to this question can sometimes be difficult to answer. What do we mean by reasonable? In statistical analysis, there are a few ways that we consider reasonableness of an estimate. We will explore these below.\n\n\n2.2.1.1 Sampling Error: Quantifying the Amount of Uncertainty in our Sample Estimate\nOne way we consider whether an answer is reasonable is how much uncertainty we have in the estimate from our sample. Imagine if you repeated the study, but this time, you collected data on the monthly living expenses in a different section of EPSY 5261. The average computed from these data would likely be different from the average you computed from your section of EPSY 5261, and therefore your guess for the average monthly living expenses for all graduate students at the University of Minnesota would be different. This is because you would have different cases in your sample.\n\nWhen using a sample to infer about a population, our guesses or estimates vary depending on the cases in our sample. This means that when we make inferences there is always some degree of uncertainty in our estimates.\n\nThe idea that estimates from samples vary depending on the cases in your sample is well known and is referred to as sampling error. In carrying out statistical inference, we not only acknowledge that we have uncertainty in our guess from the sample data, but we also try and quantify how much uncertainty there is in that estimate. For example, do we think that the average monthly living expenses for all graduate students at the University of Minnesota is within a few dollars of our sample estimate? Or do we think that it is within a few hundred dollars of our sample estimate? By providing this estimate of the uncertainty, it lets other people know ‚Äúhow reasonable‚Äù our guess might be.\n\n\n\n\n\nFigure¬†2.2: Estimates for the mean living expense for all graduate students at the University of Minnesota will vary from sample to sample because of sampling error. In statistical inference this is expected and quantifying the amount of sampling error gives us an indication of how much uncertainty we have in our estimate.\n\n\n\n\n\n\n\n2.2.1.2 Sampling Bias: Does the Sample Represent the Population?\nA second way we consider whether an answer is reasonable is to consider whether our sample of cases is representative of the population as a whole. In our example, we are asking the question of whether the students in your section of EPSY 5261 are representative of all graduate students at the University of Minnesota when it comes to living expenses. This is a really difficult question to answer, but generally (unless you have selected your sample randomly from the population), your sample is not representative. The key here is that the sampling method (how you chose your cases) matters!\n\nWhen a sample is not randomly selected from the population we say that the sampling method is biased.\n\nA biased sampling method leads to systematically wrong answers. For example, again say you were interested in determining the average monthly living expenses for all graduate students at the University of Minnesota. This time, your sampling method is to collect data about the monthly living expenses from all the graduate students who live in a particular apartment building in Downtown Minneapolis. Would these students‚Äô living expenses be representative of all graduate students at the University of Minnesota?\nAgain, probably not. The living expenses in Downtown Minneapolis are different (generally higher) than the living expenses of students who live in Dinkytown or one of the suburbs. Because the cases in your sample all come from the same apartment building in Downtown Minneapolis, their average monthly living expense will be systematically higher than the average monthly living expenses for all graduate students at the University of Minnesota.\nWhat about our original sampling method of collecting data from each of the graduate students in your EPSY 5261 section? While these students might live in different areas, and seem more representative, this sampling method is likely still biased. Even if we have a hard time identifying how, the estimate for the average monthly living expenses based on students in EPSY 5261 is likely systematically different than the average monthly living expenses for all graduate students at the University of Minnesota. (It may be systematically too low, or too high.)\n\nThe only sampling method that is guaranteed to be unbiased (and therefore representative) is to select your sample randomly from the population.\n\n\n\n\n\n2.2.2 Random Sampling\nThere are many methods for randomly selecting a sample from the population. The simplest method that incorporates randomness into the sampling process is Simple Random Sampling. In simple random sampling each case in the population has an equal probability of being selected into the sample.1\n\nIn the discipline of statistics, there are words that we use that have very different meanings from their use in colloquial English. ‚ÄúRandom‚Äù is one of those words. In our everyday language ‚Äúrandom‚Äù might mean happenstance or unexpected. For example: It was so random that I saw Ferris Bueller at the 31 Flavors last night. In statistics, ‚Äúrandom‚Äù does not mean happenstance at all. Random sampling is quite formal in ensuring that cases have a specified probability of being selected into the sample.\n\nOne of the most compelling and useful results in statistics is that a simple random sample is representative of the population, and moreover that even small samples that are selected with this method can be representative of very large populations. This is powerful!\nBut, it can sometimes be very difficult to draw a simple random sample in practice. For one thing, it requires that you have a list of all the cases in the population (called a sampling frame). This allows you to make sure that everyone in the population has the same probability of being selected. While it might be possible to obtain a list of all graduate students enrolled at the University of Minnesota, it is another thing to obtain a list of all people living in Minnesota. Or even a list of people living in Dinkytown. Depending on your population of interest you may not be able to get a simple random sample.2\n\nYour Turn\nWhat is the sampling method for the New York Time best sellers data. Based on this method, are the estimates of the population parameters we compute from these data going to be biased or unbiased?\n\nShow/Hide Solution\n\n\nThe New York Time best sellers data was sampled using simple random sampling. Because the sampling method employed randomness, the estimates we compute from the data are unbiased estimates of the population parameters."
  },
  {
    "objectID": "02-01-data-structure-and-attributes.html#summary",
    "href": "02-01-data-structure-and-attributes.html#summary",
    "title": "2¬† Data Structure and Attribute Classification",
    "section": "2.3 Summary",
    "text": "2.3 Summary\nEvery time you encounter data, you should identify the cases and attributes in the data. Understanding the cases, especially in relation to the cases you want to draw conclusions about, helps you identify whether you have a sample, or the entire population. Classifying the attributes helps you think about the type of analysis you can undertake. If your data are a sample (rather than a population), you also need to ask how they were collected. Were they collected using randomness in the sampling method? Or is the sampling method used to collect the data biased?"
  },
  {
    "objectID": "02-01-data-structure-and-attributes.html#references",
    "href": "02-01-data-structure-and-attributes.html#references",
    "title": "2¬† Data Structure and Attribute Classification",
    "section": "2.4 References",
    "text": "2.4 References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGould, R., & Ryan, C. (2013). Introductory statistics: Exploring the world through data. Pearson.\n\n\nPruett, J. (2021). NYT hardcover fiction bestsellers. Post45 Data Collective, V1. https://doi.org/https://doi.org/10.18737/CNJV1733p4520220211\n\n\nStevens, S. S. (1946). On the theory of scales of measurement. Science, 103(2684), 677‚Äì680."
  },
  {
    "objectID": "02-01-data-structure-and-attributes.html#footnotes",
    "href": "02-01-data-structure-and-attributes.html#footnotes",
    "title": "2¬† Data Structure and Attribute Classification",
    "section": "",
    "text": "Technically the definition of a simple random sample is all potential samples of size n have the same probability of being selected, which implies that each case in the population has an equal probability of being selected into the sample. Conceptually, however, it is easier to think about the probability of each case rather than of the probability of groups of size n.‚Ü©Ô∏é\nIn this case there are other more complex methods of random sampling that you could use (e.g., stratified random sampling, cluster random sampling.‚Ü©Ô∏é"
  },
  {
    "objectID": "03-00-exploring-and-describing-data.html#goals-for-summarization-and-visualization",
    "href": "03-00-exploring-and-describing-data.html#goals-for-summarization-and-visualization",
    "title": "Summarizing and Visualizing Data",
    "section": "Goals for Summarization and Visualization",
    "text": "Goals for Summarization and Visualization\nData scientists and statisticians visualize data and compute numerical summaries to explore and understand data. In addition to visualizing distributions of data, it is common to also summarize certain feature of the data using numbers. (For example, the mean is one summarization of a distribution of quantitative data.) Together visualizing and summarizing data can help analysts identify features in the data such as typical or extreme observations, and also describe and explore the variation in the data. Data exploration is an important first step in any statistical analysis."
  },
  {
    "objectID": "03-00-exploring-and-describing-data.html#college-scorecard-data",
    "href": "03-00-exploring-and-describing-data.html#college-scorecard-data",
    "title": "Summarizing and Visualizing Data",
    "section": "College Scorecard Data",
    "text": "College Scorecard Data\nThroughout the chapters in this section we will use the College Scorecard data to illustrate the methods of data exploration. These data were collected and made available by the U.S. Department of Education (DOE). The DOE publishes data on institutions of higher education in their College Scorecard to facilitate transparency and provide information for interested stakeholders (e.g., parents, students, educators). A subset of this data is provided in the file college-scorecard.csv.\n\nCSV File\nData Codebook"
  },
  {
    "objectID": "03-01-categorical-attributes.html#hypothetical-example-pet-ownership",
    "href": "03-01-categorical-attributes.html#hypothetical-example-pet-ownership",
    "title": "3¬† Summarizing and Visualizing Categorical Attributes",
    "section": "3.1 Hypothetical Example: Pet Ownership",
    "text": "3.1 Hypothetical Example: Pet Ownership\nImagine you have surveyed 10 pet owners about the type of pet they own.1 The data you collected is shown in Figure¬†3.1.\n\n\n\n\n\nFigure¬†3.1: Data collected from 10 pet owners about the type of pet the own.\n\n\n\n\nOur goal in exploratory analysis is to describe the data. One way of describing the data is to list all of the values. For example here we could say the sample included a turtle, a fish, a cat, a dog, another dog, another fish, another cat, another cat, another cat, and another dog. While this is an accurate description, it isn‚Äôt very generalizable. (Imagine trying to describe the data from 1000 pet owners or 10,000 pet owners!)\n\n\n3.1.1 Numerically Summarizing Categorical Attributes\nA more natural way to describe these data is to summarize them by providing counts of each pet type. For example, describing our sample data using counts:\n\n1 of the pet owners sampled owned a turtle,\n2 of the pet pet owners sampled owned a fish,\n3 of the pet pet owners sampled owned a dog, and\n4 of the pet pet owners sampled owned a cat.\n\nSummarizing each type of pet owned by reporting counts of them is a much more natural way of describing the data. (This is also useful when the sample size is much larger.)\nAnother summary that could be used to describe this sample is to give the proportion of each type of pet owned. To compute the proportion, we take the count of each type of pet owned, and divide it by the total sample size.\n\\[\n\\mathrm{Proportion} = \\frac{\\mathrm{Count~of~Pet~Type}}{\\mathrm{Total~Sample~Size}}\n\\]\nFor example, to compute the proportion of pet owners in our sample that owned a dog, we use:\n\\[\n\\mathrm{Proportion~of~Dogs} = \\frac{3}{10} = 0.30\n\\]\n\nProportions will always be between 0 and 1. If you add all of the proportions of each category together you will get 1, so long as values can only belong to one category.\n\nDescribing our sample data using proportions:\n\n0.10 of the pet owners sampled owned a turtle,\n0.20 of the pet pet owners sampled owned a fish,\n0.30 of the pet pet owners sampled owned a dog, and\n0.40 of the pet pet owners sampled owned a cat.\n\nThe count and proportion values are often reported in a table, especially if there are more than a couple values in the categorical attribute. Table¬†3.1 is an example table indicating the counts and proportions of values in our hypothetical pet example.\n\n\n\n\n\n  \n  Table¬†3.1:  Counts and proportions of pet owners who own each type of pet. \n  \n    \n    \n      Pet\n      Count\n      Proportion\n    \n  \n  \n    \n3\n0.30\n    \n4\n0.40\n    \n2\n0.20\n    \n1\n0.10\n  \n  \n  \n\n\n\n\n\n\n\n\n3.1.2 Visualizing Categorical Attributes\nTo visualize categorical attributes we typically use a bar chart. Figure¬†3.2 shows a bar chart of the pet data.\n\n\n\n\n\nFigure¬†3.2: Bar chart indicating the counts of each type of pet owned.\n\n\n\n\nA bar chart (also known as a bar graph) shows a bar for each category of the categorical attribute. In our example, we have four bars, one for each pet type. The height of the bar indicates the count of each category. For example, the bar for cats has a height of four on the y-axis.\nSometimes the axes in the bar chart are transposed; categories are placed on the y-axis and counts on the x axis. Also, you might see a bar chart indicating proportions rather than counts. Figure¬†3.3 shows a transposed bar chart indicating the proportion of each pet type.\n\n\n\n\n\nFigure¬†3.3: Bar chart indicating the counts of each type of pet owned. In this plot the categories are placed on the y-axis and the scale on the the x axis indicates proportions rather than counts.\n\n\n\n\n\nWhen proportions are used in a bar chart, it is coventional to extend that axis from 0 to 1 (the range of potential proportions).\n\n\n\n3.1.2.1 Bar Chart Variations\nThere are several variations on the bar chart that you may see in practice. For example, the segmented bar chart is a variation of the bar chart. This variation of the plot, which always uses proportions rather than counts, has a single bar that is split into segments‚Äîone for each category. A segmented bar chart summarizing the pet data is shown in Figure¬†3.4.\n\n\n\n\n\nFigure¬†3.4: Segmented bar chart indicating the proportion of each type of pet owned.\n\n\n\n\nAnother variation of the bar chart is the donut chart. A donut chart is simply a segmented bar chart that is presented in a circular layout. Figure¬†3.5 presents a donut chart summarizing the pet data. Because there is no axis to indicate the proportion of each category in a donut chart, it is conventional to indicate the percentages of each category in the plot. Here percentages are used rather than proportions.\n\n\n\n\n\nFigure¬†3.5: Donut chart indicating the percentage of each type of pet owned. A donut chart is simply a segmented bar chart presented in a circular layout.\n\n\n\n\n\n\n\n3.1.2.2 Pie Charts\nOne last plot used to visualize summaries of categorical dat is the pie chart. Figure¬†3.6 shows a pie chart summarizing the pet data. Unlike any of the bar charts that we have looked at, a pie chart uses the proportional area of a circle to indicate the proportion or percentage of each pet type rather than the length of a bar.\n\n\n\n\n\nFigure¬†3.6: Pie chart indicating the percentage of each type of pet owned. A pie chart uses the proportional area of a circle to indicate the proportion or percentage of each pet type.\n\n\n\n\n\nIn a pie chart, each section of the pie is determined by proportionally dividing the 360¬∞ of the circle based on the data, and then making each section have the computed angle. For example the proportion of pet owners who have a cat is 0.4, and 0.4 of 360¬∞ is 144¬∞‚Äîso the cats section has an angle of 144¬∞. While these computations and the drawing of the pie chart would be done by the computer, it has implications for interpretations. Namely, research has suggested that humans may not be as adept at making accurate comparisons involving angles and the areas of sections based on those angles. Because of this, the recommendation from the data visualization community is to use bar charts rather than pie charts when displaying summaries of categorical data."
  },
  {
    "objectID": "03-01-categorical-attributes.html#using-r-to-numerically-summarize-categorical-attributes",
    "href": "03-01-categorical-attributes.html#using-r-to-numerically-summarize-categorical-attributes",
    "title": "3¬† Summarizing and Visualizing Categorical Attributes",
    "section": "3.2 Using R to Numerically Summarize Categorical Attributes",
    "text": "3.2 Using R to Numerically Summarize Categorical Attributes\nTo illustrate how we can summarize and visualize categorical attributes using R, we will use the college-scorecard.csv data. As a reminder, we will start by loading three libraries: {tidyverse}, {ggformula}, and {mosaicCore}. Then we will import the college scorecard data using the read_csv() function and assign the data into an object called colleges. Finally, we view the data to make sure it read in properly.\n\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaicCore)\n\n# Import data\ncolleges &lt;- read_csv(\"https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/college-scorecard.csv\")\n\n# View data\ncolleges\n\n\n\n  \n\n\n\nRecall that in these data each case in these data is an institution of higher education and there are 234 institutions in our sample. This dataset has several categorical attributes, including: state, region, type of institution, and control.\nThe first attribute we will summarize and visualize is the control variable. Looking at the data codebook we find that this attribute can take on three different values: Public, Private nonprofit, and Private for-profit. Our first task is to get institution counts for each category of control. To do this, we will use the df_stats() function. The general syntax to obtain these counts is shown below.\n\n# General syntax to counts the categories in a categorical attribute\ndf_stats( ~ attribute_name, data = data_name, counts)\n\nWe first need to identify the name of the categorical attribute. To tell df_stats() that this is an attribute, we place a tilde (~) in front of the attribute name. Then we use the data= argument to identify the data object that includes our categorical attribute. Finally, we use counts to indicate that we want to compute the category counts.\n\nIt is a good idea to learn how to read R syntax. The tilde operator indicates a special kind of expression called a formula, and can be read as ‚Äúmodel‚Äù. So the general syntax above is read as, ‚Äúmodel the attribute_name found in the data_name data by counting the categories in the attribute‚Äù.\n\nPutting this into practice to count the categories in the control attribute which is in our colleges data:\n\n# Syntax to count the categories in the control attribute\ndf_stats(~control, data = colleges, counts)\n\n\n\n  \n\n\n\nIf we were to read this syntax, ‚Äúmodel the control attribute found in the colleges data by counting the categories in the attribute‚Äù.\nBased on the counts, we find that most of the institutions of higher learning in our sample are private nonprofit institutions (n = 146). There are also several public institutions of higher learning in our sample (n = 71). Lastly, there are also a few private for-profit institutions of higher learning in our sample (n = 13).\n\nIt is common to use n or N to denote sample size. Some textbooks and authors will use N to indicate the overall sample size (e.g., in the college scorecard data, N = 234) and n to indicate the sample size of subgroups within the sample (e.g., n = 71 for public institutions). Other authors might use n to define the overall sample size (e.g., n = 234) and then use subscripts on n to denote the sample size of different groups (e.g., \\(n_{\\mathrm{Public}}=71\\)). There is not a single unified agreed upon way to denote these things.\n\nWe also might want to compute the proportions for each category of control. To do this, we can again use the function df_stats(), but instead of providing counts we will provide props.\n\n# Syntax to compute the proportion for each category in the control attribute\ndf_stats(~control, data = colleges, props)\n\n\n\n  \n\n\n\nThese proportions (which can also be turned into percentages2) tell a similar story to what the counts did. Most of the institutions in our sample are private nonprofit (63.5%) and public (30.9%) institutions. There is a smaller percentage of institutions that are private for-profit (5.7%).3 As we did with the numerical summaries of the pet data, we can include these values in a table.\n\n\n\n\n\n  \n  Table¬†3.2:  Counts and proportions of institutions of higher learning by\ncontrol. \n  \n    \n    \n      Control\n      Count\n      Proportion\n    \n  \n  \n    Public\n71\n0.309\n    Private Nonprofit\n146\n0.635\n    Private For-Profit\n13\n0.057\n  \n  \n  \n\n\n\n\n\n\n\nYour Turn\nWrite the syntax to compute the counts of the region attribute.\n\nShow/Hide Solution\n\n\n\ndf_stats(~ region, data = colleges, counts)\n\n\n\n  \n\n\n\n\nWrite the syntax to compute the proportions of the region attribute.\n\nShow/Hide Solution\n\n\n\ndf_stats(~ region, data = colleges, counts)"
  },
  {
    "objectID": "03-01-categorical-attributes.html#creating-a-bar-chart-to-summarize-counts-using-r",
    "href": "03-01-categorical-attributes.html#creating-a-bar-chart-to-summarize-counts-using-r",
    "title": "3¬† Summarizing and Visualizing Categorical Attributes",
    "section": "3.3 Creating a Bar Chart to Summarize Counts using R",
    "text": "3.3 Creating a Bar Chart to Summarize Counts using R\nTo create a bar chart, we will use the gf_counts() function. This general syntax for gf_counts() is\n\n# General syntax to create a bar chart\ngf_counts(~ attribute_name, data = data_name)\n\nIn this function we indicate the name of the attribute we want to create a bar chart for with a tilde (~) precedeing the attribute name. We also give the name of the data object in the data= argument. For example, the syntax to create a bar chart summarizing the counts of the control attribute is:\n\n# Syntax to create a bar chart for the control attribute\ngf_counts(~ control, data = colleges)\n\n\n\n\nFigure¬†3.7: Bar chart summarizing the counts of the control attribute.\n\n\n\n\n\nReading the syntax: Create a bar chart by modeling the counts of the control attribute in the colleges data object.\n\nWe can make this plot nicer by changing the axes labels. For example, we might change the y-axis label to ‚ÄúCount‚Äù and the x-axis label to ‚ÄúType of Institution‚Äù. To do this we include the xlab= and ylab= arguments in the gf_counts() function. The labels we want depicted are given as text inside of quotation marks. Remember that each argument needs to be separated by a comma!\n\nAs you include additional arguments in the function, it can be useful to include each argument on different lines. This will help you troubleshoot syntax that doesn‚Äôt work.\n\n\n# Syntax to create a bar chart for the control attribute\ngf_counts( \n  ~ control, data = colleges,\n  xlab = \"Type of Institution\",\n  ylab = \"Count\"\n  )\n\n\n\n\nFigure¬†3.8: Bar chart summarizing the counts of institutions of higher learning by type.\n\n\n\n\n\nYour Turn\nWrite the syntax to create a bar chart that summarizes the counts of the region attribute.\n\nShow/Hide Solution\n\n\n\ngf_counts(~ region, data = colleges)\n\n\n\n\n\nAdd better labels to the x- and y-axis of the bar chart you just created.\n\nShow/Hide Solution\n\n\n\ngf_counts(\n  ~ region, data = colleges,\n  xlab = \"Region of the United States\",\n  ylab = \"Count\"\n  )"
  },
  {
    "objectID": "03-01-categorical-attributes.html#creating-a-bar-chart-to-summarize-proportions-using-r",
    "href": "03-01-categorical-attributes.html#creating-a-bar-chart-to-summarize-proportions-using-r",
    "title": "3¬† Summarizing and Visualizing Categorical Attributes",
    "section": "3.4 Creating a Bar Chart to Summarize Proportions using R",
    "text": "3.4 Creating a Bar Chart to Summarize Proportions using R\nTo create a bar chart that summarizes the proportion of each category (rather than counts) we can use the gf_props() function. The syntax for this function is identical to that of gf_counts(). The syntax to create a bar chart summarizing the proportion of the control attribute is:\n\n# Syntax to create a bar chart for the control attribute\ngf_props(\n  ~control, data = colleges,\n  xlab = \"Type of Institution\",\n  ylab = \"Count\"\n  )\n\n\n\n\nFigure¬†3.9: Bar chart summarizing the proportions of institutions of higher learning by type.\n\n\n\n\nYou can also create a bar chart that summarizes the percentage of each category using the gf_percents() function.\n\n# Syntax to create a bar chart for the control attribute\ngf_percents(\n  ~control, data = colleges,\n  xlab = \"Type of Institution\",\n  ylab = \"Percent\"\n  )\n\n\n\n\nFigure¬†3.10: Bar chart summarizing the percentage of institutions of higher learning by type.\n\n\n\n\n\nYour Turn\nWrite the syntax to create a bar chart that summarizes the proportions of the region attribute. Also change the axis labels on both the x- and y-axes.\n\nShow/Hide Solution\n\n\n\ngf_props(\n  ~ region, data = colleges,\n  xlab = \"Region of the United States\",\n  ylab = \"Proportion\"\n  )"
  },
  {
    "objectID": "03-01-categorical-attributes.html#creating-horizontal-bar-charts-using-r",
    "href": "03-01-categorical-attributes.html#creating-horizontal-bar-charts-using-r",
    "title": "3¬† Summarizing and Visualizing Categorical Attributes",
    "section": "3.5 Creating Horizontal Bar Charts Using R",
    "text": "3.5 Creating Horizontal Bar Charts Using R\nTo create horizontal bar charts we can use gf_countsh(), gf_propsh(), or gf_percentsh(). These functions again take the same syntax as their vertical counterparts. For example, to create a horizontal bar chart summarizing the counts of each type of institution we can use the following syntax:\n\n# Syntax to create a hirizontal bar chart for the control attribute\ngf_countsh(\n  ~control, data = colleges,\n  xlab = \"Count\",\n  ylab = \"Type of Institution\"\n  )\n\n\n\n\nFigure¬†3.11: Horizontal bar chart summarizing the number of institutions of higher learning by type.\n\n\n\n\n\nYour Turn\nWrite the syntax to create a horizontal bar chart that summarizes the percent of the region attribute. Also change the axis labels on both the x- and y-axes.\n\nShow/Hide Solution\n\n\n\ngf_percentsh(\n  ~ region, data = colleges,\n  xlab = \"Percent\",\n  ylab = \"Region of the United States\"\n  )"
  },
  {
    "objectID": "03-01-categorical-attributes.html#summary",
    "href": "03-01-categorical-attributes.html#summary",
    "title": "3¬† Summarizing and Visualizing Categorical Attributes",
    "section": "3.6 Summary",
    "text": "3.6 Summary\nThere are several R functions that are useful for summarizing categorical attributes. Table¬†4.1 shows the functions (and their descriptions) you will use to summarize and visualize categorical attributes. Note that they all have very parallel syntax.\n\n\n\n\n\n  \n  Table¬†3.3:  The functions and their descriptions to summarize and visualize\ncategorical attributes. \n  \n    \n    \n      Function\n      Description\n    \n  \n  \n    \n      Summarize\n    \n    df_stats(~attribute, data = data_object, counts)\n\nCompute counts of a categorical attribute\n    df_stats(~attribute, data = data_object, props)\n\nCompute proportions of a categorical attribute\n    df_stats(~attribute, data = data_object, percs)\n\nCompute percentages of a categorical attribute\n    \n      Visualize\n    \n    gf_counts(~attribute, data = data_object)\n\nCreate bar chart of counts\n    gf_props(~attribute, data = data_object)\n\nCreate bar chart of proportions\n    gf_percs(~attribute, data = data_object)\n\nCreate bar chart of percentages\n    gf_countsh(~attribute, data = data_object)\n\nCreate horizontal bar chart of counts\n    gf_propsh(~attribute, data = data_object)\n\nCreate horizontal bar chart of proportions\n    gf_percsh(~attribute, data = data_object)\n\nCreate horizontal bar chart of percentages\n  \n  \n  \n\n\n\n\n\nLastly, there are several optional arguments you can include in the visualization (gf_) functions to improve the aesthetic quality of your plot. Some of these are listed in Table¬†3.4.\n\n\n\n\n\n  \n  Table¬†3.4:  Optional argumnents that can be included in the visualization\n(gf_) functions. \n  \n    \n    \n      Argument\n    \n  \n  \n    \n      Labels\n    \n    xlab = \"x-axis label\"\n\n    ylab = \"y-axis label\"\n\n    title = \"Title for your plot.\"\n\n    subtitle = \"Subtitle for your plot.\"\n\n    caption = \"Caption for your plot.\"\n\n    \n      Color\n    \n    fill = \"color name for bar color\"\n\n    color = \"color name for bar outlines\"\n\n  \n  \n  \n\n\n\n\n\nRemember to separate each argument with a comma if you include multiple arguments in the function."
  },
  {
    "objectID": "03-01-categorical-attributes.html#footnotes",
    "href": "03-01-categorical-attributes.html#footnotes",
    "title": "3¬† Summarizing and Visualizing Categorical Attributes",
    "section": "",
    "text": "To keep it simple, assume each pet owner only has a single pet.‚Ü©Ô∏é\nYou can also use df_stats() to compute percentages by providing percs.‚Ü©Ô∏é\nNote that these percentages do not quite add up to 100%. This is because of the rounding that we did.‚Ü©Ô∏é"
  },
  {
    "objectID": "03-02-quantitative-attributes.html#importing-the-data",
    "href": "03-02-quantitative-attributes.html#importing-the-data",
    "title": "4¬† Summarizing and Visualizing Quantitative Attributes",
    "section": "4.1 Importing the Data",
    "text": "4.1 Importing the Data\nTo illustrate how we can summarize and visualize quantitative attributes using R, we will again use the college-scorecard.csv data. As a reminder, we will start by loading three libraries, {tidyverse}, {ggformula}, and {mosaicCore}. Then we will import the college scorecard data using the read_csv() function and assign the data into an object called colleges. Finally, we view the data to make sure it read in properly.\n\nlibrary(ggformula)\nlibrary(mosaicCore)\nlibrary(tidyverse)\n\n\n# Import data\ncolleges &lt;- read_csv(\"https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/college-scorecard.csv\")\n\n# View data\ncolleges\n\n\n\n  \n\n\n\nRecall that in these data each case in these data is an institution of higher education and there are 234 institutions in our sample. This dataset has several quantitative attributes, including: admission rate, number of undergraduate students, median debt for all students, median debt for graduates, and median earnings."
  },
  {
    "objectID": "03-02-quantitative-attributes.html#histograms",
    "href": "03-02-quantitative-attributes.html#histograms",
    "title": "4¬† Summarizing and Visualizing Quantitative Attributes",
    "section": "4.2 Histograms",
    "text": "4.2 Histograms\nThe first visualization we will examine is a histogram. We can create a histogram of the admission rates using the gf_histogram() function.1 This function takes the same general syntax as the gf_ functions you learned about in Chapter¬†3:\n\nThe first argument is a formula using the tilde operator (~) that identifies the attribute to be plotted, and\nThe second argument, data =, specifies the data object that was assigned on data import.\n\nThe syntax used to create a histogram of the admission rates is:\n\ngf_histogram(~ adm_rate, data = colleges)\n\n\n\n\nFigure¬†4.1: Histogram of admission rates for the institutions of higher learning in the sample.\n\n\n\n\n\n\n4.2.1 Interpretting Histograms\nHistograms are created by collapsing the data into bins and then counting the number of observations that fall into each bin. To show this more clearly in the figure created previously, we can color the bin lines to highlight the different bins. To do this we include an additional argument, color =, in the gf_histogram() function. We can also set the color for the bins themselves using the fill = argument. Here we color the bin lines black and set the bin color to yellow.2\n\ngf_histogram(\n  ~ adm_rate, data = colleges, \n  color = \"black\", \n  fill = \"yellow\",\n  xlab = \"Admission Rate\",\n  ylab = \"Count\"\n  )\n\n\n\n\nFigure¬†4.2: Histogram of college admission rates. Here the color of the bin lines to black and fill in the bars with yellow. Axes labels are also added to the plot.\n\n\n\n\nEach bar in a histogram represents indicates the number of cases with a range of values for the plotted variable. For example, the bar that is just to the right of 0.50, shows there are approximately 16 institutions of higher learning with admissions rates between about 0.50 and 0.54. Similar interpretations can be made for all of the other bars as well.\nOne common assumption made with a histogram is that the width of each bar covers the same range over the attribute plotted. In this histogram, there are 25 total bars, which means that the range of each is 0.04 on the admission rate scale (i.e., 25 * .04 = 1.00 which is the rtange of the entire attribute).\n\nYour Turn\nInterpret the bar that is immediately to the left of 1.00.\n\nShow/Hide Solution\n\n\nThere are approximately 12 institutions of higher learning with admissions rates between about 0.96 and 1.00.\n\n\n\n\n4.2.2 Describing the Distribution\nRather than focusing on any one bin, we typically want to describe the distribution of the attribute plotted as a whole. For example, it appears as though most institutions admit a high proportion of applicants since the bins to the right of 0.50 have higher counts than the bins that are below 0.50. (In fact, the highest bins seem to be above 0.75.) There are a few institutions, however, that are quite selective, admitting fewer than 25% of the students who apply.\nStatistically we would say that the distribution of admission rates is left-skewed. A left skewed distribution has the majority of cases on the right side of the distribution (i.e., at higher values of the attribute). In contrast, a distribution that has the majority of cases on the left side of the distribution (i.e., at lower values of the attribute) is called right-skewed. Figure¬†4.3 shows examples of both a left-skewed and right-skewed distribution.\n\n\n\n\n\n\n\n(a) Left-Skewed Distribution\n\n\n\n\n\n\n\n(b) Right-Skewed Distribution\n\n\n\n\nFigure¬†4.3: Examples of a left-skewed and right-skewed distribution.\n\n\nThe skewness of a distribution describes a characteristic that we refer to as the shape of the distribution. Some distributions are not skewed, these distributions have a symmetric shape. Figure¬†4.4 shows an example of a symmetric distribution.\n\n\n\n\n\nFigure¬†4.4: Example of a symmetric distribution."
  },
  {
    "objectID": "03-02-quantitative-attributes.html#density-plots",
    "href": "03-02-quantitative-attributes.html#density-plots",
    "title": "4¬† Summarizing and Visualizing Quantitative Attributes",
    "section": "4.3 Density plots",
    "text": "4.3 Density plots\nAnother plot that is sometimes more useful for determining the shape of the distribution for a quantitative attribute is the density plot. This plot is a smoothed out version of a histogram.\nDensity plots can be created with the gf_density() function which takes the same arguments as the other gf_ functions. Similar to these function, you can include optional arguments to color the plot and add axis labels.\nFigure¬†4.5 shows the density plot for the density plot for the admissions rate attribute plotted earlier in a histogram.\n\ngf_density(\n  ~ adm_rate, data = colleges,\n  color = \"black\", \n  fill = \"yellow\",\n  xlab = \"Admission Rate\",\n  ylab = \"Density\"\n  )\n\n\n\n\nFigure¬†4.5: Density plot of college admission rates.\n\n\n\n\nBased on the density plot, we can see that the shape of the distribution of admission rates is left-skewed.\n\nThe metric on the y-axis in a density plot is no longer counts, it is ‚Äúprobability density‚Äù, or just ‚Äúdensity‚Äù. We don‚Äôt interpret those values, but rather focus on the relative height of the curve. That is, areas of the density curve that are higher indicate more data in those areas of the attribute of interest. Places where the density curve is lower indicates areas where data occur infrequently. Density plots are interpreted similarly to a histogram in that\n\nOur interpretation of the distribution of admission rates remains that most institutions of higher learning admit a high proportion of applicants. In fact, colleges that admit around 75% of their applicants have the highest probability density, indicating this is where most of the institutions are found in the distribution. Additionally, there are just a few institutions that are have an admission rate 25% or less.\n\n\n4.3.1 More about Shape\nIn addition to the overall symmetry or direction of skewness, another aspect of shape that we should describe is the number of modes in the distribution. The distributions we have looked as so far have been unimodal, that is, they have a single mode or ‚Äúhump‚Äù in the distribution.\nOther distributions have multiple modes. For example, the distribution in Figure¬†4.6 is bimodal (it has two modes). If a distribution has more than one mode, it often indicates that there are different groups that have been mixed into the data. For example, in Figure¬†4.6 we see one mode around 180 minutes (3 hours) and another taller mode around 300 minutes (5 hours). This might indicate that there are two different groups of runners that competed in the Legacy Marathon‚Äîone smaller group that was faster (e.g., elite runners) and one larger group that was slower.\n\n\n\n\n\nFigure¬†4.6: Distribution of times for a random sample of runners who competed in the 2022 Legacy Marathon.\n\n\n\n\n\n\n\n4.3.2 Center and Variation: Two Additional Characteristics to Describe\nIn addition to the describing the shape of the distribution, there are two other characteristics of a quantitative distribution that we want to describe: the center and the variation.\nThe ‚Äúcenter‚Äù of a distribution is misleading in that it doesn‚Äôt literally mean the center of the distribution. What it really means is ‚Äútypical value‚Äù. In the distribution of admission rates presented in Figure¬†4.5, a typical admission rate might be around 0.75. This value is at the mode in the distribution.\nWe also need to describe the variation in the distribution. When estimating the variation from a density plot we typically describe the overall range of values, as well as, the range of values within which most of the data falls. In the distribution of admission rates presented in Figure¬†4.5, there are admission rates between 0 and 1, but most of the institutions of higher learning have an admission rate between 0.65 and 0.85. This could also be given as a range of values around the typical value‚Äî\\(0.75 \\pm 0.10\\)\n\nA full description of a quantitative distribution includes shape, center, and variation. Here is how we might descirbe the distribution of admission rates presented in Figure¬†4.5:\n\nThe distribution of admission rates is left-skewed. Most institutions admit a high proportion of applicants. A typical institution in the distribution admits around 75% of its applicants (\\(\\pm\\) 10%). There are, however, a few institutions that are quite selective, admitting fewer than 25% of the students who apply.\n\n\nIn a multi-modal distribution, the identification of center and variation is more difficult, since there are multiple typical values. For example, in the distribution of marathon times presented in Figure¬†4.6, there are two typical values, one around 180 minutes and another around 300 minutes. Similarly, in describing the variation we often describe a range of values around each typical value that depicts where most of the data fall. From Figure¬†4.6, most of the faster runners finished the Legacy Marathon between 160 and 190 minutes, whereas most of the slower runners finished with a time between 200 and 400 minutes.\n\nYour Turn\nCreate a density plot for the distribution of median earnings for students. Add appropriate labels to the axes.\n\nShow/Hide Solution\n\n\n\ngf_density(\n  ~ median_earnings, data = colleges,\n  color = \"black\", \n  fill = \"yellow\",\n  xlab = \"Median Earnings\",\n  ylab = \"Density\"\n  )\n\n\n\n\n\nDescribe the shape, center, and variation of this distribution and what it tells you about students‚Äô median earnings 10 years after being enrolled in college.\n\nShow/Hide Solution\n\n\nThe distribution of median earnings is right skewed. The median amount of money earned 10 years after graduation is around $50k (\\(\\pm \\$10k\\)). There are some institutions where the median earnings of students is quite high (~100k)."
  },
  {
    "objectID": "03-02-quantitative-attributes.html#numerically-summaries",
    "href": "03-02-quantitative-attributes.html#numerically-summaries",
    "title": "4¬† Summarizing and Visualizing Quantitative Attributes",
    "section": "4.4 Numerically Summaries",
    "text": "4.4 Numerically Summaries\nIn the previous examples, we estimated the typical value (center) and variation from a visualization of the distribution. If the distribution is unimodal, we can also obtain more precise values for these characteristics by computing different numerical summaries.3\n\n4.4.1 Learn More\nIn this class we will focus on the computation of these values using R and their interpretations rather than on the mathematical manipulation and formulas. Here are some links to learn more about the underlying calculations of the measures that we will focus on in this class if you are interested: mean, median, and mode, range interquartile range (IQR), standard deviation.\n\nIn describing the center of the distribution, we estimated the typical value based on the value for the ‚Äúmodal hump‚Äù in the density plot. There are two numerical values that are often computed to summarize the center of the distribution: the mean and the median.\nIn a perfectly symmetric distributions, the mean and median are the same value. In practice, they are rarely exactly the same. If the distribution is roughly symmetric, these values should be similar. In skewed distributions, the mean and median will be different. In these distributions, the mean will be further in the tail of the distribution than the median. Comparing the mean and median is another way to verify the symmetry or asymmetry of the distribution.\n\n\n\n\n\n\n\n(a) Left-Skewed Distribution\n\n\n\n\n\n\n\n(b) Symmetric Distribution\n\n\n\n\n\n\n\n(c) Right-Skewed Distribution\n\n\n\n\nFigure¬†4.7: Mean, median, and modal values in a left-skewed, symmetric, and right-skewed distribution.\n\n\nIn a symmetric distribution, any of the three center values (mean, median, or mode) are a good summarization of a typical value since they are all roughly the same. In skewed distributions, because the mean is further in the tail, it is often not a good reflection of a typical value in the distribution. Instead, the median or mode is often a better summary in these distributions.\n\n\n4.4.2 Computing Numerical Summaries in R\nTo compute numerical summary values, including the mean and median, we use the df_stats() function from the {mosaicCore} package. This function takes the exact same arguments as the gf_ functions. The syntax below shows how to compute numerical summaries for the admissions rate attribute.\n\n# Compute numerical summaries\ndf_stats(~adm_rate, data = colleges)\n\n\n\n  \n\n\n\nThe mean, or average, of the 230 institutions‚Äô admission rates is 0.68 and the median admission rate for these institutions is 0.72. The slightly lower mean value is consistent with how the mean and median compare in a left-skew distribution. Because the distribution is skewed the median of 0.68, or modal value of 0.75, is a better indication of a typical admission rate.\n\nYour Turn\nCompute the mean and median for the distribution of students‚Äô median earnings 10 years after being enrolled in college\n\nShow/Hide Solution\n\n\n\ndf_stats(~median_earnings, data = colleges)\n\n\n\n  \n\n\n\nThe mean earnings is $51,134 and the median earnings is $49,316.\n\nBased on the distribution of students‚Äô median earnings 10 years after being enrolled in college is the mean or median a better summary of a typical value in the distribution? Explain.\n\nShow/Hide Solution\n\n\nBecause the distribution of earnings is right-skewed, the median is a better summary of a typical value in the distribution.\n\n\n\n\n\n4.4.3 Numerically Summarizing Variation\nThere are several summary measures that statisticians use to summarize the variation in a quantitative distribution. In this class we will focus on three of these measures: the range, the standard deviation, and the interquartile range (IQR).\n\nThe range is the difference between the maximum and minimum values in the distribution.\nThe standard deviation is measure of how far, on average, values in the distribution are from the mean.\nThe interquartile ranger (IQR) is the range of the middle 50% of the distribution.\n\nTo illustrate how to compute these values, we will again use df_stats() to compute the summaries for the admission rate attribute.\n\n# Compute numerical summaries\ndf_stats(~adm_rate, data = colleges)\n\n\n\n  \n\n\n\nThe range of the distribution is \\(1 - 0.0519 = 0.9481\\). Note that the range is a single value. Alternatively, we can say the admission rates range from 0.0519 to 1. This summary indicates the overall variation in the attribute.\nThe standard deviation is computed and returned in the sd column of the df_stats() output. The standard deviation of the admission rates is 0.217. Interpreting this, we would say that on average, most admission rates are within .216 of the mean. That is, most admission rates are between 0.47 and 0.90. To compute this range:\n\\[\n\\begin{split}\n&\\mathrm{Mean} \\pm \\mathrm{SD}\\\\[2ex]\n&0.683 - 0.217 = 0.47 \\\\[2ex]\n&0.47 + 0.217 = 0.90\n\\end{split}\n\\] The last summary measure of variation we will compute is the IQR. The IQR is the difference between the 75th-percentile value (Q3) and the 25th-percentile value (Q1). In the admission rates attribute this is:\n\\[\n\\begin{split}\n\\mathrm{IQR} &= 0.840975 - 0.5597 \\\\[2ex]\n&= 0.281\n\\end{split}\n\\]\nThat is the middle 50% of the admission rates have a range of 0.281‚Äîthey range from 0.56 to 0.84.\n\nYour Turn\nCompute the range, IQR, and standard deviation for the distribution of students‚Äô median earnings 10 years after being enrolled in college.\n\nShow/Hide Solution\n\n\n\ndf_stats(~median_earnings, data = colleges)\n\n\n\n  \n\n\n# Range = 132969 - 19513 = 113456\n# IQR = 56535.75 - 43031 = 13504.75\n# SD = 14.601k\n\n\nBased on the distribution of students‚Äô median earnings 10 years after being enrolled in college which summary measure(s) of variation would you report? Explain.\n\nShow/Hide Solution\n\n\nBecause the distribution of earnings is right-skewed, we might report the IQR in addition to the range.\n\n\nSimilar to the numerical summaries for the center of a distribution, some measures of variation are better suited toward summarizing symmetric distributions and others for skewed distributions. For all distributions, the range is typically provided to summarize the overall variation. In addition, for symmetric distributions, the standard deviation is also conventionally used to summarize the variation for most cases. In skewed distributions, the IQR is a better numerical summary of the variation than the standard deviation.\n\nAdding the numerical summaries to our previous description of the distribution of admission rates presented in Figure¬†4.5:\n\nThe distribution of admission rates is left-skewed. There is a great deal of variation in addmission rates, with institutions of higher learning admitting as few as .5% of their applicants, and some as many as 100%. A typical institution in the distribution admits around 72% (median) of its applicants with half admitting between 56% (Q1) and 84% (Q3) of their applicants. This suggests that most institutions admit a high proportion of applicants. There are, however, a few institutions that are quite selective, admitting fewer than 25% of the students who apply.\n\n\nThere is no one correct way to summarize and report the characteristics of a quantitative distribution. You will want to describe the shape, center, and variation, but how you do that may be different from how another person chooses to do that. For example, in the description of the distribution of admission rates I chose to report the characteristics using text/prose. Another researcher might have instead chosen to report this information in a table rather than writing about it. As an applied scientist or researcher you need to be able to do both.\n\nIn practice you might try multiple reporting strategies within a paper before you settle on one that is ‚Äúbest‚Äù for that paper. Think of it like you do in drafting writing‚Äîoften your first attempt isn‚Äôt the same as the final product, and it may take several iterations to get to that final draft."
  },
  {
    "objectID": "03-02-quantitative-attributes.html#summary",
    "href": "03-02-quantitative-attributes.html#summary",
    "title": "4¬† Summarizing and Visualizing Quantitative Attributes",
    "section": "4.5 Summary",
    "text": "4.5 Summary\nThere are several R functions that are useful for summarizing categorical attributes. Table¬†4.1 shows the functions (and their descriptions) you will use to summarize and visualize quantitative attributes. Note that they all have very parallel syntax.\n\n\n\n\n\n  \n  Table¬†4.1:  The functions and their descriptions to summarize and visualize\nquantitative attributes. \n  \n    \n    \n      Function\n      Description\n    \n  \n  \n    \n      Summarize\n    \n    df_stats(~attribute, data = data_object)\n\nCompute common summaries of a quantitative attribute\n    \n      Visualize\n    \n    gf_histogram(~attribute, data = data_object)\n\nCreate histogram\n    gf_density(~attribute, data = data_object)\n\nCreate density plot\n  \n  \n  \n\n\n\n\n\nWhen describing a quantitative distribution, there are three characteristics to attend to: shape, center, and variation. These can be estimated from a plot of the distribution, and the center and variation can also be summarized numerically. Often the shape of the distribution dictates which measures are provided in the description of the attribute. Table¬†4.2 presents a guide for thinking about what should be reported based on the shape of the distribution.\n\n\n\n\n\n  \n  Table¬†4.2:  Characteristics (shape, center, and variation) of different\nquantitative distributions that might get reported to summarize\nthem. \n  \n    \n    \n      Example\n      Shape\n      Center\n      Variation\n    \n  \n  \n    \nLeft-skewed\nMedian\\nMode\nRange (overall) and IQR (typical)\n    \nSymmetric\nMean, Median, or Mode\nRange (overall) and SD (typical)\n    \nRight-skewed\nMedian or Mode\nRange (overall) and IQR (typical)"
  },
  {
    "objectID": "03-02-quantitative-attributes.html#footnotes",
    "href": "03-02-quantitative-attributes.html#footnotes",
    "title": "4¬† Summarizing and Visualizing Quantitative Attributes",
    "section": "",
    "text": "This function is part of the {ggformula} package which needs to be loaded prior to using the gf_histogram() function.‚Ü©Ô∏é\nR knows the names of 657 colors. To see these names type colors() at the command prompt.‚Ü©Ô∏é\nIf the distribution is multi-modal many of these summaries cannot be computed, and even if they can be computed they are meaningless as summaries of a typical value or variation.‚Ü©Ô∏é"
  },
  {
    "objectID": "04-00-comparing-to-a-standard.html",
    "href": "04-00-comparing-to-a-standard.html",
    "title": "Comparing Data to a Standard",
    "section": "",
    "text": "One task that is commonly performed in research is to compare the data you have to a specified standard or value. For example, is the average income for a community higher than the poverty level? Or, is the mean admission rate for institutions of higher learning in the United States higher than 0.50?\nIn previous chapters you learned how to compute characteristics of the distribution (e.g., the mean) that would allow us to answer these questions about the sample. For example, in ?sec-quantitative, we found that the average admission rate for our 230 sample institutions of higher learning was 0.68. Based on this, we could say that the average admission rate for our sample of 230 schools was higher than 0.50. But, is this true when we grow our sample to ALL institutions of higher learning? Is the mean admission rate for ALL institutions of higher learning in the United States higher than 0.50?\nDrawing conclusions beyond the data we have is called inference, and the associated methods that allow researchers to allows us to learn from incomplete or imperfect data are referred to as statistical inference (Gelman & Hill, 2007). In this part of the textbook, you will learn about a set of statistical inferential methods that allow you to compare a sample of data to some standard in order to draw inferences about how the population compares to that standard (e.g., is the average income for a community higher than the poverty level?). To answer this type of inferential question, you will learn about how we quantify the amount of uncertainty associated with our sample numerical estimate when we have incomplete data (i.e., only a sample of data from the population we want to infer to). You will also learn how we then use that quantification in a one-sample hypothesis test to draw an inference about how a population parameter compares to that standard. Finally, you will learn about potential errors that can be made when conducting hypothesis tests and also assumptions underlying the methods we use to carry out these tests.\n\n\n\n\nGelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press."
  },
  {
    "objectID": "04-01-case-study-teen-sleep.html#descriptive-analysis",
    "href": "04-01-case-study-teen-sleep.html#descriptive-analysis",
    "title": "5¬† Case Study: Teen Sleep",
    "section": "5.1 Descriptive Analysis",
    "text": "5.1 Descriptive Analysis\nAfter importing the data, we will start by exploring the data. Because we are interested in the hours of sleep teens are getting, we will visualize and numerically describe the hrs_sleep attribute.\n\n# Create density plot\ngf_density(\n  ~ hrs_sleep, data = teen_sleep,\n  color = \"black\", \n  fill = \"skyblue\",\n  xlab = \"Hours of Sleep\",\n  ylab = \"Density\"\n  )\n\n# Compute numerical summaries\ndf_stats(~hrs_sleep, data = teen_sleep)\n\n\n\n\n\n\nFigure¬†5.1: Density plot of the hours of sleep for 75 teenagers.\n\n\n\n\n\n\nFigure¬†5.2: Density plot of the hours of sleep for 75 teenagers.\n\n\n\n\nDescribing the amount of sleep teens get we might say:\n\nThere is a great deal of variation in the hours of sleep that teens get, with some teenagers getting as much as 11 hours of sleep a night, and others getting as little as 4 hours. A typical teen in the sample gets around 7.5 hours of sleep a night. The SD of 1.5 further indicates most teens get between 6 and 9 hours of sleep a night. There is some evidence in the density plot that suggests the distribution of teen sleep may be bimodal, with a much smaller group of teens averaging around 5 hours of sleep a night.\n\nBased on the visual and numerical evidence, we might conclude that, on average, teens are not getting the recommended 9 hours of sleep a night. The key words here in the conclusion are: ‚Äúon average‚Äù. Statistically, we are saying that a typical teen in the distribution (around 7.5 hours of sleep a night) is not getting the recommended 9 hours of sleep a night. It is important to note that we are not talking about individual teens here (some of the teens in the sample are getting 9 or more hours of sleep a night), but rather are asking the question: on average, are teens getting the recommended 9 hours of sleep a night.\nUsing this interpretation, we can definitively say that the 75 teens in our sample are, on average, not getting 9 or more hours of sleep a night‚Äîthey are averaging 7.4 hours of sleep a night (based on the sample mean). Our research question, however is not whether these 75 teens are getting the recommended amount of sleep, but whether teens more generally are getting the recommended 9 hours of sleep a night. To answer this requires that we infer beyond our sample of 75 teens to the broader population of teens from which they were sampled from."
  },
  {
    "objectID": "04-01-case-study-teen-sleep.html#statistical-inference-using-sample-data-to-draw-conclusions-about-the-population",
    "href": "04-01-case-study-teen-sleep.html#statistical-inference-using-sample-data-to-draw-conclusions-about-the-population",
    "title": "5¬† Case Study: Teen Sleep",
    "section": "5.2 Statistical Inference: Using Sample Data to Draw Conclusions about the Population",
    "text": "5.2 Statistical Inference: Using Sample Data to Draw Conclusions about the Population\nIn many studies, the primary interest is to learn about one or more characteristics about a population. Although these characteristics must be estimated from sample data, the sample (and thus the estimate computed from the sample) form an incomplete picture of the population. As an example, consider the pictures of Goldy Figure¬†5.3. In the left-hand panel every pixel of the picture is shown. In the right-hand panel, we have only shown a random sample of the pixels. While the picture on the right is incomplete, we can still infer what the original picture looks like. This idea is similar to how we can make inferences about a population from a subset of cases (i.e., a sample).\n\n\n\n\n\n\n\n(a) Population\n\n\n\n\n\n\n\n(b) Sample\n\n\n\n\nFigure¬†5.3: Goldy.\n\n\nThe issue with using the sample data to draw a conclusion about the larger population is that the sample is an incomplete picture of the larger population. This means that any estimate we obtain from the sample needs to account for this incompleteness or uncertainty. For example, in the right-hand panel of Figure¬†5.3 the incomplete data leads to uncertainty about the actual picture. The information there allows us to make some inferences (e.g., it is a photo of Goldy), but there is still uncertainty because some of the pixels are not shown.\nIn our example of determining whether teenagers are sleeping 9 or more hours, we used sample data to obtain an estimate of the average amount of sleep teens are getting in a night, namely 7.4 hours of sleep a night. Because we based this estimate on incomplete data (i.e., from a sample) there is uncertainty associated with this estimate‚Äî7.4 hours is probably not the average number of hours ALL teens sleep a night. Quantifying the uncertainty gives us a better estimate. For example, this quantification might suggest that teens sleep, on average, between 6.0 and 8.8 hours a night.\n\n\n5.2.1 Sources of Uncertainty\nWhy does incomplete data lead to uncertainty? It turns out, there are several sources of uncertainty, the most common of which are:\n\nSampling variation (a.k.a, sampling error); and\nMeasurement variation (a.k.a., measurement error)\n\nSampling variation is the idea that different samples that can be drawn from the same population produce different estimates.\nFor example, what if if we had used a different sample of 75 teenagers. The amount of sleep these teens got would likely be different than our original 75 teens, which means that their average amount of sleep (the sample mean) would also be different than the original average of 7.4. This tells us that the uncertainty in the estimate is a function of the random nature of the sampling.\nAnother source of uncertainty in estimates is imperfect measurement, or measurement error. This arises most often when the constructs we are measuring can not be directly observed (i.e., they are latent) and we have to use proxies of these construct in our analysis. For example, the way the researchers in the original study measured the amount of sleep was via a self-report survey; they asked teens and their parents to indicate the teen‚Äôs typical bedtime and wake-up time using a drop down menu that gave times in 5 minute increments. Self-reporting, even with the parent responses as a check, are likely imperfect measures of the amount of sleep a teen gets. Further, if we compute a numerical summary, say the mean, from scores that are imperfect measures, then that mean will also be an uncertain estimate. The uncertainty in the sample estimate is now not only due to sampling error, but also because of the measurement error inherent in its computation.\nIn practice, despite these being very different sources of variation, measurement error and sampling error are often combined and treated as if all of the uncertainty was due to sampling error. This\n\nEPsy 5261, we will focus on quantifying uncertainty via estimating the sampling variation/sampling error. You can learn more about how to compute and account for measurement error in courses like EPsy 5221: Priciples of Educational and Psychological Measurment."
  },
  {
    "objectID": "04-01-case-study-teen-sleep.html#uncertainty-due-to-sampling-error",
    "href": "04-01-case-study-teen-sleep.html#uncertainty-due-to-sampling-error",
    "title": "5¬† Case Study: Teen Sleep",
    "section": "5.3 Uncertainty Due to Sampling Error",
    "text": "5.3 Uncertainty Due to Sampling Error\nTo give you a better sense of the uncertainty in an estimate that is due to sampling error and the methods we use to quantify sampling error, consider the following thought experiment: Imagine the population of amount of sleep per night taken for every teenager (past, present, and future). There would be an infinite number of these values, but theoretically, we could plot all of these values, and compute numerical summaries such as the mean and standard deviation of these values. (The mean of this distribution is what we are trying to estimate using our sample data.) The conceptual idea of estimating sampling variation is that we are going to draw a sample of 75 students and compute the mean amount of sleep for that sample. Then we are going to repeat this process again, and again, each time drawing 75 observations and computing the mean amount of sleep. Figure¬†6.1 shows a visual depiction of this thought experiment that was carried out 1000 times.\n\n\n\n\n\nFigure¬†5.4: Thought experiment for drawing random samples of size 75 from the population to obtain different samples. The average amount of sleep per night is computed for each sample drawn.\n\n\n\n\nThe 1000 sample means can then be plotted, and we can compute a numeric summary of the variability in the distribution of sample means.\n\n\n\n\n\nFigure¬†5.5: Distribution of 1000 sample means. Each observation in the distribution is a mean indicating the average amount of sleep per night for one sample. The red dots show the means computed from Figure¬†6.1.\n\n\n\n\nIn Figure¬†5.5 each observation plotted in the distribution is one of the means from a sample drawn in our thought experiment. For example, the red dots indicate the five sample means reported in Figure¬†6.1, namely 7.48 (mean of Sample 1), 6.90 (mean of Sample 2), 7.78 (mean of Sample 3), 7.32 (mean of Sample 4), and 7.24 (mean of Sample 1000). When the cases in a distribution are numerical summaries, we call that distribution a sampling distribution. The distribution in Figure¬†5.5 is a sampling distribution of the mean, indicating that the numerical summaries being plotted are means.\nRemember that our purpose for producing this distribution is to summarize the variation in the mean values, that is, we want to produce a numerical summary of the variation in the sampling distribution of the means shown in Figure¬†5.5. This distribution is approximately normal (unimodal and symmetric), so the standard deviation will be a good summary of the variation in this distribution.\n\nIf you have a distribution that is unimodal and symmetric, you can estimate the standard deviation by determining the halfway point of the height of the middle of the distribution. You can then follow that out to either side of the distribution and that width is a good guess for the standard deviation.\n\n\n\n\n\nFigure¬†5.6: Visual depiction of how to estimate the standard deviation in a unimodal symmetric distribution.\n\n\n\n\n\nIn the sampling distribution, in Figure¬†5.5, we estimate the standard deviation to be approximately 0.2. Because this standard deviation is quantifying the variability in a distribution of summary statistics, we refer to it as a standard error (SE).\nRecall that each of the samples were randomly sampled from the same population and that the sample mean is a guess for the value of the population mean‚Äîthe average amount of nightly sleep for ALL teens. Since the only source of variation in the sampling distribution is sampling error (that is, the only reason the sample means are different is that different teens were chosen to be a part of each sample), the SE is a quantification of the uncertainty due to sampling error we expect in our estimate. Based on a typical value in the distribution of about 7.4, and using our SE of 0.20, we can say:\n\nWe think that the mean amount of sleep each night for ALL teens is between 7.2 and 7.6. This range of values captures the uncertainty in the estimate (the sample mean) that is due to sampling error."
  },
  {
    "objectID": "04-01-case-study-teen-sleep.html#looking-ahead",
    "href": "04-01-case-study-teen-sleep.html#looking-ahead",
    "title": "5¬† Case Study: Teen Sleep",
    "section": "5.4 Looking Ahead",
    "text": "5.4 Looking Ahead\nIn practice, you do not draw 1000 samples; you have one sample of data that you have collected. So one of the things we will need to learn is how to produce a standard error based on only one sample of data. You will also learn about how to use the standard error in hypothesis tests evaluate how well data conforms to particular quantitative hypotheses we may have about the population. Finally, you will learn about a theoretical result in statistics known as the Empirical Rule, which will help us put probabilistic statements around the quantification of uncertainty to produce confidence intervals."
  },
  {
    "objectID": "04-01-case-study-teen-sleep.html#references",
    "href": "04-01-case-study-teen-sleep.html#references",
    "title": "5¬† Case Study: Teen Sleep",
    "section": "5.5 References",
    "text": "5.5 References\n\n\n\n\nJohns Hopkins University. (2023). Teenagers and sleep: How much sleep is enough? https://www.hopkinsmedicine.org/health/wellness-and-prevention/teenagers-and-sleep-how-much-sleep-is-enough\n\n\nNational Institutes of Health. (2021). Good sleep for good health: Get the rest you need. In NIH News in Health. https://newsinhealth.nih.gov/2021/04/good-sleep-good-health"
  },
  {
    "objectID": "04-02-simulation.html#bootstrapping",
    "href": "04-02-simulation.html#bootstrapping",
    "title": "6¬† Bootstrapping: Using Simulation to Estimate the Uncertainty",
    "section": "6.1 Bootstrapping",
    "text": "6.1 Bootstrapping\nThe key question addressed by using any statistical method of inference is ‚Äúhow much variation is expected in a particular test statistic if one repeatedly draws random samples from the same population?‚Äù In the thought experiment we introduced in Chapter¬†5, the method for quantifying the uncertainty was to repeatedly sample from the population and measure the variation in the sample means. Recall that the quantification of the uncertainty (i.e., variation in the sample means) is referred to as the standard error.\n\n\n\n\n\nFigure¬†6.1: Thought experiment for drawing random samples of size 75 from the population to obtain different samples. The average amount of sleep per night is computed for each sample drawn.\n\n\n\n\nBradley Efron introduced the methodology of bootstrapping in the late 1970s as an alternative method to compute the standard error.\ncomputer-based simulation framework to replace the inaccurate and complicated approximations that theoretical methods provide.1\nEfron‚Äôs big discovery was that in the thought experiment, we could replace the population with a sample, and then randomly sample from that initial sample. He proved that using this methodology, you can obtain a good estimate of the sampling variation.\n\n\n\n\n\nFigure¬†6.2: Thought experiment for bootstrapping random samples of size 75 from the original sample of 75 students‚Äô sleep times to obtain different samples. The average amount of sleep per night is computed for each re-sample drawn.\n\n\n\n\nBecause we need to randomly sample 75 observations out of the original sample (which itself only includes 75 observations), we need to sample WITH REPLACEMENT when we draw our re-samples. In this way, we mimic drawing random samples from a larger population without actually needing the larger population."
  },
  {
    "objectID": "04-02-simulation.html#importing-the-teen-sleep-data",
    "href": "04-02-simulation.html#importing-the-teen-sleep-data",
    "title": "6¬† Bootstrapping: Using Simulation to Estimate the Uncertainty",
    "section": "6.2 Importing the Teen Sleep Data",
    "text": "6.2 Importing the Teen Sleep Data\nWe will use the data in teen-sleep.csv to bootstrap a standard error of the mean. These data include the bedtime, wake-up time, and hours slept for a sample of \\(n=75\\) American teens in Grades 9‚Äì12.\n\nteen-sleep.csv\nData Codebook\n\nWe will prepare for the analysis by loading in the {tidyverse}, {ggformula}, and {mosaicCore} libraries and importing the teen sleep data. We will also load the {mosiaic} package.\n\nlibrary(ggformula)\nlibrary(mosaicCore)\nlibrary(mosaic)\nlibrary(tidyverse)\n\n\n# Import data\nteen_sleep &lt;- read_csv(\"https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/teen-sleep.csv\")\n\n# View data\nteen_sleep"
  },
  {
    "objectID": "04-02-simulation.html#bootstrapping-from-the-teen-sleep-data",
    "href": "04-02-simulation.html#bootstrapping-from-the-teen-sleep-data",
    "title": "6¬† Bootstrapping: Using Simulation to Estimate the Uncertainty",
    "section": "6.3 Bootstrapping from the Teen Sleep Data",
    "text": "6.3 Bootstrapping from the Teen Sleep Data\nThe process for computing the standard error via bootstrapping is:\n\nSTEP 1: Randomly sample n observations from the observed sample of size n (with replacement) This is called a bootstrap sample or a re-sample.\nSTEP 2: Compute the mean of the bootstrap sample.\nSTEP 3: Repeat the first two steps in the process many times (say 1000 times), each time recording the mean.\nSTEP 4: Find the standard deviation of these means (i.e., the standard error of the mean).\n\nThe computations we do will parallel each step of this process. As you learn how to do this, it is easy to get lost in the computing and forget why you are doing this. Remember, the end goal is to mimic the thought experiment so we can quantify the variation in the sample means.\n\n\n6.3.1 STEP 1: Randomly sample 75 observations from the observed sample of size 75 teen sleep amounts (with replacement)\nTo randomly sample from a set of values we use the sample() function. We will need to specify the values we are sampling from (i.e., the original sample) as an input to the function. The data we want to randomly sample from is in a column called hrs_sleep inside the data object called teen_sleep. To specify a particular column in a data object we use the following notation: teen_sleep$hrs_sleep. We also need to set the number of observations to randomly sample, and tell this function that we are sampling with replacement.\nThus to draw a random sample of values from our data we use:\n\n# Randomly sample from the hrs_sleep column located in the teen_sleep data object\n# Draw 75 observations\n# Sample with replacement\nsample(teen_sleep$hrs_sleep, size = 75, replace = TRUE)\n\n [1]  9.166667  8.083333  7.750000  7.666667  6.666667  7.333333  4.416667\n [8]  4.583333  7.083333  7.333333  7.416667  6.750000  4.500000  6.916667\n[15]  7.750000  9.916667  4.833333  7.083333  8.833333  9.166667  8.750000\n[22]  7.833333  8.583333  8.083333  7.750000  7.333333 10.083333  8.833333\n[29]  8.083333  8.333333  6.083333  4.166667  7.583333  7.916667  4.500000\n[36]  8.500000 11.083333  6.083333  7.916667  7.583333  9.750000  9.750000\n[43] 10.083333  9.750000 11.083333  9.166667  7.916667 10.083333  5.916667\n[50]  8.916667  6.916667  6.666667  7.583333  6.583333  7.666667  7.916667\n[57]  6.916667  6.083333  7.583333  6.416667  7.583333  7.083333  7.750000\n[64]  7.500000  7.750000  7.500000  4.416667  9.916667  6.083333  7.666667\n[71]  4.416667  6.333333  5.416667  8.916667  7.750000\n\n\nThis is akin to drawing a bootstrap sample from the original sample. Note that because we are drawing randomly, if you are trying this on your computer, you might get a different bootstrap sample than the one shown here. If you re-run this syntax, you will get a different bootstrap sample.\n\n# Draw a second bootstrap sample of 75 observations\nsample(teen_sleep$hrs_sleep, size = 75, replace = TRUE)\n\n [1]  8.833333  7.583333  7.583333  6.916667  6.750000  7.500000  8.083333\n [8]  7.666667  5.166667  7.583333  5.916667  7.166667 10.333333  6.583333\n[15] 11.083333  8.500000  8.333333  7.500000  9.166667  6.666667  7.833333\n[22]  4.500000  6.500000  5.916667  7.916667  7.666667  6.333333  8.333333\n[29]  6.083333  7.583333  8.500000  7.916667  7.416667  6.083333  5.416667\n[36]  8.333333  8.916667  7.500000  7.666667 10.083333  4.833333  6.583333\n[43]  6.750000  7.916667  7.083333  7.583333  7.083333  7.500000  4.916667\n[50]  7.750000  7.583333  7.500000  6.833333  4.916667  9.166667  7.750000\n[57]  4.416667  7.583333  7.166667  7.916667  9.166667  7.666667  7.083333\n[64] 11.083333  4.583333  7.333333  7.083333  6.666667  6.083333  7.583333\n[71]  7.750000  8.333333  9.166667  7.500000  9.750000\n\n\n\n\n\n6.3.2 STEP 2: Compute the mean of the bootstrap sample.\nTo compute the mean of a bootstrap sample, we are just going to embed our sample() syntax inside of the mean() function. For example,\n\n# Draw a bootstrap sample of 75 observations and compute the mean\nmean(sample(teen_sleep$hrs_sleep, size = 75, replace = TRUE))\n\n[1] 7.482222\n\n\nYou could re-run this syntax to draw another bootstrap sample and compute the mean.\n\n\n\n6.3.3 STEP 3: Repeat the first two steps in the process many times (say 1000 times), each time recording the mean.\nTo repeat a set of computations, we are going to use the do() function from the {mosaic} package. As a reminder, you will need the {mosiac} package loaded prior to using this function. The syntax for the do() function takes the following format:\ndo(N times) * {Computations to repeat}\nAs an example, if we wanted to carry out our computations to draw a bootstrap sample and compute the mean 10 times, the synatx is:\n\n# Draw a bootstrap sample of 75 observations and compute the mean\n# Do this 10 times \ndo(10) * {mean(sample(teen_sleep$hrs_sleep, size = 75, replace = TRUE))}\n\n\n\n  \n\n\n\nThe computations are carried out 10 times and the results are recorded in a column (result) of a data object. Because we will ultimately want to compute on this data object, when we run this, we will want to assign the data into an object. Below, we draw 1000 bootstrap samples, each time computing the mean, and assign them into a data object called bootstrap_means.\n\n# Draw a bootstrap sample of 75 observations and compute the mean\n# Do this 1000 times \n# Assign these into an object called bootstrap_means\nbootstrap_means &lt;- do(1000) * {mean(sample(teen_sleep$hrs_sleep, size = 75, replace = TRUE))}\n\n# View the results\nbootstrap_means\n\n\n\n  \n\n\n\n\n\n\n6.3.4 STEP 4: Find the standard deviation of these means (i.e., the standard error of the mean).\nRemember our goal was to compute the standard error, which quantifies the uncertainty in the sample mean estimates that is due to sampling variation. Before we do that, we will visualize the distribution of bootstrapped means.\n\n# Create a density plot of the bootstrapped means\ngf_density(\n  ~result, data = bootstrap_means,\n  xlab = \"Mean value\",\n  ylab = \"Density\"\n)\n\n\n\n\nFigure¬†6.3: Distribution of 1000 bootstrapped means.\n\n\n\n\nThe distribution of bootstrapped means is unimodal and symmetric. This indicates that the standard deviation is a reasonable numeric summary of the variation. Again, since the cases in the distribution are means (summary measures), the standard deviation is referred to as a standard error. To compute the standard error, we use df_stats():\n\n# Compute SE\ndf_stats(~result, data = bootstrap_means)\n\n\n\n  \n\n\n\nHere the standard error (found in the sd column) is 0.17.\n\nThe distribution of bootstrapped means should be centered at the value of the original sample mean. In our teen sleep example, the original sample had a mean of 7.4. This value is roughly at the center of the distribution in Figure¬†6.3. This can be a self-check when you create a bootstrap distribution."
  },
  {
    "objectID": "04-02-simulation.html#references",
    "href": "04-02-simulation.html#references",
    "title": "6¬† Bootstrapping: Using Simulation to Estimate the Uncertainty",
    "section": "6.4 References",
    "text": "6.4 References\n\n\n\n\nRaspe, R. E. (1948). Singular travels, campaigns and adventures of Baron Munchausen (J. Carswell, Ed.). Cresset Press."
  },
  {
    "objectID": "04-02-simulation.html#footnotes",
    "href": "04-02-simulation.html#footnotes",
    "title": "6¬† Bootstrapping: Using Simulation to Estimate the Uncertainty",
    "section": "",
    "text": "The nomenclature of bootstrapping comes from the idea that the use of the observed data to generate more data is akin to a method used by Baron Munchausen, a literary character, after falling ‚Äúin a hole nine fathoms under the grass,‚Ä¶observed that I had on a pair of boots with exceptionally sturdy straps. Grasping them firmly, I pulled with all my might. Soon I had hoist myself to the top and stepped out on terra firma without further ado‚Äù (Raspe, 1948, p. 22)‚Ü©Ô∏é"
  },
  {
    "objectID": "04-03-one-sample-test.html#statistical-inference-and-hypothesis-testing",
    "href": "04-03-one-sample-test.html#statistical-inference-and-hypothesis-testing",
    "title": "7¬† Hypothesis Testing: One-Sample t-Test",
    "section": "7.1 Statistical Inference and Hypothesis Testing",
    "text": "7.1 Statistical Inference and Hypothesis Testing\nOne common approach to statistical inference‚Äîthe drawing of conclusions about populations based on sample observations‚Äîis to use the sample observations to test a priori hypothesis1 about the population. Hypotheses are mathematical statements about population parameters which are often formed based on prior knowledge and substantive literature in the area of content.\nIn the social sciences, we typically write out two hypotheses about the population parameters: the null hypothesis (\\(H_0\\)), often referred to as a statement of no effect, and the alternative hypothesis (\\(H_A\\)), often termed the research (or alternative) hypothesis. For example, here are a set of potential hypotheses about teen sleep:\n\\[\n\\begin{split}\n&H_0: \\textrm{The mean amount of sleep for all teens is equal to 9 hours.} \\\\[1ex]\n&H_A: \\textrm{The mean amount of sleep for all teens is less than 9 hours.}\n\\end{split}\n\\] There are a few things to notice about these hypotheses:\n\nThe statements are about the mean amount of sleep (i.e., summary measure).\nThe statements are about the population (all teens), not the sample.\nThe null hypothesis (\\(H_0\\)) is a statement of equality (is equal to).\nThe alternative hypothesis often indicates the researcher‚Äôs belief about the population summary (e.g., we think the average amount of sleep for all teens is less than 9 hours).\n\nStatisticians often use the language of mathematics to express these hypotheses. The same hypotheses expressed via the language of mathematics are:\n\\[\n\\begin{split}\n&H_0: \\mu_{\\textrm{Sleep}}=9 \\\\[1ex]\n&H_A: \\mu_{\\textrm{Sleep}} &lt; 9\n\\end{split}\n\\]\nThe Greek letter mu (\\(\\mu\\)) denotes a population mean. In general Greek letters represent population parameters while Roman letters represent sample statistics. Here are a list of common statistical summaries and the mathematical notation used to denote them.\n\n\n\n\n\n  \n  Table¬†7.1:  Some common statistical summaries and the mathematical notation used\nto denote them. \n  \n    \n    \n      Summary\n      Sample\n      Population\n    \n  \n  \n    Mean\n$$\\bar{x},~\\textit{M}$$\n$$\\mu$$\n    Standard Deviation\n$$\\textit{s},~\\textit{SD}$$\n$$\\sigma$$\n    Variance\n$$s^2,~\\textit{Var}$$\n$$\\sigma^2$$\n  \n  \n  \n\n\n\n\n\nThe alternative hypothesis is always an inequality. In this example, the alternative hypothesis is the mean is LESS THAN 9 hours. Another potential alternative hypothesis would be that the mean is GREATER THAN 9 hours, while a third possibility is that the mean is NOT EQUAL TO 9 hours. Mathematically these could be expressed as \\(\\mu_{\\textrm{Sleep}}&lt;9\\), \\(\\mu_{\\textrm{Sleep}}&gt;9\\), and \\(\\mu_{\\textrm{Sleep}}\\neq9\\). The alternative hypothesis you choose is based on your conjecture about the population. For example, if we believed that teens sleep, on average, less than 9 hours a night, then the alternative hypothesis we choose would be \\(\\mu_{\\textrm{Sleep}}&lt;9\\). If we thought they sleep more than 9 hours, on average, we would adopt the alternative hypothesis of \\(\\mu_{\\textrm{Sleep}}&gt;9\\). If we are unsure about whether they sleep less or more than 9 hours, then our alternative hypothesis would be \\(\\mu_{\\textrm{Sleep}}\\neq9\\).\n\n\n7.1.1 The Null Model\nA hypothesis test is predicated on the assumption that the null hypothesis is true. Thus, we want to produce a sampling distribution of potential sample summaries that we could see if \\(H_0: \\mu_{\\textrm{Sleep}}=9\\) is actually true. In other words, we are carrying out a thought experiment assuming that the average amount of sleep for all teens is actually 9 hours.\n\n\n\n\n\nFigure¬†7.1: Visual depiction of the thought experiment underlying a hypothesis test assuming that the mean amount of sleep for all tennagers is 9 hours.\n\n\n\n\nIf we were able to carry out this thought experiment, here is what the sampling distribution of the sample means would look like:\n\n\n\n\n\nFigure¬†7.2: Density plot of the sampling distribution of the sample means based on the thought experiment underlying a hypothesis test assuming that the mean amount of sleep for all tennagers is 9 hours.\n\n\n\n\nThis distribution is also known as the null distribution since it is the sampling distribution that arises from the thought experiment assuming the null hypothesis is true. Describing the features (shape, center, and variation) of this null distribution we find:\n\nThe distribution is unimodal and symmetric.\nThe mean (center) of the distribution is at 9 hours.\nThe SD of this distribution (the SE) is approximately 0.20.\n\nInterpreting these features, we find that in the thought experiment where we assume the mean amount of sleep is 9 hours, a typical mean is 9 hours! But, sample means will vary from 9 hours. That is, even if the true mean amount of sleep is 9 hours, we could expect a sample mean that differs from 9 hours. How much they will vary depends on the SE, which is approximately 0.20.2 So, it would not be unusual to see a sample mean as low as 8.6 (\\(9 - 2(0.20) = 8.6\\)) or as high as 9.4 (\\(9 + 2(0.20) = 9.4\\)).\n\nThe null distribution will always be centered at the parameter value specified in the null hypothesis! The SE of the null distribution gives us an indication of how much a sample statistic is likely to vary from the parameter specified in the null hypothesis. We expect most values will be within 2 standard errors of the center.\n\n\n\n\n7.1.2 Evaluating the Observed Sample Mean\nThe null distribution gives us an indication of the range of sample mean values that are expected assuming the null hypothesis is true. Using the null distribution as a reference, we can evaluate the mean we obtained from the observed data, which was 7.5 hours (see Chapter¬†5).\n\n\n\n\n\nFigure¬†7.3: Density plot of the sampling distribution of the sample means based on the thought experiment underlying a hypothesis test assuming that the mean amount of sleep for all tennagers is 9 hours. The blue shaded area represents the sample mean values we expect under the hypothesis that the average amount of sleep for all tennagers is 9 hours. The pink dot represnts the sample mean of 7.5 that we observed in the teen sleep data.\n\n\n\n\nWe can see from Figure¬†7.3 that the observed mean of 7.5 is not a value we expect if the population mean amount of sleep teenagers get is truly 9 hours. It is, in fact, far less than we expect. In other words,\n\nThe sample mean of 7.5 hours of sleep we observed in the data is not consistent with the hypothesis that the average amount of sleep teenagers get a night is 9 hours.\n\nBecause the data were not consistent with our initial hypothesis, we would reject the null hypothesis, that is, the empirical evidence (data) does not support the hypothesis.\nWhile the method we used allows us to say whether the empirical data are consistent with the null hypothesis that teenagers get, on average, 9 hours of sleep, it does not tell us the level of consistency. Is it slightly inconsistent? Or really inconsistent? Because of this, applied researchers will often quantify this via two measures: (1) the t-value, and the p-value."
  },
  {
    "objectID": "04-03-one-sample-test.html#quantifying-the-level-of-consistency-with-the-null-hypothesis-the-t-value",
    "href": "04-03-one-sample-test.html#quantifying-the-level-of-consistency-with-the-null-hypothesis-the-t-value",
    "title": "7¬† Hypothesis Testing: One-Sample t-Test",
    "section": "7.2 Quantifying the Level of Consistency with the Null Hypothesis: The t-Value",
    "text": "7.2 Quantifying the Level of Consistency with the Null Hypothesis: The t-Value\nThe t-value quantifies how far away the observed mean is from the hypothesized mean value in standard error units. To compute the t-value we use the following:\n\\[\nt = \\frac{\\mathrm{Obs.~Mean} - \\mathrm{Hyp.~Mean}}{SE}\n\\]\nwhere, Obs. Mean is the observed sample mean from the data, Hyp. Mean is the hypothesized value in the null hypothesis, and SE is the standard error in the null distribution (which we compute via bootstrapping). Computing this for our example,\n\\[\n\\begin{split}\nt &= \\frac{7.5 - 9}{0.17} \\\\[2ex]\n&= -8.82\n\\end{split}\n\\] The t-value indicates that our observed sample mean of 7.5 is 8.82 standard errors below the hypothesized population mean value of 9. Changing the distance metric to standard error units helps standardize the distance for other scholars so they can better interpret how discrepant the observed mean is from the hypothesized value.\nFor example, if we hadn‚Äôt divided by the SE, we would have said our observed mean of 7.5 hours of sleep is 1.5 hours less than the hypothesized mean of 9 hours of sleep. Is this a lot less? Or a little less? The answer to that depends on how much we expect sample means to vary from the population mean under random sampling. This is what the SE quantifies. So dividing by the SE accounts for this expected variation and also changes the units from ‚Äúhours of sleep‚Äù to ‚Äústandard errors‚Äù.\nNow that we have a t-value, how do we judge its magnitude? To do this, we can again look back to the null distribution in Figure¬†7.3. Based on the null distribution, we said we expected sample means to fall in between 8.6 and 9.4. What are the t-values associated with 8.6 and 9.4?\n\\[\n\\begin{split}\nt &= \\frac{8.6 - 9}{0.17} \\\\[2ex]\n&= -2.35 \\\\[4em]\nt &= \\frac{9.4 - 9}{0.17} \\\\[2ex]\n&= 2.35\n\\end{split}\n\\]\nExpecting a sample mean between 8.6 and 9.4 is essentially the same as expecting a t-value between \\(-2.35\\) and 2.35.\n\nRule-of-Thumb: An observed mean that has a t-value with an absolute value less than 2 is fairly consistent with the null hypothesis being true. An observed mean that has a t-value with an absolute value greater than 2 is less consistent with the null hypothesis being true, and the further away from 2, the more evidence against the null hypothesis.\n\n\n\n7.2.1 The t-Distribution\nRecall that the null distribution is simply a distribution of sample means we expect if the null hypothesis is true. Because each case in this distribution is a sample mean, we could transform each case into a t-value. If we do that, the resulting distribution is a t-distribution.\n\n\n\n\n\nFigure¬†7.4: Density plot of the t-distribution of the sample means based on the thought experiment underlying a hypothesis test assuming that the mean amount of sleep for all tennagers is 9 hours (t value of 0). The blue shaded area represents the t-values we expect if the null hypothesis is true. The pink dot represents the observed t-value of -8.82.\n\n\n\n\nThe t-distribution is is the sampling distribution that arises from converting the null distribution from the thought experiment assuming the null hypothesis is true to t-values. Describing the features (shape, center, and variation) of this t-distribution we find:\n\nThe distribution is unimodal and symmetric.\nThe mean (center) of the t-distribution is 0.\nThe SD of this t-distribution (the SE) is approximately 1.\n\nNote that the descriptions referred to ‚Äúthis t-distribution‚Äù. That is because there are many different t-distributions; in fact there are an infinite number of them. Each t-distribution is based on a parameter called the degrees-of-freedom (df), which is in turn based on the sample size for the observed data. The df for the t-distribution is computed as:\n\\[\n\\mathit{df} = n - 1\n\\]\nwhere n is the sample size.\nThe degrees-of-freedom parameter impacts the shape and SE (variation) in the t-distribution. Figure¬†7.5 shows the t-distribution based on a few different degrees-of-freedom values. From this figure we can see:\n\nEvery t-distribution is unimodal and symmetric, although t-distributions with smaller degrees-of-freedom parameters are shorter and have thicker tails than t-distributions with higher degrees-of-freedom parameters.\nThe mean (center) of every t-distribution is 0.\nThe SD of this t-distribution (the SE) depends on the degrees-of-freedom, and t-distributions with smaller degrees-of-freedom parameters have a higher SE than t-distributions with higher degrees-of-freedom parameters.\n\n\n\n\n\n\nFigure¬†7.5: Density plot of three different t-distributions. The t-distributions shown have 3 degree-of-freedom (SE = 1.73), 5 degree-of-freedom (SE = 1.29), and 99 degree-of-freedom (SE = 1.01), respectively. Note that the degrees-of-freedom value impacts the shape and variation in the distribution.\n\n\n\n\nThe SE of a t-distribution depends directly on the degrees-of-freedom. Specifically,\n\\[\n\\mathit{SE} = \\begin{cases}\n\\mathrm{Undefined}, & \\text{if } &\\mathit{df}\\leq1\\\\[2ex]\n\\infty, & \\text{if } &1&lt;\\mathit{df}\\leq2\\\\[2ex]\n\\sqrt{\\frac{\\mathit{df}}{\\mathit{df}-2}}, & \\text{if } &\\mathit{df}&gt;2\n\\end{cases}\n\\]\nIn empirical data analyses, the df will almost always be higher than 2 since the sample size for most analyses will be \\(n\\geq3\\). Memorizing these formulas is not important (you an always look them up on Wikipedia), the important thing to see is that when df gets bigger the SE becomes approximately 1.\n\nWhen you report t-values or give information about a t-distribution, you should always report the degrees-of-freedom.\n\n\n\n\n7.2.2 The t-Distribution for the Teen Sleep Example\nNow that we understand a bit more about the properties of the t-distribution, we can sketch the t-distribution for the teen sleep example. Recall that the sample size for the observed data was \\(n=75\\). The df for the resulting t-distribution is:\n\\[\n\\begin{split}\n\\mathit{df} &= 75 - 1 \\\\[1ex]\n&= 74\n\\end{split}\n\\]\nThis helps us think about the properties for this t-distribution:\n\nThe distribution is unimodal and symmetric.\nThe mean (center) of the t-distribution is 0.\nThe SD of this t-distribution (the SE) is \\(\\sqrt{\\frac{74}{74-2}}=1.014\\).\n\nUsing these properties, we can sketch this t-distribution. We can also add the observed t-value of \\(-8.82\\) into the distribution. (Note: We will have to go out about 9 SEs from the center value of 0 to place the observed t-value onto the distribution!)\n\n\n\n\n\nFigure¬†7.6: Sketch of the t-distribution with 74 degrees-of-freedom. The observed value (purple dot) of -8.82 is also shown. The shaded blue area indicates the magnitude of t-values that would be expected if the null hypothesis is true.\n\n\n\n\nBeing able to create this sketch helps us understand how the observed data fit with or don‚Äôt fit with the null hypothesis. It also helps us understand the mechanics of what the computations for the t-test actually mean. In practice, you would not create this distribution for a manuscript, but rather report the pertinent information from these computations, namely the t-value, and the df for the t-distribution. In our example, we might report this as:\n\n\\(t(74)=-8.82\\)\n\nThis small amount of information allows another researcher to re-create the sketch of the t-distribution that we made in Figure¬†7.6. We can also see that the observed data is not very consistent with the null hypothesis. If the true mean amount of sleep for all teenagers is 9 hours, we would expect that the magnitude of an observed t-value would be between \\(-2.028\\) and \\(+2.2028\\). (The t-value of zero corresponds to an average of 9 hrs of sleep, but we expect deviation from this in a sample mean because of sampling variation.) In the data, we found a t-value of \\(-8.82\\)! This indicates that the sample mean for the observed data was 8.82 standard errors below the expected t-value of 0. Moreover, a t-value of \\(-8.82\\) is quite a bit lower than we would expect if teenagers actually average 9 hours of sleep a night."
  },
  {
    "objectID": "04-03-one-sample-test.html#quantifying-the-level-of-consistency-with-the-null-hypothesis-the-p-value",
    "href": "04-03-one-sample-test.html#quantifying-the-level-of-consistency-with-the-null-hypothesis-the-p-value",
    "title": "7¬† Hypothesis Testing: One-Sample t-Test",
    "section": "7.3 Quantifying the Level of Consistency with the Null Hypothesis: The p-Value",
    "text": "7.3 Quantifying the Level of Consistency with the Null Hypothesis: The p-Value\nComputing the observed t-value gives us a method for determining how far (in SE units) the sample mean for the observed data are from a mean value specified in the null hypothesis. By placing the observed t-value in the appropriate t-distribution, we can also say whether the observed data are consistent with the claim made in the null hypothesis. Applied researchers also augment this information with one more piece of evidence called the p-value.\nThe p-value provides a quantification of the probability of observing data at least as extreme as what we observed if the null hypothesis is true. In other words, with the t-value and t-distribution w can say that it is unlikely that we would observe a sample mean as small as 7.5 if teenagers really do average 9 hours of sleep a night. The p-value will take this one step further and quantify exactly how unlikely that would be.\nThe computation of the p-value is based around the alternative (research) hypothesis. Recall that the alternative hypothesis was a statement of inequality about the population mean value. In our sleep example the alternative hypotheis was:\n\\[\nH_A: \\mu &lt; 9\n\\]\nBut it could also have been one of these other inequalities depending on the researcher‚Äôs hypothesis about how the population mean compared to 9 hours.\n\\[\n\\begin{split}\nH_A: \\mu &gt; 9 \\\\[1ex]\nH_A: \\mu \\neq 9\n\\end{split}\n\\] In computing p-value, we have to identify values that are at least as extreme as the observed data. Extremeness varies depending on the direction of the inequality. For example in the example alternative hypothesis that we had: \\(H_A: \\mu &lt; 9\\), a value more extreme than our observed sample mean of 7.5 would be less than 7.5. So to compute the p-value for this alternative hypothesis, w need to find:\n\\[\nP(\\bar{y} \\leq 7.5) ~~~ \\text{if the null hypothesis is true}\n\\]\nNote that \\(P(\\cdot)\\) is the notation to indicate the probability of whatever is in the parentheses. In our case we are finding the probability of a sample mean (\\(\\bar{y}\\)) that is at least as extreme as the one we saw in our observed data (7.5) where extreme is defined in the alternative hypothesis (\\(\\leq\\)).\nTo find this probability we have to go back to the null distribution‚Äîwhich is based on the null hypothesis being true. (The probability defined above assumed the null hypothesis to be true.) We then need to identify all values that are less than or equal to the observed value and find their probability within that distribution. Typically, we do this in the t-distribution, so rather than finding the probability of values less than or equal to 7.5, we need to find:\n\\[\nP(t \\leq -8.82)\n\\]\nAs an example, consider the dotplot in Figure¬†7.7.\n\n\n\n\n\nFigure¬†7.7: Dot plot of an example t-distribution. The vertical blue dashed line indicates the observed value of -8.82. The yellow dots (to the left of -8.82) are the t-values that are at least as extreme as -8.82 based the alternative hypothesis (mu &lt; -8.82).\n\n\n\n\nThis plot shows 300 t-values that are part of a t-distribution created by assuming the null hypothesis was true. Of these 300 t-values, only 2 of them are less than or equal to the observed value of \\(-8.82\\). Thus we can compute the probability of observing data at least as extreme as what we observed as:\n\\[\n\\begin{split}\nP(t \\leq -8.82) &= \\frac{2}{300} \\\\[1ex]\n&= .007\n\\end{split}\n\\]\nThat is, if the null hypothesis is true, the probability we would observe a sample mean at least as extreme as we did is .007. (We report p-values as: p = .007) This is a very unlikely event if the null hypothesis is true. So because we did actually observe a mean this extreme, it causes us to reject the null hypothesis in favor of the alternative hypothesis. The empirical evidence does not seem consistent with teenagers getting 9 hours of sleep a night. It is more consistent with teenagers getting less than 9 hours of sleep a night, on average.\n\nRule-of-Thumb: A p-value that is less than .05 usually is evidence against the null hypothesis in favor of the alternative hypothesis. In contrast, a p-value that is .05 or higher means that the evidence is consistent with the null hypothesis.\nBeing consistent with the null hypothesis does not mean that the null hypothesis is necessarily true, but rather that it could be true. Because of this, if \\(p \\geq .05\\) we never ‚Äúaccept the null hypothesis‚Äù, but instead we ‚Äúfail to reject the null hypothesis‚Äù.\n\n\n\n7.3.1 Computing the p-Value for Other Alternative Hypotheses\nIn the example, we computed the p-value based on the alternative hypothesis, \\(H_A:\\mu&lt;9\\). To do this we counted the cases in the t-distribution that were more extreme than our observed t-value of \\(-8.82\\), which in this alternative hypothesis corresponded to the t-values that were less than or equal to \\(-8.82\\) and computed a probability (proportion) by dividing by the total number of values in the distribution. To compute this for other null hypotheses, we do the same thing, but we have to re-define extreme.\nFor example if we had the alternative hypothesis \\(H_A:\\mu&gt;9\\), values at least as extreme as \\(-8.82\\) correspond to all the values that are greater than or equal to \\(-8.82\\). These are shown a the yellow cases in the distribution below.\n\n\n\n\n\nFigure¬†7.8: Dot plot of an example t-distribution. The vertical blue dashed line indicates the observed value of -8.82. The yellow dots (to the right of -8.82) are the t-values that are at least as extreme as -8.82 based the alternative hypothesis (mu &gt; -8.82).\n\n\n\n\nBased on this alternative hypothesis, we would have counted 298 cases that have a t-value greater than or equal to \\(-8.82\\). Based on this alternative hypothesis, the p-value would be computed as:\n\\[\n\\begin{split}\np &= \\frac{298}{300} \\\\[1ex]\n&= .993\n\\end{split}\n\\]\nThis level of evidence does not support the alternative hypothesis since the p-value is not less than .05. Because of this, we do not think the average amount of sleep teenagers get a night is greater than 9 hours. The empirical evidence doesn‚Äôt support this. However, it isn‚Äôt clear from this test that the empirical evidence supports that students get 9 hours of sleep (i.e., \\(\\mu=9\\)); it may be they are getting less than 9 hours of sleep (\\(\\mu&lt;9\\)). That is why we cannot accept the null hypothesis that \\(\\mu=9\\). The test has only ruled out values for \\(\\mu\\) that are greater than 9 hours; the mean actually being 9 hours is only one possibility of many that remain after we eliminate those values greater than 9!\nAnother potential alternative hypothesis that a researcher might have is \\(H_A:\\mu\\neq9\\). In this research hypothesis the researcher is not positing a direction‚Äîthey are just saying we think it differs from 9 hours; it might be higher, it might be lower. What this means for identifying cases in the t-distribution that are at least as extreme as \\(-8.82\\) is that we have to identify all values less than or equal to \\(-8.82\\) AND all values greater than or equal to \\(+8.82\\). These are shown a the yellow cases in the distribution below.\n\n\n\n\n\nFigure¬†7.9: Dot plot of an example t-distribution. The vertical blue dashed lines indicates the observed value of -8.82 and its counterpart at +8.82. The yellow dots (to the left of -8.82 and to the right of +8.82) are the t-values that are at least as extreme as -8.82 based the alternative hypothesis (mu ‚â† -8.82).\n\n\n\n\nBased on this alternative hypothesis, we would have counted 4 cases that have a t-value at least as extreme as \\(-8.82\\). Based on this alternative hypothesis, the p-value would be computed as:\n\\[\n\\begin{split}\np &= \\frac{4}{300} \\\\[1ex]\n&= .013\n\\end{split}\n\\]\nBased on this p-value, which is less than .05, we would reject the null hypothesis in favor of the alternative hypothesis. This implies that the empirical evidence does not support the claim that teenagers get, on average, 9 hours of sleep, but rather that they get a different amount of sleep on average. Based on the results of this test, we cannot tell whether they get, on average, more or less sleep than 9 hours‚Äîonly that it is likely a different amount.\n\n\n\n7.3.2 p-Values in Density Plots\nIn the previous example we have looked at to compute the p-value, the t-distribution was presented as a dotplot. This makes it easy to count the observations at least as extreme as the observed value. In most cases, the t-distribution is presented as a density plot. Because individual cases are not shown in a density plot, we need to have another method of computing the p-value hat is not based on counting.\nThe method we use with density plots is to compute the area under the density curve that corresponds to at least as extreme as the observed value. Figure¬†7.10 shows both the dotplot and superimposed density curve for an example t-distribution. If the alternative hypothesis was \\(H_A:\\mu&lt;9\\), rather than counting the cases to that are less than the observed value of \\(-8.82\\), we would find the area under the curve that is less than \\(-8.82\\). This area is shaded in the figure.\n\n\n\n\n\nFigure¬†7.10: Dot plot of an example t-distribution wityh superimposed density. The vertical blue dashed line indicates the observed value of -8.82. The yellow dots (to the left of -8.82) are the t-values that are at least as extreme as -8.82 based the alternative hypothesis (mu &lt; -8.82). The area under the density curve that is less than -8.82 is also shaded.\n\n\n\n\nFinding the area under the density curve requires calculus, or software. We will show you how to use R to find this area in ?sec-using-r-for-t-tests. We can, however estimate this area for a quick approximation. Since the area under the entire density curve is 1, the shaded area (p-value) is found by determining the proportion that the shaded area is of the whole curve. (Remember that a proportion is a value between 0 and 1; it is not a percent.) In this example, the shaded area is roughly .01 of the whole curve, so we would say the p-value was .01.\n\nIt is very difficult to get an accurate p-value from estimating it from the density curve, especially when the p-value is small. In practice, we always use software to obtain the p-value. However, understanding that the software is calculating the area under the density curve is useful for ‚Äúgut-checking‚Äù the size of the p-value that the software gives us. For example, based on the shaded area in Figure¬†7.10, we would not expect a p-value of 0.5 since the shaded are is not half of the whole curve."
  },
  {
    "objectID": "04-03-one-sample-test.html#putting-it-all-together",
    "href": "04-03-one-sample-test.html#putting-it-all-together",
    "title": "7¬† Hypothesis Testing: One-Sample t-Test",
    "section": "7.4 Putting It All Together",
    "text": "7.4 Putting It All Together\nIt is important to not lose the forest in the trees when you are conducting a hypothesis test. We set out to answer a substantive question about whether or not teens are getting the recommended amount of sleep. All of the steps we carried out in the hypothesis test were a means to an end of actually answering this question based on the data we collected. So when we report results from the t-test, we need to not only report the pertinent statistical evidence (t-value, df, p-value), but we also need to answer the substantive/research question that drove this test in the first place. Below is an example write-up that an applied researcher might use:\n\nTo determine whether or not teens are getting the recommended amount of sleep, a one-sample t-test was used to compare the sample mean amount of sleep for 75 teens to a hypothesized population mean of 9 hours (the amount of sleep recommended by medical experts). The sample mean of 7.40 hours of sleep (SD = 1.52) was found to be inconsistent with the hypothesis that teens are getting 9 (or more) hours of sleep a night, on average; \\(t(74) = ‚àí8.82\\), \\(p = .007\\). This suggest that teens might not be getting the recommended amount of sleep every night."
  },
  {
    "objectID": "04-03-one-sample-test.html#looking-forward",
    "href": "04-03-one-sample-test.html#looking-forward",
    "title": "7¬† Hypothesis Testing: One-Sample t-Test",
    "section": "7.5 Looking Forward",
    "text": "7.5 Looking Forward\nIn the next chapter, we will introduce how to use R to carry out the one-sample t-test. Then in chapter ?sec-one-sample-examples, you will get you a chance to practice working through the process of carrying out a one-sample t-test.\nAs you work through these chapters, you will become more comfortable with the vocabulary and ideas that underlie hypothesis tests. This same set of vocabulary and ideas will come up again when we use hypothesis tests when we compare a sample proportion to a standard and to compare two samples. Because of this it may be useful to put together a summary of the ideas and vocabulary from this chapter that you can refer to (e.g., on a notecard). Here are some of the ideas and vocabulary that are important in hypothesis testing to get you started:\n\nStatistical inference\nNull hypothesis\nNull model\nAlternative hypothesis\nStandard error\nt-value\nt-distribution\np-value"
  },
  {
    "objectID": "04-03-one-sample-test.html#footnotes",
    "href": "04-03-one-sample-test.html#footnotes",
    "title": "7¬† Hypothesis Testing: One-Sample t-Test",
    "section": "",
    "text": "a priori means the hypothesis is made prior to collecting any data‚Ü©Ô∏é\nTechnically, we computed the SE as 0.17 in Chapter¬†6.‚Ü©Ô∏é"
  },
  {
    "objectID": "04-04-one-sample-test-computation.html#teen-sleep-a-quick-re-cap",
    "href": "04-04-one-sample-test-computation.html#teen-sleep-a-quick-re-cap",
    "title": "8¬† One-Sample t-Test Using R",
    "section": "8.1 Teen Sleep: A Quick Re-Cap",
    "text": "8.1 Teen Sleep: A Quick Re-Cap\nIn this case study, researchers collected data on the bedtime, wake-up time, and hours slept for a sample of \\(n=75\\) American teens in Grades 9‚Äì12. These data were used to evaluate the following statistical hypotheses For example, here are a set of potential hypotheses about teen sleep:\n\\[\n\\begin{split}\nH_0: \\mu = 9 \\\\[1ex]\nH_A: \\mu &lt; 9\n\\end{split}\n\\] The analysis started by importing the data and visualizing and numerically describing the amopunt of sleep for the teens in our sample.\n\n# Create density plot\ngf_density(\n  ~ hrs_sleep, data = teen_sleep,\n  color = \"black\", \n  fill = \"skyblue\",\n  xlab = \"Hours of Sleep\",\n  ylab = \"Density\"\n  )\n\n# Compute numerical summaries\ndf_stats(~hrs_sleep, data = teen_sleep)\n\n\n\n\n\n\nFigure¬†8.1: Density plot of the hours of sleep for 75 teenagers.\n\n\n\n\n\n\nFigure¬†8.2: Density plot of the hours of sleep for 75 teenagers.\n\n\n\n\nThese analyses suggest that, on average, the 75 teens in the sample are not getting the recommended 9 hours of sleep a night. They seem to be getting much less sleep on average, with a typical teen in the sample getting around 7.5 hours of sleep a night (SD = 1.5). To evaluate whether this lower amount of sleep we are seeing in the sample data is only a function of sampling uncertainty, we will carry out a one-sample t-test. To do this, we need to convert our sample mean to a t-value and then evaluate it in a t-distribution with \\(n-1\\) df.\n\\[\nt = \\frac{\\mathrm{Obs.~Mean} - \\mathrm{Hyp.~Mean}}{SE}\n\\]\nWe have the observed mean (\\(\\bar{x}=7.39\\)), and the hypothesized mean (\\(\\mu=9\\)) from the data and null hypothesis, respectively. To obtain the SE we bootstrapped from the data.\n\n# Draw a bootstrap sample of 75 observations and compute the mean\n# Do this 1000 times \n# Assign these into an object called bootstrap_means\nset.seed(42)\nbootstrap_means &lt;- do(1000) * {mean(sample(teen_sleep$hrs_sleep, size = 75, replace = TRUE))}\n\n# Compute numerical summaries to get SE\ndf_stats(~result, data = bootstrap_means)\n\n\n\n  \n\n\n\nBased on the bootstrapping, the SE is 0.170. Putting this together, we compute the t-value as:\n\\[\n\\begin{split}\nt &= \\frac{7.39 - 9}{.170} \\\\[2ex]\n&= -9.47\n\\end{split}\n\\] We can then sketch the t-distribution with 74 df, include the t-value we just computed, and shade the area under the density plot that corresponds to the alternative hypothesis.\n\n\n\n\n\nFigure¬†8.3: Density plot of the t-distribution of the sample means based on the thought experiment underlying a hypothesis test assuming that the mean amount of sleep for all tennagers is 9 hours (t value of 0). The blue shaded area represents the t-values we expect if the null hypothesis is true. The pink dot represents the observed t-value of -9.47.\n\n\n\n\nThe p-value (proportion of the pink shaded area to the whole area under the curve) is quite small. Because it is so small, it is difficult to even estimate its size‚Äî\\(p&lt;.001\\). This small p-value leads us to reject the null hypothesis, indicating that the data suggest that the average amount of sleep teens are getting is likely less than 9 hours and that this result is not only because of sampling uncertainty. That is, the empirical evidence is pointing us to the conclusion that teens are not getting the recommended amount of sleep."
  },
  {
    "objectID": "04-04-one-sample-test-computation.html#using-the-t_test-function",
    "href": "04-04-one-sample-test-computation.html#using-the-t_test-function",
    "title": "8¬† One-Sample t-Test Using R",
    "section": "8.2 Using the t_test() Function",
    "text": "8.2 Using the t_test() Function\nRather than bootstrapping the SE, we will use the t_test() function to compute the SE directly. This function is part of the {mosaic} library, and takes the following arguments:\n\nA formula using the tilde (~), similar to the gf_ and df_stats functions, that specifies the attribute to carry out the one-sample t-test on.\ndata= specifying the name of the data object,\nmu= indicating the value of the mean in the null hypothesis,\nalternative= indicating one of three potential alternative hypotheses: \"less\", \"greater\", or \"two.sided\" (not equal). Note that these need to be enclosed in quotation marks.\n\nTo carry out the one-sample t-test in the earlier case study, we will use the following syntax. We assign the results of this t-test to an object (in this case, I called it my_t).\n\n# One-sample t-test\nmy_t &lt;- t_test(~hrs_sleep, data = teen_sleep, mu = 9, alternative = \"less\")\n\nTo see the results of the test, you can just call my_t, or whatever you named the object storing the t-test results. The output, however, is a bit unorganized. Instead, we are going to use two functions from the {educate} package to view the results of the t-test: t_results() and plot_t_dist(). To use these functions, we will need to load the {educate} library. Then, we can use each of these functions by supplying it with the name of the object storing our t-test results. We begin by using the t_results() function.\n\n# Load educate library\nlibrary(educate)\n\n# View t-test results\nt_results(my_t)\n\n\n--------------------------------------------------\nOne Sample t-test\n--------------------------------------------------\n\nH[0]: mu = 9\nH[A]: mu &lt; 9\n\nt(74) = -9.150303\np = 4.328872e-14\n\n--------------------------------------------------\n\n\nThis function outputs the null and alternative hypotheses being tested in the one-sample t-test. It also provide the observed t-value (\\(-9.15\\)) and the df (74) for the t-distribution. Finally, it outputs the p-value for the test. When p-values are really small, R will output the p-value in scientific notation. The e-14 part of the p-value means \\(\\times 10^{-14}\\), which means, move the decimal point 14 places to the left. Thus the p-value is:\n\\[\n\\begin{split}\np &= 4.328872 \\times 10^{-14} \\\\[2ex]\n&= .0000000000000433\n\\end{split}\n\\] Note that the t-value we get from this function was different than the t-value we got earlier. This is because the SE computed by the t_test() function is different than the SE we get when we bootstrap. Because of this, it is very important to indicate the method you used to get the t-value; was it based on bootstrapping a SE? Or did you use the t_test() function, which uses a normal-based method for computing the SE?\nWe can also use the plot_t_dist() to visualize the t-distribution with 74 df, where our observed t-value of \\(-9.15\\), falls in this distribution, and the shaded area under the curve associated with the p-value based on the specified alternative hypothesis. The results form the t-test will also be printed above the plot.\n\n# View t-distribution\nplot_t_dist(my_t)\n\n\n\n\nFigure¬†8.4: Density plot of the t-distribution of the sample means based on the thought experiment underlying a hypothesis test assuming that the mean amount of sleep for all tennagers is 9 hours (t value of 0). The red vertical line represents the observed t-value of -9.15. The shaded area under the curve to the left of -9.15 shows the associated p-value of \\(4.33\\times10^{-14} = .0000000000000433\\) that corresponds to the alternative hypothesis that \\(\\mu&lt;9\\)."
  },
  {
    "objectID": "04-04-one-sample-test-computation.html#case-study-2-continuous-assessment",
    "href": "04-04-one-sample-test-computation.html#case-study-2-continuous-assessment",
    "title": "8¬† One-Sample t-Test Using R",
    "section": "8.3 Case Study 2: Continuous Assessment",
    "text": "8.3 Case Study 2: Continuous Assessment\nTo study the practice of continuous assessment in Ethiopian primary schools, Abejehu (2016) collected survey responses from several primary school teachers. One tenet of continuous assessment is that to evaluate learning, teachers need to understand students‚Äô prior knowledge. One item on the survey asked teachers about this: ‚ÄúI always assess students‚Äô prior knowledge before starting new lesson.‚Äù Teachers responded on a Likert scale, with options: Strongly Agree (4), Agree (3), Disagree (2), and Strongly Disagree (1). The responses for 30 teachers is given in the prior_knowledge attribute of the continuous-assessment.csv file (see codebook for additional detail).\n\n# Import data\ncontinuous_assessment &lt;- read_csv(\"https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/continuous-assessment.csv\")\n\nRows: 30 Columns: 3\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (3): prior_knowledge, only_achievement, prompt_feedback\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View data\ncontinuous_assessment\n\n\n\n  \n\n\n\nTo evaluate whether Ethiopian primary teachers are measuring students‚Äô prior knowledge, we will analyze the data in the prior_knowledge attribute. Because there is not substantive work on whether teachers actually do or do not assess students‚Äô prior knowledge, we don‚Äôt have a priori conjectures about whether they will generally agree (3 or 4) or disagree (1 or 2) with the statement in the survey item. Because of that, we will examine the following set of potential hypotheses:\n\\[\n\\begin{split}\nH_0: \\mu = 2.5 \\\\[1ex]\nH_A: \\mu \\neq 2.5\n\\end{split}\n\\] Before we carry out a hypothesis test, we should always explore the data by creating visualizations and numerical summaries of the attribute. Because the data in the attribute is more discrete (can only be 1‚Äì4 with no values in between), we will create a histogram rather than a density plot of the responses. We will also set the bins= argument to 4 since there are only four possible response options.\n\n# Create histogram\ngf_histogram(\n  ~prior_knowledge, data = continuous_assessment,\n  bins = 4,\n  color = \"black\", \n  fill = \"skyblue\",\n  xlab = \"Level of Agreement\",\n  ylab = \"Count\"\n  )\n\n# Compute numerical summaries\ndf_stats(~prior_knowledge, data = continuous_assessment)\n\n\n\n\n\n\nFigure¬†8.5: Histogram of teachers responses to the survey item: I always assess students‚Äô prior knowledge before starting new lesson.\n\n\n\n\n\n\nFigure¬†8.6: Histogram of teachers responses to the survey item: I always assess students‚Äô prior knowledge before starting new lesson.\n\n\n\n\nThe histogram suggests that the distribution of responses is somewhat symmetric, with roughly an equal number of teachers assessing (3 and 4) and not assessing (1 and 2) students‚Äô prior knowledge. Most teachers did not indicate strong agreement nor strong disagreement. The average response is 2.43, which indicates that a typical teacher does not assess students‚Äô prior knowledge. However, the relatively large SD (0.90) suggests that there is a great deal of individual variation in the responses. Next, we carry out a one-sample t-test.\n\n# One-sample t-test\nmy_t &lt;- t_test(~prior_knowledge, data = continuous_assessment, mu = 2.5, alternative = \"two.sided\")\n\n# View t-test results\nt_results(my_t)\n\n\n--------------------------------------------------\nOne Sample t-test\n--------------------------------------------------\n\nH[0]: mu = 2.5\nH[A]: mu ‚â† 2.5\n\nt(29) = -0.4067897\np = 0.6871492\n\n--------------------------------------------------\n\n# View t-distribution\nplot_t_dist(my_t)\n\n\n\n\nFigure¬†8.7: Density plot of the t-distribution of the sample means based on the thought experiment underlying a hypothesis test assuming that the mean response for all Ethiopian primary school teachers is 2.5 (t value of 0). The red vertical line represents the observed t-value of -0.41. The shaded area under the curve to the left of -0.41 and to the right of +0.41 shows the associated p-value of .687 that corresponds to the alternative hypothesis that \\(\\mu eq2.5\\).\n\n\n\n\nBased on the p-value of .0687, we would fail to reject the null hypothesis. We do not have evidence that the average response for all Ethiopian primary school teachers differs from 2.5; that is the empirical data is consistent with the hypothesis that the average response for all Ethiopian primary school teachers is 2.5.\n\nSUPER IMPORTANT NOTE\nJust because data are consistent with a hypothesis does not mean that hypothesis is true. As an example, consider a patient who goes to the doctor with a set of symptoms (e.g., aches, fever, congestion). The symptoms are the data the doctor will use to help make a diagnosis (hypothesis) which is consistent with the symptoms. However, there are likely several diagnoses that are consistent with the same set of symptoms. This is also true of hypotheses: The data can be consistent with several different hypotheses.\nIn our example, the data were consistent with the null hypothesis that the average response for all Ethiopian primary school teachers is 2.5. It turns out, that the data is also consistent with the hypothesis that the average response for all Ethiopian primary school teachers is 2.6. And 2.7, and 2.5. In fact, there are several different hypotheses that the data are consistent with. This is why we cannot say that the average response for all Ethiopian primary school teachers IS 2.5, but can only say that it IS CONSISTENT with the hypothesis that the average is 2.5."
  },
  {
    "objectID": "04-04-one-sample-test-computation.html#case-study-house-prices",
    "href": "04-04-one-sample-test-computation.html#case-study-house-prices",
    "title": "8¬† One-Sample t-Test Using R",
    "section": "8.4 Case Study: House Prices",
    "text": "8.4 Case Study: House Prices\nThe average price of a single-family house in Minneapolis is $322.46k (as of May 2023). Are houses near the University of Minnesota campus more expensive than that, on average? The data in zillow.csv include the listing price (in thousands of dollars) for 15 houses in neighborhoods adjacent to the UMN campus (e.g., Como, Marcy-Holmes, Cedar-Riverside). We will use these data to evaluate the following hypotheses:\n\\[\n\\begin{split}\nH_0: \\mu = 322.46 \\\\[1ex]\nH_A: \\mu &gt; 322.46\n\\end{split}\n\\]\n\n#| label: fig-house-prices\n#| fig-cap: \"Histogram of the asking price for 15 houses in neighborhoods adjacent to the UMN campus. \"\n#| warning: false\n#| \n# Import data\nzillow &lt;- read_csv(\"https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/zillow.csv\")\n\nRows: 15 Columns: 1\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (1): price\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View data\nzillow\n\n\n\n  \n\n\n# Create histogram\ngf_histogram(\n  ~price, data = zillow,\n  binwidth = 75,\n  color = \"black\", \n  fill = \"skyblue\",\n  xlab = \"House Price\",\n  ylab = \"Count\"\n  )\n\n\n\n# Compute numerical summaries\ndf_stats(~price, data = zillow)\n\n\n\n  \n\n\n\nThe sample distribution is left-skewed indicating that more of the houses are at the higher and of the price range. A typical single-family house near the UMN campus costs a little over 400 thousand dollars (M = $404.97k). There is a lot of variation in house price, with some as low as $250k and others as high as $550k (SD = $102.43k). The sample evidence supports the hypothesis that the average price of a house near the UMN campus costs more than the average house in Minneapolis. Next, we will carry out a one-sample t-test to determine whether this difference is more than we expect because of sampling variation.\n\n# One-sample t-test\nmy_t &lt;- t_test(~price, data = zillow, mu = 322.46, alternative = \"greater\")\n\n# View t-test results\nt_results(my_t)\n\n\n--------------------------------------------------\nOne Sample t-test\n--------------------------------------------------\n\nH[0]: mu = 322.46\nH[A]: mu &gt; 322.46\n\nt(14) = 3.1196\np = 0.003766644\n\n--------------------------------------------------\n\n# View t-distribution\nplot_t_dist(my_t)\n\n\n\n\nFigure¬†8.8: Density plot of the t-distribution of the sample means based on the thought experiment underlying a hypothesis test assuming that the mean cost for all houses near campus is $322.46k (t value of 0). The red vertical line represents the observed t-value of 3.12. The shaded area under the curve to the right of 3.12 shows the associated p-value of .004 that corresponds to the alternative hypothesis that \\(\\mu&gt;\\$322.46k\\).\n\n\n\n\nThe results of the t-test, \\(t(14)=3.12\\), \\(p = .004\\), indicate we should reject the null hypothesis. This suggests that the empirical evidence is consistent with the average cost of a house near the UMN campus being higher than the average cost of a house in Minneapolis more broadly."
  },
  {
    "objectID": "04-04-one-sample-test-computation.html#references",
    "href": "04-04-one-sample-test-computation.html#references",
    "title": "8¬† One-Sample t-Test Using R",
    "section": "8.5 References",
    "text": "8.5 References\n\n\n\n\nAbejehu, S. B. (2016). The Practice of Continuous Assessment in Primary Schools: The Case of Chagni, Ethiopia. Journal of Education and Practice."
  },
  {
    "objectID": "04-05-one-sample-test-assumptions.html#type-i-and-type-ii-errors",
    "href": "04-05-one-sample-test-assumptions.html#type-i-and-type-ii-errors",
    "title": "9¬† One-Sample t-Test: Errors and Assumptions",
    "section": "9.1 Type I and Type II Errors",
    "text": "9.1 Type I and Type II Errors\nThe conclusions we draw about hypotheses are based on a p-value. Remember the p in p-value stands for probability. That means the conclusions are probabilistic in nature. For example, in our teen sleep case study the p-value was .0000000000000433. If we were interpreting this value, we would say:\n\nIf the null hypothesis is true, the probability of seeing a sample mean at least as extreme as 7.5 (the mean we observed in the data), is .0000000000000433.\n\nFrom this we reasoned that because seeing the sample mean we observed was so unlikely that the null hypothesis was probably not true; we rejected the null hypothesis. In reality, however, this decision to reject might be wrong. It may be that the null hypothesis is true, and we just observed something that was a really, really low probability event. After all, the probability of seeing such an extreme sample mean did not have a zero probability.\n\n\n9.1.1 Type I Errors\nAnytime we reject the null hypothesis in a hypothesis test, we might make what is called a Type I error.1 A Type I error is rejecting the null hypothesis when it is, in fact, true. In our example this would mean concluding that the average amount of sleep for all teens was less than 9 hours, when in fact it really wasn‚Äôt.\nUnfortunately, when we reject the null hypothesis, we can never know if we made a Type I error or not. In order to determine that, you would actually need to know what the population mean was, in which case you wouldn‚Äôt have to test what it was. While we can‚Äôt know if we made a Type I error, we can set the probability of making a Type I error. The probability of making a Type I error is called \\(\\alpha\\) (alpha)2 and is the value we compare our p-value to. In the social sciences, this probability is almost always .05. That is why we compare our p-value to .05 in order to decide whether to reject the null hypothesis.\n\n\n\n9.1.2 Type II Errors\nA Type II error is failing to reject the null hypothesis when it is, in fact, false. For example, if we had found a p-value in the teen sleep case study that was bigger than .05, we would have failed to reject the null hypothesis. In this case, our conclusion would have been that the average amount of sleep for all teens was NOT less than 9 hours. Once again, once you have drawn a conclusion about the null hypothesis, there is some probability that the conclusion you drew was wrong.\nSimilar to our potential for making an error when we reject a null hypothesis, we can never know for sure whether we have made a Type II error or not after we fail to reject a null hypothesis. We can only know the probability of making a Type II error which is conventionally termed \\(\\beta\\) (beta)3. The value of \\(\\beta\\) is somewhat complex, but has an inverse relationship with the value of \\(\\alpha\\), such that as the value of \\(\\alpha\\) gets smaller, the value of \\(\\beta\\) gets larger, and vice versa. This means that if you make \\(\\alpha\\) smaller (to protect against making a Type I error), you will make the probability of making a Type II error larger!\n\nThe type of error you can make is completely dependent on the conclusion you draw about the null hypothesis. For example, if we reject the null hypothesis, the only type of error you can make is a Type I error. You cannot make a Type II error since that requires failing to reject the null hypothesis. That is, when you reject the null hypothesis, the probability of making a Type II error is 0. Similarly, if you fail to reject the null hypothesis, then you cannot make a Type I error; the probability of making a Type I error after you fail to reject the null hypothesis is 0.\n\n\n\n\n9.1.3 Some Practice with Type I and Type II Errors\n\nYour Turn\nIn the continuous assessment case study from Chapter¬†8, we evaluated the following set of hypotheses about whether, on average, Ethiopian primary school teachers agree/disagree with the statement that they assess students‚Äô prior knowledge:\n\\[\n\\begin{split}\nH_0: \\mu = 2.5 \\\\[1ex]\nH_A: \\mu \\neq 2.5\n\\end{split}\n\\]\nBased on the results of the one-sample t-test, \\(t(29) = -0.41\\), \\(p = 0.687\\), which type of error could we have made if we were using an \\(\\alpha\\)-value of .05? Explain.\n\nShow/Hide Solution\n\n\nUsing an \\(\\alpha\\)-value of .05, we would have failed to reject the null hypothesis since our p-value was bigger than .05. Because we failed to reject the null hypothesis, the only type of error we can make is a Type II error.\n\nUsing the context of the problem (i.e., survey responses of the assessment of prior knowledge) and the hypotheses tested, what does it mean to make this type of error?\n\nShow/Hide Solution\n\n\nA Type II error in this context indicates that we come to the conclusion that Ethiopian primary school teachers neither agree nor disagree with the statement that they assess students‚Äô prior knowledge on average, but in reality they actually do agree or disagree with that statement, on average.\n\nIn the house prices case study from Chapter¬†8, we evaluated the following set of hypotheses about whether, on average, houses near the University of Minnesota campus more expensive than $322.46k (the average price of a single-family house in Minneapolis as of May 2023).\n\\[\n\\begin{split}\nH_0: \\mu = 322.46 \\\\[1ex]\nH_A: \\mu &gt; 322.46\n\\end{split}\n\\]\nBased on the results of the one-sample t-test, \\(t(14) = 3.12\\), \\(p = 0.004\\), which type of error could we have made if we were using an \\(\\alpha\\)-value of .05? Explain.\n\nShow/Hide Solution\n\n\nUsing an \\(\\alpha\\)-value of .05, we would have rejected the null hypothesis since our p-value was smaller than .05. Because we rejected the null hypothesis, the only type of error we can make is a Type I error.\n\nUsing the context of the problem (i.e., house prices near the UMN campus) and the hypotheses tested, what does it mean to make a this type of error?\n\nShow/Hide Solution\n\n\nA Type I error in this context indicates that we rejected the null hypothesis that the average price of a house near the UMN campus is not more than $322.46k, but in reality it is not."
  },
  {
    "objectID": "04-05-one-sample-test-assumptions.html#assumptions",
    "href": "04-05-one-sample-test-assumptions.html#assumptions",
    "title": "9¬† One-Sample t-Test: Errors and Assumptions",
    "section": "9.2 Assumptions",
    "text": "9.2 Assumptions\nWhether or not the p-value we obtain from the t-test is accurate depends on a set of statistical assumptions about the population. If the assumptions we make about the population are erroneous, then all of our inferences that we draw about the population mean are dubious, since these inferences are based on the p-value. This implies that it is critical that we have some sense of whether these statistical assumptions we are making are valid.\nFor the one-sample t-test there are two statistical assumptions we make about the population:\n\nThe distribution of values in the population is normally distributed.\nThe values in the population are independent from each other.\n\nFor example, in the teen sleep case study we need to know that:\n\nIf you took the hours of sleep measurements from every teen and plotted them, the distribution would be normally distributed.\nThe hours of sleep measurements from every teen is independent from those of every other teen.\n\nUnfortunately, we can never know for sure whether these assumptions are met since we do not have data from the entire population. Instead, we have to decide whether these assumptions seem tenable based on the sample of data we have.\n\n\n9.2.1 Evaluating the Assumption of Normality\nTo evaluate the first assumption that the distribution of values in the population is normally distributed, we plot the sample data and then ask the question: Is this distribution close to normal? The density plot of the sample teen sleep data is shown in Figure¬†9.1.\n\n\n\n\n\nFigure¬†9.1: Density plot of the hours of sleep for 75 teenagers.\n\n\n\n\nThe sample distribution looks symmetric, but perhaps not exactly normal. This is okay since the assumption is about whether the POPULATION distribution is normal, not whether the sample distribution is normal. We are only asking whether we believe that the population distribution is normal based on what we see in the sample distribution. To answer this question, we need some idea of what sample distributions of size 75 look like if they truly do come from a population that is normally distributed. Below are the distributions for five samples of size 75 that were actually drawn from a normal distribution.\n\n\n\n\n\nFigure¬†9.2: Density plots for five samples of size 75 that were actually drawn from a population that was normally distributed.\n\n\n\n\nThe density plots in Figure¬†9.2 suggest that the distribution for a sample that was actually drawn from a normal distribution does not necessarily look exactly normal. For example the plots for Sample 1 and Sample 3 look slightly right-skewed, while the plots for Samples 2, 4, and 5 look more symmetric. Some of the plots look more peaked (Samples 1 and 2) while others look thicker in the middle (Samples 4 and 5). All of this suggests that the sample doesn‚Äôt have to look perfectly normal for it to have been drawn from a population that is normal.\nLooking back at the sample distribution of teen sleep measurements in Figure¬†9.1, it seems that this distribution is not so different from the distributions of the five other samples. This suggests that it too might have been drawn from a normally distributed population. In light of this, we would say:\n\nThe assumption of normality seems tenable given the distribution of the sample data.\n\n\n\n9.2.1.1 Sample Size and the Assumption of Normality\nHow different would the sample distribution have to look in order for us to conclude that the assumption of normality was not tenable given the distribution of the sample data? This is a difficult question to answer without other statistical tools.4 One tool that we rely on a great deal is something called the Central Limit Theorem (CLT). This theorem basically says that if the sample size of your data is over 30 that it doesn‚Äôt matter if the normality assumption is met, the p-value will not be impacted if the population is not normal.\nIn our example the sample size is 75. This means that it doesn‚Äôt matter if the population is normal or not‚Äîthe CLT basically says that we can assume that the p-value is not impacted by any deviation in the population distribution from normality. This is good news as it means we don‚Äôt have to guess whether or not our sample distribution of teen sleep measurements actually comes from a populatin that is normally distributed.\n\n\n\n\n9.2.2 Evaluating the Independence Assumption\nThe definition of independence relies on formal mathematics. Loosely speaking a set of observations is independent if knowing the value for one observation in the distribution conveys no information about the value for any other observation in the same distribution. If observations are not independent, we say they are dependent or correlated.\nTo evaluate the independence assumption we need to know something about the the study design, in particular how the data were collected. Using random chance to select the sample data (random sampling) will guarantee independence of the observations. There are also a few times that we can ascertain that the independence assumption would be violated. These instances are also a function of the data collection process. One such instance common to social science research is when the observations (i.e., cases, subjects) are collected within a physical or spatial proximity to one another. For example, this is typically the case when a researcher gathers a convenience sample based on location, such as sampling students from the same school. Another violation of independence occurs when observations are collected over time (longitudinally), especially when the observations are repeated measures from the same subjects.\nIndependence is often difficult to ascertain, and its tenability is made via a logical argument. In the teen sleep case study, for example, the study design did not employ random sampling‚Äîthe participants volunteered to be a part of the study‚Äîso we can not guarantee independence that way. (If a study uses random sampling in the data collection, it will say this directly in the paper.) On the other hand, all the participants did come from the same school district. This may violate the independence assumption since the teens in the study are all from the area (i.e., there was some degree of spatial proximity that influenced the data collection process).\nThe violation of independence is, however, not clear cut. The big question is does knowing that one teen‚Äôs sleep duration convey any information about any other teen‚Äôs sleep duration? If for example all the teens came from the same family this would likely be the case since teens in a family are often similar sleep schedules. It is less likely that this is a case for teens in the same school district. Because of this, we could argue that the independence assumption seems tenable.\n\nBecause the tenability of the independence assumption is made via a logical argument, outside the use of random sampling to collect the data, different scholars might disagree about whether this assumption is tenable. In this class, and in your own work, you need to lay out the argument for why you think the independence assumption is reasonable or not.\n\n\n\n\n9.2.3 Evaluating Assumptions in the Continuous Assessment Case Study\n\nYour Turn\nIn the continuous assessment case study from Chapter¬†8, we evaluated whether, on average, Ethiopian primary school teachers agree/disagree with the statement that they assess students‚Äô prior knowledge. The histogram of the 30 teachers‚Äô responses is shown below. Based on this plot and the sample size, do you believe the normality assumption is tenable?\n\n\n\n\n\nFigure¬†9.3: Histogram of teachers responses to the survey item: I always assess students‚Äô prior knowledge before starting new lesson.\n\n\n\n\n\nShow/Hide Solution\n\n\nYes, the normality assumption seems tenable. While the distribution of the sample responses is not normally distributed, it is not egregiously skewed. Moreover, since the sample size is 30, the CLT would suggest that we can assume that the p-value is not going to be impacted by any deviation in the population distribution from normality.\n\nBased on the data collection process described in the data codebook, do you believe the independence assumption is tenable?\n\nShow/Hide Solution\n\n\nYes, the independence assumption is tenable. The use of random chance to select the sample data (random sampling) guarantees independence.\n\n\n\n\n\n9.2.4 Evaluating Assumptions in the House Prices Case Study\n\nYour Turn\nIn the house prices case study from Chapter¬†8, we evaluated whether, on average, houses near the University of Minnesota campus are more expensive than $322.46k (the average price of a single-family house in Minneapolis as of May 2023). The histogram of the 15 sample house prices is shown below. Based on this plot and the sample size, do you believe the normality assumption is tenable?\n\n\n\n\n\nFigure¬†9.4: Histogram of the asking price for 15 houses in neighborhoods adjacent to the UMN campus.\n\n\n\n\n\nShow/Hide Solution\n\n\nThe normality assumption may not be tenable. The distribution of the sample responses is not normally distributed, and is clearly left-skewed. Moreover, since the sample size is less than 30, we can not rely on the CLT.\n\nBased on the data collection process described in the data codebook, do you believe the independence assumption is tenable?\n\nShow/Hide Solution\n\n\nIt is unclear that the independence assumption is tenable. The data collection did not use random chance to select the sample data. Moreover, it is likely that knowing the sale price for one house would give us information about the sale price for other houses since houses in the same neighborhood are often sold for similar prices."
  },
  {
    "objectID": "04-05-one-sample-test-assumptions.html#references",
    "href": "04-05-one-sample-test-assumptions.html#references",
    "title": "9¬† One-Sample t-Test: Errors and Assumptions",
    "section": "9.3 References",
    "text": "9.3 References"
  },
  {
    "objectID": "04-05-one-sample-test-assumptions.html#footnotes",
    "href": "04-05-one-sample-test-assumptions.html#footnotes",
    "title": "9¬† One-Sample t-Test: Errors and Assumptions",
    "section": "",
    "text": "Conventionally, we use Roman numerals to indicate the different types of errors we can make in hypothesis testing.‚Ü©Ô∏é\nFun Fact: \\(\\alpha\\) is the first letter in the Greek alphabet (equivalent to the Roman letter ‚Äúa‚Äù) so that is why it is associated with the first type (i.e., Type I) error.‚Ü©Ô∏é\nFun Fact: \\(\\beta\\) is the second letter in the Greek alphabet (equivalent to the Roman letter ‚Äúb‚Äù) so that is why it is associated with the second type (i.e., Type II) error.‚Ü©Ô∏é\nIf you go on and take EPsy 8251, you will learn about some of these tools.‚Ü©Ô∏é"
  },
  {
    "objectID": "04-06-one-sample-test-proportions.html#case-study-lead-levels-in-minnesota-children",
    "href": "04-06-one-sample-test-proportions.html#case-study-lead-levels-in-minnesota-children",
    "title": "10¬† One-Sample z-Test: Evaluating Proportions Against a Standard",
    "section": "10.1 Case Study: Lead Levels in Minnesota Children",
    "text": "10.1 Case Study: Lead Levels in Minnesota Children\nLead exposure has been shown to have deleterious effects on peoples‚Äô health and well-being, especially children. The Center for Disease Control collects blood lead surveillance data from all 50 states in the United States. In 2012, the proportion of children tested under 6 years of age that had lead levels in their blood above the Minnesota Department of Health (MDH) reference level for high blood lead levels (5¬µg/dL) was .029. The data in mn-lead.csv contains the measured lead levels in the blood for all Minnesota children tested under 6 years of age in 2018. There is also an attribute (ebll) that indicates whether the lead level indicates an elevated blood lead level according to MDH (i.e., is blood lead level above 5 ¬µg/dL).\n\nlibrary(educate)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(mosaicCore)\nlibrary(tidyverse)\n\n\n# Import data\nmn_lead &lt;- read_csv(\"https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/mn-lead.csv\")\n\n# View data\nmn_lead\n\n\n\n  \n\n\n\nOne question health officials might ask is: Is the proportion of all Minnesota children under 6 years of age who are above the MDH reference level in 2018 lower than the proportion in 2012? That is, they might wish to examine the following hypotheses:\n\\[\n\\begin{split}\nH_0: \\pi_{\\text{Above MDH}} = .029 \\\\[1ex]\nH_A: \\pi_{\\text{Above MDH}} &lt; .029\n\\end{split}\n\\] where \\(\\pi_{\\text{Above MDH}}\\) (the Greek letter equivalent of ‚Äúp‚Äù) is the proportion of all Minnesota children under 6 years of age who are above the MDH reference level. (Note that it is convention to indicate the group you are hypothesizing the proportion for in the subscript of \\(\\pi\\).) To answer this question, we can compute the proportion of Minnesota children under 6 years of age (who were tested) who have above blood lead levels above the MDH reference level in 2018. We can then carry out a one-sample z-test to see if any differences between the 2012 and 2018 proportions are just due to sampling error.\n\n\n10.1.1 Summarizing the Sample Data\nWe will start the analysis by summarizing the ebll attribute to determine the proportion of Minnesota children in 2018 under 6 years of age (who were tested) who have above blood lead levels above the MDH reference level.\n\n# Syntax to compute the proportion for each category in the control attribute\ndf_stats(~ebll, data = mn_lead, props)\n\n\n\n  \n\n\n\nThese summaries indicates that in 2018, the proportion of Minnesota children under 6 years of age (who were tested) who have above blood lead levels above the MDH reference level was .0139. The notation we use to denote a sample proportion is \\(\\hat{p}\\).\n\\[\n\\hat{p} = .0139\n\\]\nThis is a lower proportion than was found in the 2012 data by .0151. The next question we would want to tackle is whether this difference is more than we expect because of sampling error. To determine this, we need to carry out a hypothesis test.\n\n\n\n10.1.2 Testing Proportions Using the One-Sample z-Test\nThe hypothesis test we use to compare a sample against a known standard is the one-sample z-test. The process we use is very similar to that for the one-sample t-test, which was:\n\nUse the sample proportion to compute a t-value;\nLocate the observed z-value in the t-distribution; and\nDetermine the p-value by computing the area under the curve in the t-distribution that is at least as extreme as the observed t-value based on the alternative hypothesis.\n\nFor the one-sample z-test, the process is:\n\nUse the sample proportion to compute a z-value;\nLocate the observed z-value in the z-distribution; and\nDetermine the p-value by computing the area under the curve in the z-distribution that is at least as extreme as the observed z-value based on the alternative hypothesis.\n\nTo compute the z-value, we use:\n\\[\nz = \\frac{\\hat{p} - \\pi}{SE_{\\hat{p}}}\n\\]\nwhere \\(\\hat{p}\\) is the sample proportion, \\(\\pi\\) is the value hypothesized in the null hypothesis, and \\(SE_{\\hat{p}}\\) is the standard error for the proportion. This SE is computed as:\n\\[\nSE_{\\hat{p}} = \\sqrt{\\frac{\\pi(1 - \\pi)}{n}}\n\\] where again, \\(\\pi\\) is the hypothesized proportion in the null hypothesis and \\(n\\) is the sample size. In our example, the z-value is:\n\\[\n\\begin{split}\nz &= \\frac{.01391403 - .029}{\\sqrt{.029(1 - .029)/91706}} \\\\[1ex]\n&= \\frac{-.0151}{.0006} \\\\[1ex]\n&= -27.22\n\\end{split}\n\\]\nSimilar to a t-value, a z-value indicates how many standard errors the sample mean is from the hypothesized value, In our case, the sample proportion we computed in the data of .0139 is 27.22 standard errors below the hypothesized value of .029. We can evaluate this in the z-distribution.\nUnlike the t-distribution which is different depending on the df, there is only one z-distribution. The z-distribution is a normal distribution that has a mean of 0 and a standard deviation of 1. The z-distribution is shown in Figure¬†10.1.\n\n\n\n\n\nFigure¬†10.1: Density plot of the z-distribution.\n\n\n\n\nTo find the p-value associated with the alternative hypothesis that \\(\\pi&lt;.029\\), we will include the observed z-value of -27.22 into the z-distribution and shade the area under the curve that is less than the observed z-value. A sketch of this is shown in Figure¬†10.2.\n\n\n\n\n\nFigure¬†10.2: Sketch of the density plot of the z-distribution with the observed z-value of -27.22 also included. The shaded area to the left of -27.22 constitutes the p-value associate with the null hypothesis that the population proportion is less than .029.\n\n\n\n\nThe p-value here is going to be quite small since the area under the curve to the left of \\(-37.75\\) is quite small relative to the area under the whole curve.\nIn practice, we will use the prop_test() function from the {mosaic} package to carry out the one-sample z-test and compute the p-value. This function takes:\n\nA formula using the tilde (~), similar to the gf_ and df_stats functions, that specifies the attribute to carry out the one-sample z-test on. We also need to specify the level of the attribute we want to compute the sample proportion for using == and then giving the exact name for that level inside quotation marks.\ndata= specifying the name of the data object,\np= indicating the value of the proportion in the null hypothesis,\nalternative= indicating one of three potential alternative hypotheses: \"less\", \"greater\", or \"two.sided\" (not equal). Note that these need to be enclosed in quotation marks.\ncorrect=FALSE indicating that we want to do the calculation of the z-value without a correction for continuity which will mimic the formula.\n\nTo carry out the one-sample z-test, we will use the following syntax. Note that in the formula we also indicate that we want to compute the sample proportion for the \"Yes\" values. We assign the results of this z-test to an object (in this case, I called it my_z). Then we can use the z_results() and plot_z_dist() functions (both from the {educate} package) to show the results of the z-test and plot the z-distribution along with the observed z-value and shaded area associated with the p-value.\n\n# One-sample z-test\nmy_z &lt;- prop_test(\n  ~ebll == \"Yes\", \n  data = mn_lead, \n  p = .029, \n  alternative = \"less\",\n  correct = FALSE\n  )\n\n# Plot z-distribution, observed z-value and shaded p-value\nplot_z_dist(my_z)\n\n# Show z-test results\nz_results(my_z)\n\n\n--------------------------------------------------\n1-sample proportions test without continuity correction\n--------------------------------------------------\n\nH[0]: pi = 0.029\nH[A]: pi &lt; 0.029\nz = -27.22473\np = 1.655524e-163\n\n--------------------------------------------------\n\n\n\n\n\nFigure¬†10.3: Density plot of the z-distribution with the observed z-value of -27.22 also included. The shaded area to the left of -27.22 constitutes the p-value associate with the null hypothesis that the population proportion is less than .029.\n\n\n\n\nBased on the p-value, and using an \\(\\alpha\\)-value of .05, we would reject the null hypothesis. This suggests it is likely that the proportion of all Minnesota children under 6 years of age who are above the MDH reference level in 2018 is lower than the proportion in 2012. In this interpretation we call attention to the words ‚Äúit is likely‚Äù to remind you that it is possible we may have made a Type I error in which case the proportion of all Minnesota children under 6 years of age who are above the MDH reference level in 2018 is NOT lower than the proportion in 2012."
  },
  {
    "objectID": "04-06-one-sample-test-proportions.html#assumptions-for-the-one-sample-z-test",
    "href": "04-06-one-sample-test-proportions.html#assumptions-for-the-one-sample-z-test",
    "title": "10¬† One-Sample z-Test: Evaluating Proportions Against a Standard",
    "section": "10.2 Assumptions for the One-Sample z-Test",
    "text": "10.2 Assumptions for the One-Sample z-Test\nWhether or not the p-value we obtain from the z-test is accurate depends on the following set of statistical assumptions:\n\nThe values in the population follow a binomial distribution. This is true so long as there are only two values the attribute can take on (e.g., ‚ÄúYes‚Äù or ‚ÄúNo‚Äù).\nThe values in the population are independent from each other.\nThe quantities \\(n(\\hat{p})\\) and \\(n(1-\\hat{p})\\) are both greater than 10, where n is the sample size and \\(\\hat{p}\\) is the sample proportion value.\n\nTo evaluate the first assumption that the distribution of values in the population follow a binomial distribution, we only need to confirm that the population only has two values. In our example, this is true; the only two values a case can have is ‚ÄúYes‚Äù (blood lead level is above the MDH reference) or ‚ÄúNo‚Äù (blood lead level is not above the MDH reference).\nWe will evaluate the independence assumption the same way we did for the one-sample t-test, by referring to the study design. In our example, the cases in the data do not constitute a random sample of all Minnesota children. Does knowing that one Minnesota child‚Äôs blood lead level is above (or below) the MDH reference level give us any information about whether any other Minnesota child‚Äôs blood lead level is above (or below) the MDH reference level? Without additional data it is difficult to know, so we could argue that the independence assumption seems tenable.1\nLastly we compute the quantities in the third assumption and check that they are both greater than 10.\n\\[\n\\begin{split}\nn(\\hat{p}) &= 91706(.01391403) \\\\[1ex]\n&= 1276 \\\\[3ex]\nn(1-\\hat{p}) &= 91706(1 - .01391403) \\\\[1ex]\n&= 90430\n\\end{split}\n\\]\n\n\n10.2.1 Case Study: Rotten Tomatoes\nThe Rotten Tomatoes website includes both audiences‚Äô and critics‚Äô ratings for different movies. The ratings are then classified into one of two categories: ‚ÄúFresh‚Äù (which indicates a positive review) or ‚ÄúRotten‚Äù (which indicates a negative review). The data in fastx-reviews.csv includes the critic ratings (as of May 23, 2023) for the film Fast X (the 10th installment of the Fast & the Furious franchise). You will use the data in the fresh_rotten attribute to evaluate whether the proportion of all critics‚Äô ‚ÄúFresh‚Äù reviews is greater than .50. Mathematically the hypotheses you will evaluate are:\n\\[\n\\begin{split}\nH_0: \\pi_{\\text{Fresh}} = .50 \\\\[1ex]\nH_A: \\pi_{\\text{Fresh}} &gt; .50\n\\end{split}\n\\]\n\nYour Turn\nImport the fastx-reviews.csv data and compute the sample proportion of ‚ÄúFresh‚Äù reviews.\n\nShow/Hide Solution\n\n\n\n# Import data\nfastx &lt;- read_csv(\"https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/fastx-reviews.csv\")\n\n# Compute sample proportion of Fresh reviews\ndf_stats(~fresh_rotten, data = fastx, props)\n\n\n\n  \n\n\n\nThe sample proportion of ‚ÄúFresh‚Äù reviews is 0.538.\n\nWhich hypothesis the null or alternative, does the sample evidence support? Explain\n\nShow/Hide Solution\n\n\nThe sample evidence supports the alternative hypothesis since \\(\\hat{p}\\) is greater than the hypothesized value of 0.5.\n\nCompute the observed z-value using the formula given earlier in the chapter. Also interpret what this value tells you about how far the sample proportion is from the hypothesized value.\n\nShow/Hide Solution\n\n\n\\[\n\\begin{split}\nz &= \\frac{.5378151 - .50}{\\sqrt{.50(1 - .50)/238}} \\\\[1ex]\n&= \\frac{0.0378151}{0.03241019} \\\\[1ex]\n&= 1.17\n\\end{split}\n\\]\nThe observed z-value tells us that the sample proportion is 1.17 standard errors higher than the hypothesized value of 0.50.\n\nCarry out a one-sample z-test to evaluate whether the sample evidence is only due to sampling error. Report the pertinent results from this test, and use those resultsa to draw a conclusion about the hypotheses assuming an \\(\\alpha\\)-value of 0.05.\n\nShow/Hide Solution\n\n\n\n# One-sample z-test\nmy_z &lt;- prop_test(\n  ~fresh_rotten == \"fresh\", \n  data = fastx, \n  p = .50, \n  alternative = \"greater\",\n  correct = FALSE\n  )\n\n# Plot z-distribution, observed z-value and shaded p-value\n#plot_z_dist(my_z)\n\n# Show z-test results\nz_results(my_z)\n\n\n--------------------------------------------------\n1-sample proportions test without continuity correction\n--------------------------------------------------\n\nH[0]: pi = 0.5\nH[A]: pi &gt; 0.5\nz = -1.166767\np = 0.1216523\n\n--------------------------------------------------\n\n\nThe results of the one-sample z-test, \\(z=-1.17\\), \\(p=.122\\), suggest we should failt to reject the null hypothesis. It is likely that the population proportion of ‚ÄúFresh‚Äù reviews is not greater than 0.50.\n\nSketch a picture of the z-distribution (try to do this initially without using R). Then add a vertical line at the observed z-value. Finally, shade the area under the z-distribution that is assciated with the p-value. Check your sketch by using the plot_z_dist() function.\n\nShow/Hide Solution\n\n\nYour sketch should look like the distribution plotted in Figure¬†10.4.\n\n# Check your work\nplot_z_dist(my_z)\n\n\n\n\nFigure¬†10.4: Density plot of the z-distribution with the observed z-value of 1.17 also included. The shaded area to the right of 1.17 constitutes the p-value associate with the null hypothesis that the population proportion is greater than .50.\n\n\n\n\n\nBased on your decision, what type of error might you have made? Explain.\n\nShow/Hide Solution\n\n\nBecause we failed to reject the null hypothesis, we may have made a Type II error. It may be that the population proportion of ‚ÄúFresh‚Äù reviews actually is greater than 0.5 and we erroneously concluded it was not.\n\nCheck and evaluate all of the assumptions for the one-sample z-test.\n\nShow/Hide Solution\n\n\nThe assumptions are:\n\nThe values in the population follow a binomial distribution. This is true so long as there are only two values the attribute can take on (e.g., ‚ÄúYes‚Äù or ‚ÄúNo‚Äù).\nThe values in the population are independent from each other.\nThe quantities \\(n(\\hat{p})\\) and \\(n(1-\\hat{p})\\) are both greater than 10, where n is the sample size and \\(\\hat{p}\\) is the sample proportion value.\n\nThe first assumption is met‚Äîthe only two values for a review are ‚ÄúFresh‚Äù or ‚ÄúRotten‚Äù.\nThe second assumption also seems tenable. Although the sample is not chosen randomly, knowing one reviewer‚Äôs rating does not likely give us information about another reviewer‚Äôs rating. (If you said that the independence assumption is not tenable, you would need to provide an explanation as to why one reviewer‚Äôs rating gives us information about another reviewer‚Äôs rating. For example, reviewers read each others‚Äô reviews so one reviewer‚Äôs rating often influences another reviewer‚Äôs rating.)\nLastly we compute the quantities in the third assumption and check that they are both greater than 10. This is the case, so the third assumption is also met.\n\\[\n\\begin{split}\nn(\\hat{p}) &= 238(.5378151) \\\\[1ex]\n&= 128 \\\\[3ex]\nn(1-\\hat{p}) &= 238(1 - .5378151) \\\\[1ex]\n&= 110\n\\end{split}\n\\]"
  },
  {
    "objectID": "04-06-one-sample-test-proportions.html#references",
    "href": "04-06-one-sample-test-proportions.html#references",
    "title": "10¬† One-Sample z-Test: Evaluating Proportions Against a Standard",
    "section": "10.3 References",
    "text": "10.3 References"
  },
  {
    "objectID": "04-06-one-sample-test-proportions.html#footnotes",
    "href": "04-06-one-sample-test-proportions.html#footnotes",
    "title": "10¬† One-Sample z-Test: Evaluating Proportions Against a Standard",
    "section": "",
    "text": "Just remember other scholars might argue that the population values are not independent.‚Ü©Ô∏é"
  },
  {
    "objectID": "05-00-comparing-two-groups.html",
    "href": "05-00-comparing-two-groups.html",
    "title": "Comparing Two Groups",
    "section": "",
    "text": "Another task that is commonly performed in research is to compare the data you have collected from two different groups with the goal of inferring whether a particular population parameter differs between those groups. For example, is the average cost-of-living higher in Minneapolis than it is in St.¬†Paul? Or, is the proportion of students who own an automobile different for students living at home versus those living in the dorms?\nComparing two groups is one of the most important endeavors in social science and educational research. It is the basis of all experimental work (e.g., does the treatment group perform better, on average, than the control group?). It is also used in non-experimental work and is crucial in calling out societal injustices (e.g., are college-educated women earning less, on average, than college-educated men?).\nThere are many parallels between the one-sample methods you learned about in ?sec-compare-standard and the methods used to compare two groups. Similar to the one-sample tests you learned about, the methods you learn in this section will quantify the amount of uncertainty in a sample numerical estimate, but now there is uncertainty that needs to be quantified for estimates from two different samples of data. The assumptions underlying the methods we use to compare groups are also similar, but have to be performed on both groups and also include additional assumptions that the one-sample tests did not have."
  },
  {
    "objectID": "05-01-two-sample-t-test.html#case-study-cannabis-effects-on-iq",
    "href": "05-01-two-sample-t-test.html#case-study-cannabis-effects-on-iq",
    "title": "11¬† Two-Sample t-Test",
    "section": "11.1 Case Study: Cannabis Effects on IQ",
    "text": "11.1 Case Study: Cannabis Effects on IQ\nIn recent years, many states have legalized the use of marijuana for medical and recreational purposes. Marijuana use has been shown to have adverse impact on peoples‚Äô health and well-being (Volkow et al., 2014), including lng-term effects on IQ. This is especially true for adolescents and young adults since their brain is still developing. As you might expect, marijuana usage among younger people is quite prevalent. Based on data collected from the Center for Disease Control in 2019, 37% of U.S. high school students reported using marijuana at least once, and 22% reported use in the past 30 days.\nTo study the long-term effects of cannabis, Meier et al. (2012) examined data from a cohort of people who were followed for a 20 year time span. They identified participants who became persistent marijuana users during the course of the study and collected data on the change in IQ score between the start of the study (prior to the onset of any cannabis use) and then again 20 years later. Importantly, some of these participants were diagnosed with cannabis dependence prior to the 18 years of age (teen-onset) and others were not diagnosed with cannabis dependence until after 18 years of age (adult-onset). To determine whether the effects on IQ were more adverse for users who started using marijuana as a teen Meier et al.¬†posited that persistent marijuana users that became dependent as teens would have a bigger decline in IQ than those who became dependent as adults.\nWe will use the data in cannabis.csv to evaluate this hypothesis.\n\nlibrary(educate)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(tidyverse)\n\n\n# Import data\ncannabis &lt;- read_csv(\"https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/cannabis.csv\")\n\n# View data\ncannabis"
  },
  {
    "objectID": "05-01-two-sample-t-test.html#writing-the-set-of-hypotheses",
    "href": "05-01-two-sample-t-test.html#writing-the-set-of-hypotheses",
    "title": "11¬† Two-Sample t-Test",
    "section": "11.2 Writing the Set of Hypotheses",
    "text": "11.2 Writing the Set of Hypotheses\nWhen comparing two groups, the null hypothesis is that the population mean for the two groups is the same. In our example, the null hypothesis would be that the average decline in IQ scores for all persistent marijuana users that became dependent as adults is the same as the average decline in IQ scores for all persistent marijuana users that became dependent as teens. Mathematically:\n\\[\nH_0: \\mu_{\\text{Adult-onset}} = \\mu_{\\text{Teen-onset}}\n\\]\nThe alternative hypothesis is either that the populations means are different, or a specification of how they are different. Since the researchers posited that persistent marijuana users that became dependent as teens would have a bigger decline (more negative values, on average) in IQ than those who became dependent as adults, the alternative hypothesis would be:\n\\[\nH_A: \\mu_{\\text{Adult-onset}} &gt; \\mu_{\\text{Teen-onset}}\n\\]\n\nThe order of the two groups in the hypotheses does not matter. For example, we could also have written the hypotheses so that the mean for teen-onset was written before the mean for adult-onset:\n\\[\n\\begin{split}\n&H_0: \\mu_{\\text{Teen-onset}} = \\mu_{\\text{Adult-onset}} \\\\\n&H_A: \\mu_{\\text{Teen-onset}} &lt; \\mu_{\\text{Adult-onset}}\n\\end{split}\n\\]\nHowever, when we go to use the t_test() function, the alternative hypothesis we specify will correspond to an alphabetical ordering of the groups based on their values in the attribute. In the cannabis_dep attribute, the two values are Teen and Adult. Since Adult comes alphabetically before Teen, if we specify alternative=\"less\" we would be testing Adult \\(&lt;\\) Teen. Because of this, it is a good idea to put the means in alphabetical order when writing the hypotheses so that you can correctly specify the alternative hypothesis in t_test().\n\nWe can also represent the hypotheses as a difference between the two means. For example, the null hypothesis posits the two means are equal. Mathematically this can be also expressed as the difference between the two means equalling zero:\n\\[\nH_0: \\mu_{\\text{Adult-onset}} - \\mu_{\\text{Teen-onset}} = 0\n\\] In a similar fashion the alternative hypothesis could be expressed as:\n\\[\nH_A: \\mu_{\\text{Adult-onset}} - \\mu_{\\text{Teen-onset}} &gt; 0\n\\] :::fyi The output from t_result() will present the hypotheses as a difference between means. :::"
  },
  {
    "objectID": "05-01-two-sample-t-test.html#visualizing-and-numerically-describing-sample-differences",
    "href": "05-01-two-sample-t-test.html#visualizing-and-numerically-describing-sample-differences",
    "title": "11¬† Two-Sample t-Test",
    "section": "11.3 Visualizing and Numerically Describing Sample Differences",
    "text": "11.3 Visualizing and Numerically Describing Sample Differences\nAs with every analysis, we begin by visualizing and describing the sample data. Here we will create density plots of the distribution of changes in IQ scores for both groups. To do this, since the IQ chnage scores are all in a single column, we have to change the formula with the tilde we use in the gf_ and df_stats() functions so that it will plot those for each group separately. The formula will now look like:\n\\[\n\\mathtt{\\sim y ~|~ group}\n\\]\nwhere y is the attribute name that you want to plot, and group is the name of the attribute that has the groups in it. For example, since we want to plot the IQ change scores for the two groups we would use the formula:\n\\[\n\\mathtt{\\sim iq\\_change ~|~ cannabis\\_dep}\n\\]\n\nReading this syntax, we would say: ‚Äúmodel the attribute iq_change but separate (or condition) this by cannabis_dep‚Äù.\n\nThe syntax for creating density plots and computing numerical summaries is shown below.\n\n# Create histogram\ngf_density(\n  ~ iq_change | cannabis_dep, data = cannabis,\n  fill = \"skyblue\", \n  xlab = \"Change in IQ Score\",\n  ylab = \"Density\"\n  )\n\n# Compute numerical summaries\ndf_stats(~ iq_change | cannabis_dep, data = cannabis)\n\n\n\n\n\n\nFigure¬†11.1: Density plots of IQ score changes for participants who became persistent marijuana users as teens and those who became persistent marijuana users as adults.\n\n\n\n\n\n\nFigure¬†11.2: Density plots of IQ score changes for participants who became persistent marijuana users as teens and those who became persistent marijuana users as adults.\n\n\n\n\nThe density plots suggests that the distribution of change in IQ scores is not symmetric for either group. The distribution for participants who became persistent marijuana users as teens is also potentially bimodal. The average change in IQ score for participants who became persistent marijuana users as adults was \\(-2.07\\) points indicating that the average IQ score decreased by about two points throughout the duration of the study. In contrast, the average change in IQ score for participants who became persistent marijuana users as teens was \\(-8.26\\) points indicating that the average IQ score decreased by about eight points throughout the duration of the study. The was variation in IQ change in both groups, with both groups having a standard deviation near seven.\nNext, we carry out a two-sample t-test. The arguments we include are:\n\nA formula of ~y|group indicating the attribute to compare, and the groups that are being compared;\ndata= indicating the data frame the attributes are in;\nmu= to specify the value of the difference in population means specified in the null hypothesis;\nalternative= is one of two.sided, less, or greater that corresponds to the alternative hypothesis; and\nvar.equal=TRUE which ensures that you use the correct version of the two-sample t-test\n\nWe can assign the output of the t_test() function into an object, and use the t_results() and plot_t_dist() functions to examine the output of the two-sample t-test and view the t-distribution and shaded p-value, respectively.\n\n# One-sample t-test\nmy_t &lt;- t_test(\n  ~ iq_change | cannabis_dep, \n  data = cannabis, \n  mu = 0, \n  alternative = \"greater\", \n  var.equal = TRUE\n  )\n\n# View t-test results\nt_results(my_t)\n\n\n--------------------------------------------------\n Two Sample t-test\n--------------------------------------------------\n\nH[0]: mu_[Adult] = mu_[Teen]\nH[A]: mu_[Adult] &gt; mu_[Teen]\n\nt(35) = 2.474706\np = 0.009162964\n\n--------------------------------------------------\n\n\n\nWhen you get the results from the t-test, be sure it says ‚ÄúTwo Sample t-test‚Äù. If it says ‚ÄúWelch Two Sample t-test‚Äù you forgot to set the argument var.equal=TRUE.\n\n\n# View t-distribution\nplot_t_dist(my_t)\n\n\n\n\nFigure¬†11.3: Density plot of the t-distribution of the sample means based on the thought experiment underlying a hypothesis test assuming that the difference in mean IQ score change between participants who became persistent marijuana users as adults and those that became persistent marijuana users as teens is 0 (t value of 0). The red vertical line represents the observed t-value of 2.47. The shaded area under the curve to the right of 2.47 shows the associated p-value of .009 that corresponds to the alternative hypothesis that \\(\\mu&gt;0\\).\n\n\n\n\nBased on the p-value of .009, we would reject the null hypothesis. The evidence suggests that the difference in average IQ score change is greater for participants who became persistent marijuana users as adults than those that became persistent marijuana users as teens. This indicates that marijuana may have more of an adverse effect on IQ for people that became persistent marijuana users as teens."
  },
  {
    "objectID": "05-01-two-sample-t-test.html#computing-the-observed-t-value-and-degrees-of-freedom",
    "href": "05-01-two-sample-t-test.html#computing-the-observed-t-value-and-degrees-of-freedom",
    "title": "11¬† Two-Sample t-Test",
    "section": "11.4 Computing the Observed t-Value and Degrees-of-Freedom",
    "text": "11.4 Computing the Observed t-Value and Degrees-of-Freedom\nThe observed t-value is computed as part of using the t_test() function, but examining the mathematical formula may give us insight into what this t-statistic is measuring. In a two-sample t-test the t-value is computed as:\n\\[\nt = \\frac{(\\bar{y}_{1} - \\bar{y}_{2}) - \\mu_{\\text{Diff.}}}{SE_{\\text{Mean Diff.}}}\n\\]\nwhere,\n\\(\\bar{y}_1\\) is the sample mean for Group 1, \\(\\bar{y}_2\\) is the sample mean for Group 2, \\(\\mu_{\\text{Diff.}}\\) is the mean difference specified in the null hypothesis, and \\(SE_{\\text{Mean Diff.}}\\) is the standard error for the sample mean difference, which is computed as:\n\\[\nSE_{\\text{Mean Diff.}} = \\sqrt{\\frac{SD_1^2}{n_1} + \\frac{SD_2^2}{n_2}}\n\\]\nAnd \\(SD_1\\) and \\(SD_1\\) are the sample standard deviations for Groups 1 and 2, respectively, and \\(n_1\\) and \\(n_2\\) are the sample sizes for those groups.\nLooking at the numerator of the t-value, we are measuring how far the sample mean difference is from the hypothesized mean difference. Then, we are dividing by the SE, which changes the scale to standard error units. So our observed t-value is measuring the discrepancy between the sample and hypothesized mean difference in standard error units. In our example,\n\\[\n\\begin{split}\nt &= \\frac{(-2.07 - -8.26) - 0}{\\sqrt{\\frac{7.77^2}{14} + \\frac{7.14^2}{22}}} \\\\[2ex]\n&= \\frac{6.19}{2.55} \\\\[2ex]\n&= 2.47\n\\end{split}\n\\]\nThat is our observed mean difference of 6.19 is 2.47 standard errors from the hypothesized mean difference of 0. We then evaluate this t-value in a t-distribution with \\(n1 + n_2 -2\\) degrees-of-freedom. In our case the t-value of 2.47 is evaluated in a t-distribution with 35 df. This is the same thing we got in the output of t_results().\nSince the alternative hypothesis was \\(\\mu_{\\text{Adult-onset}} - \\mu_{\\text{Teen-onset}} &gt; 0\\), to find the p-value, we would find the area under the t-distribution that is greater than or equal to the observed t-value of 2.47. This area is shaded in the output of plot_t_dist() and corresponds to .009 of the entire area under the curve."
  },
  {
    "objectID": "05-01-two-sample-t-test.html#case-study-2-bachelor-and-bachelorette-ages",
    "href": "05-01-two-sample-t-test.html#case-study-2-bachelor-and-bachelorette-ages",
    "title": "11¬† Two-Sample t-Test",
    "section": "11.5 Case Study 2: Bachelor and Bachelorette Ages",
    "text": "11.5 Case Study 2: Bachelor and Bachelorette Ages\nResearch suggests that men emotionally mature years later than women. This is because the human brain develops slower in men than in women, on average. In the reality television series The Bachelor and The Bachelorette, a single person is introduced to a pool of potential romantic interests. Throughout the show, they get to know these people by going on dates with them and eliminating those they are not interested in. At the end of the show, they are supposed to select one person that will become their fianc√©. If there is a maturity gap between men and women, do we see age differences between those people selected to be the bachelor and those selected to be the bachelorette?\nWe will use the data in bachelor.csv to evaluate the following hypotheses.\n\\[\n\\begin{split}\n&H_0: \\mu_{\\text{Bachelor}} = \\mu_{\\text{Bachelorette}} \\\\\n&H_A: \\mu_{\\text{Bachelor}} &gt; \\mu_{\\text{Bachelorette}}\n\\end{split}\n\\]\n:::exercises #### Your Turn {.unnumbered}\nWrite the hypotheses as a difference in the average ages of the Bachelor and Bachelorette contestants.\n\nShow/Hide Solution\n\n\n\\[\n\\begin{split}\n&H_0: \\mu_{\\text{Bachelor}} - \\mu_{\\text{Bachelorette}} = 0 \\\\\n&H_A: \\mu_{\\text{Bachelor}} - \\mu_{\\text{Bachelorette}} &lt; 0\n\\end{split}\n\\]\n\n\n# Import data\nbachelor &lt;- read_csv(\"https://raw.githubusercontent.com/zief0002/epsy-5261/main/data/bachelor.csv\")\n\n# View data\nbachelor\n\n\n\n  \n\n\n\nBelow we create the distribution of ages for the bachelors and bachelorettes. Both distributions are right-skewed and have a range between about 25 and 40. The average age for the bachelor contestants is 30.7 while that for the Bachelorette contestants is slightly younger at 28.2. The SD for these distributions is slightly higher for the Bachelor contestants (3.80) than for the Bachelorette contestants (3.04) indicating a little more variation in the bachelors‚Äô ages.\n\n# Create histogram\ngf_histogram(\n  ~age | show, data = bachelor,\n  binwidth = 2,\n  color = \"black\", \n  fill = \"skyblue\",\n  xlab = \"Age\",\n  ylab = \"Count\"\n  )\n\n# Compute numerical summaries\ndf_stats(~age | show, data = bachelor)\n\n\n\n\n\n\nFigure¬†11.4: Histograms of ages for the 27 Bachelor and 23 Bachelorette contestants.\n\n\n\n\n\n\nFigure¬†11.5: Histograms of ages for the 27 Bachelor and 23 Bachelorette contestants.\n\n\n\n\nWhile the sample data suggests that the average age for Bachelor contestants is higher than the average age of Bachelorette contestants, is this difference only due to sampling variation? To answer this, we will carry out a two-sample t-test to evaluate the sample differences in light of the expected sampling variation.\n\n# One-sample t-test\nmy_t &lt;- t_test(\n  ~age | show, data = bachelor, \n  mu = 0, \n  alternative = \"greater\",\n  var.equal = TRUE\n  )\n\n# View t-test results\nt_results(my_t)\n\n\n--------------------------------------------------\n Two Sample t-test\n--------------------------------------------------\n\nH[0]: mu_[Bachelor] = mu_[Bachelorette]\nH[A]: mu_[Bachelor] &gt; mu_[Bachelorette]\n\nt(48) = 2.567151\np = 0.006712898\n\n--------------------------------------------------\n\n\n\nYour Turn\nUse the results of the two-sample t-test to sketch the t-distribution with 48 df, add a vertical line at the observed t-value, and shade the area under the curve corresponding to the p-value.\n\nShow/Hide Solution\n\n\nYour sketch should look something like the following.\n\n# View t-distribution\nplot_t_dist(my_t)\n\n\n\n\nFigure¬†11.6: Density plot of the t-distribution of the difference in sample means based on the thought experiment underlying a hypothesis test assuming that the difference in mean age between Bachelor and Bachelorette contestants is 0 (t value of 0). The red vertical line represents the observed t-value of 2.57. The shaded area under the curve to the right of 2.57 shows the associated p-value of .007 that corresponds to the alternative hypothesis that the average age for the Bachelor contestants is greater than the average age of the Bachelorette contestants.\n\n\n\n\n\nBased on the results of the t-test, should we reject or fail to reject the null hypothesis? Explain why and also what this implies given the context of the case study.\n\nShow/Hide Solution\n\n\nThe p-value of .007 suggest we should reject the null hypothesis that the average age of Bachelor and Bachelorette contestants are the same (or that the difference between these averages is 0) since it is less than .05. This suggests that the average age for Bachelor contestants is higher than the average age for Bachelorette contestants."
  },
  {
    "objectID": "05-01-two-sample-t-test.html#references",
    "href": "05-01-two-sample-t-test.html#references",
    "title": "11¬† Two-Sample t-Test",
    "section": "11.6 References",
    "text": "11.6 References\n\n\n\n\nMeier, M. H., Caspi, A., Ambler, A., Harrington, H., Houts, R., Keefe, R. S. E., McDonald, K., Ward, A., Poulton, R., & Moffitt, T. E. (2012). Persistent cannabis users show neuropsychological decline from childhood to midlife. Proc Natl Acad Sci U S A, 109(40), E2657‚Äì64. https://doi.org/10.1073/pnas.1206820109\n\n\nVolkow, N. D., Baler, R. D., Compton, W. M., & Weiss, S. R. B. (2014). Adverse health effects of marijuana use. N Engl J Med, 370(23), 2219‚Äì2227. https://doi.org/10.1056/NEJMra1402309"
  }
]