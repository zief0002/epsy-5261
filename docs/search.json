[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Data Analysis and Randomization Methods",
    "section": "",
    "text": "Foreword\nThis work in progress is ultimately going to be the primarily textbook resource for EPSY 5261 students. (Note: If you want to contribute to this, create a Pull Request or send me an email.) Also, feel free to offer criticism, suggestion, and feedback. You can either open an issue on the book‚Äôs github page or send me an email directly."
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "An Introduction to Data Analysis and Randomization Methods",
    "section": "Colophon",
    "text": "Colophon\nArtwork by @allison_horst\nIcon and note ideas and prototypes by Desir√©e De Leon.\nThe book is typeset using Crimson Text for the body font, Raleway for the headings and Sue Ellen Francisco for the title. The color palette was generated using coolors.co.\nStatistical Computing\n\nLaptop icon made by Tomas Knop from www.flaticon.com\nDirectory icon made by Darius Dan from www.flaticon.com\nBrain icon made by Aranagraphics from www.flaticon.com\nInternet icon made by Freepik from www.flaticon.com"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "An Introduction to Data Analysis and Randomization Methods",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "01-statistical-computation.html",
    "href": "01-statistical-computation.html",
    "title": "Statistical Computation",
    "section": "",
    "text": "The first set of tools we will discuss will be related to statistical computation. Although there are many computational tools for statistical analysis, the first tools we will add to your computational toolkit is R. R is a free software environment for statistical computing and graphics. It can be installed on a variety of operating systems, including the MacOS, Windows, and UNIX platforms. To really make use of the computational power of R, we are also going to introduce you to RStudio, an open-source front-end1 to R.\nThe initial chapters of this document will address:\n\nInstalling R and RStudio;\nGetting started with R‚Äôs computational syntax;\nWrangling data using functions from the dplyr package; and\nVisualizing data using functions from the ggplot2 package.\n\n\n\n\n\n\n\nSpecifically, RStudio is branded as an ‚Äúintegrated development environment (IDE) [that] includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.‚Äù‚Ü©Ô∏é"
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#installing-r",
    "href": "01-01-r-and-rstudio-installation.html#installing-r",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.1 Installing R",
    "text": "1.1 Installing R\nTo install R, navigate your web browser to:\n\nhttps://www.r-project.org/\n\nThen,\n\nClick the CRAN link under Download on the left-hand side of the page.\nSelect a mirror site. These should all be the same, but I tend to choose the Iowa State University link under USA.1\nIn the Download and Install R box, choose the binary that matches the operating system (OS) for your computer.\n\nThis is where the installation directions diverge depending on your OS.\nMac Instructions\nSo long as you are running MacOS 10.13 or higher just click the first link for the PKG, which will download the installer for the most current version of R (4.1.1 as of August 16, 2021). Once the download completes, open the installer and follow the directions to install R on your computer.\nIf you are running an older version of MacOS, you will have to install an older version of R. You can find these links under the Binaries for legacy OS X systems heading further down the install page. Click the appropriate PKG link for R your version of MacOS. Once the download completes, open the installer and follow the directions to install R on your computer.\nIf you are unsure which version of the MacOS is running on your computer, select About this Mac from the Apple menu in your toolbar.\nWindows Instructions\nClick the link that says Install R for the first time (or click base; they go to the same place). Then click the Download R 4.1.1 for Windows link, which will download the installer for the most current version of R (4.0.2 as of July 24, 2020). Once the download completes, open the installer and follow the directions to install R on your computer.\nLinux Instructions\nIf you are running Linux, you should know how to install things on your computer. üòÑ"
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#installing-rstudio-desktop",
    "href": "01-01-r-and-rstudio-installation.html#installing-rstudio-desktop",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.2 Installing RStudio Desktop",
    "text": "1.2 Installing RStudio Desktop\nAfter you have installed R, you next need to install RStudio Desktop. To do this, navigate your web browser to:\n\nhttps://rstudio.com/products/rstudio/download/\n\nThen,\n\nSelect the blue Download button under the free, open-source version of RStudio Desktop.\nSelect the installer associated with your computer‚Äôs OS.\nOnce the download completes, open the installer and follow the directions to install RStudio Desktop on your computer."
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#checking-that-things-worked",
    "href": "01-01-r-and-rstudio-installation.html#checking-that-things-worked",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.3 Checking that Things Worked",
    "text": "1.3 Checking that Things Worked\nFrom your Applications or Programs folder, open RStudio. If you have successfully downloaded both programs, this should open the application and you should see a message indicating that you are using ‚ÄúR version 4.1.1‚Äù (or whichever version of R you installed) in the console pane.\n\n\n\n\n\nOnce you open RStudio, you should see a message indicating that you are using R version 4.1.1 (or whichever version of R you installed) in the console pane. Here the console pane is on the left-side, but it may be in a different location for you. Your RStudio may also have a white background rather than the black background seen here."
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#customizing-rstudio",
    "href": "01-01-r-and-rstudio-installation.html#customizing-rstudio",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.4 Customizing RStudio",
    "text": "1.4 Customizing RStudio\nWhile the information in this section is not crucial for making things work, it is useful to get RStudio looking good and setting some default settings. Open the Tools > Options menu (Windows) or RStudio > Preferences (Mac).\n\n\n\n\n\nThe RStudio options/preferences menu has many settings to customize RStudio.\n\n\n\n\n\nIn the General > Basic settings, change the option on Save workspace to .Rdata on exit to be ‚ÄúNever‚Äù. Click the ‚ÄúApply‚Äù button.\nIn the Appearance settings, customize the look of RStudio to something aesthetically appealing to you. When you are finished, click the ‚ÄúApply‚Äù button.\nThere are also options you can set in the Accessibility settings if you use a screen reader. If you change anything, don‚Äôt forget to click the ‚ÄúApply‚Äù button.\n\nWhen you are finished customizing RStudio, click the ‚ÄúOK‚Äù button."
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#install-rtoolscommand-line-tools",
    "href": "01-01-r-and-rstudio-installation.html#install-rtoolscommand-line-tools",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.5 Install Rtools/Command Line Tools",
    "text": "1.5 Install Rtools/Command Line Tools\nYou may need to install some additional functionality to your system in order to get certain packages to install or load properly. On a Windows machine, you might need to install Rtools. Mac users might need to add the Command Line Tools. These tools also allow you to write and compile your own R packages. RStudio has well written instructions for adding these tools at: https://support.rstudio.com/hc/en-us/articles/200486498-Package-Development-Prerequisites."
  },
  {
    "objectID": "02-exploring-and-describing-data.html",
    "href": "02-exploring-and-describing-data.html",
    "title": "Exploring and Describing Data",
    "section": "",
    "text": "Data are at the core of all statistical analyses. In this section you will learn about:\n\nThe tabular data structure;\nUnderstanding your data including:\n\nClassifying attributes;\nWhether your data constitute the population of cases or a sample of cases;\nSampling methods for producing reasonable estimates\n\nVisualizing distributions of data; and\nNumerically summarizing distrubtions of data."
  },
  {
    "objectID": "02-01-data.html#classifying-attributes",
    "href": "02-01-data.html#classifying-attributes",
    "title": "2¬† Data",
    "section": "2.1 Classifying Attributes",
    "text": "2.1 Classifying Attributes\nOur ultimate goal is often to analyze the data we have to learn from it. For example, in our NYT Best Seller data, we may be interested in the proportion of authors that identify as female. Or, we may want to. know how many weeks a book stays on the Best Sellers list. The type of analyses we can do, however, depend on the type of attributes we have.\nWe typically classify attributes as either categorical attributes or quantitative attributes. These classifications are based on the type of information (data) in the attribute. A categorical attribute has values that represent categorical (or qualitative) differences between the cases, whereas a quantitative attribute represents numerical (or quantitative) differences between cases. For example, in the NYT Best Seller data, title and author are categorical variables, whereas year, and total number of weeks the book was on the NYT Best Sellers list are quantitative attributes.\nTypically attributes that have numerical values are quantitative, but not always. In our data, consider the attribute that indicates whether the author identifies as a female. Although the values in the data are numeric, these numbers actually represent different categories: 0 = no (not female) and 1 = yes (female). Therefore, this attribute is actually a categorical attribute, not a quantitative attribute.\nOne check of whether anattribute is actually quantitative is whether numeric computations, such as finding an average of the attribute, can be carried out and the result makes conceptual sense. For example, we cannot compute the mean author value (it is thus a categorical attribute). If we compute the mean of the female attribute we get a result, but it does not indicate anything about the gender identity of a NYT best selling author. The mean does not make conceptual sense and thus we classify female as a categorical attribute.\n\nYour Turn\nClassify the best_rank attribute (the book‚Äôs highest rank while it was on the NYT Best Sellers list) as either categorical or quantitative. Explain.\n\nShow/Hide Solution\n\n\nThe attribute best_rank is a quantitative attribute. The data in this attribute are numeric values, and it makes conceptual sense to compute summaries such as the mean for this attribute.\n\n\n\n\n2.1.1 Further Classifications of Attributes\nNominal\nOrdinal\nInterval\nRatio"
  },
  {
    "objectID": "02-01-data.html#how-were-the-data-generated",
    "href": "02-01-data.html#how-were-the-data-generated",
    "title": "2¬† Data",
    "section": "2.2 How Were the Data Generated?",
    "text": "2.2 How Were the Data Generated?\nAnother question that has direct implications on the methods used in data analysis is: How were the data generated or collected? Were they collected from a survey? Were they mined from the web? Were they generated as part of an experimental study? Knowing the answer to these questions also is important for the degree to which we can draw conclusions from the analysis.\nUnderstanding how the data were generated allows us to determine whether the data we have constitute a sample of cases or the entire population of cases we are interested in learning about. Importantly, whether you have a sample or the entire population depends on how you define the cases/observations you are interested in drawing conclusions about.\n\nA population includes all cases/observations of interest, whereas a. sample includes a subset of cases from the population.\n\nFor example, consider a child psychologist who wants to draw conclusions about all students at a particular school in Minnesota. To do this, she collects data from every student in that school. Since her data includes every case (student) she is interest in drawing conclusions for, her data would be a population, Now consider a second child psychologist who is interested in drawing conclusions about all students in Minnesota. This psychologist also collects data from every student in the same school as the first psychologist. This second psychologist‚Äôs data would be considered a sample since the cases they included in their data are only a subset of the cases they want to draw conclusions about.\n\nYour Turn\nIs the New York Time best sellers data a population or a sample? Explain.\n\nShow/Hide Solution\n\n\nThe New York Time best sellers data is a sample since it is only a subset of all the New York Times best selling books.\n\n\n\n\n2.2.1 Drawing Conclusions from a Sample\nIn practice, we rarely have data collected from an entire population, but we still want to use the data we have in our sample to draw conclusions about that population. Drawing conclusions about an entire population when you only have data from a subset cases is what statisticians call statistical inference.\n\n\n\n\n\nFigure¬†2.1: A sample of data is drawn from the population. Information from the sample is then analyzed and used to make a statistical inference about the population.\n\n\n\n\nThis can be a very tricky thing to do since the sample does not give us complete information about the population. As an example, consider if you wanted to figure out the average monthly living expenses for all graduate students at the University of Minnesota. To do this you collect data on the monthly living expenses for the students in your EPSY 5261 class and compute the average monthly living expense based on the data you collected and use that value as a guess for the average monthly living expenses for all graduate students at the University of Minnesota. (Note that the cases in your data (students in your EPSY 5261 class) are a subset of the population you want to draw conclusions about (all graduate students at the University of Minnesota) and thus are a sample.)\n\nSummaries computed from the population are referred to as parameters and summaries computed from a sample are referred to as statistics.\n\nIn statistical inference the statistics we compute from a sample are estimates for the population parameters that we are interested in. The word ‚Äúestimate‚Äù may have clued you in that the value of a statistic is generally not equal to the value of the parameter. In our example, the average monthly living expenses we computed based on your sample of cases is probably not the same as the average monthly living expenses for all graduate students at the University of Minnesota. This is because our sample only includes data for some (not all) of the cases.\nWe don‚Äôt expect the value of the statistic to be the same as that for the parameter we are trying to estimate, but a key question is: Is the value of the statistic a reasonable estimate of the parameter? The answer to this question can sometimes be difficult to answer. What do we mean by reasonable? In statistical analysis, there are a few ways that we consider reasonableness of an estimate. We will explore these below.\n\n\n2.2.1.1 Sampling Error: Quantifying the Amount of Uncertainty in our Sample Estimate\nOne way we consider whether an answer is reasonable is how much uncertainty we have in the estimate from our sample. Imagine if you repeated the study, but this time, you collected data on the monthly living expenses in a different section of EPSY 5261. The average computed from these data would likely be different from the average you computed from your section of EPSY 5261, and therefore your guess for the average monthly living expenses for all graduate students at the University of Minnesota would be different. This is because you would have different cases in your sample.\n\nWhen using a sample to infer about a population, our guesses or estimates vary depending on the cases in our sample. This means that when we make inferences there is always some degree of uncertainty in our estimates.\n\nThe idea that estimates from samples vary depending on the cases in your sample is well known and is referred to as sampling error. In carrying out statistical inference, we not only acknowledge that we have uncertainty in our guess from the sample data, but we also try and quantify how much uncertainty there is in that estimate. For example, do we think that the average monthly living expenses for all graduate students at the University of Minnesota is within a few dollars of our sample estimate? Or do we think that it is within a few hundred dollars of our sample estimate? By providing this estimate of the uncertainty, it lets other people know ‚Äúhow reasonable‚Äù our guess might be.\n\n\n\n\n\nFigure¬†2.2: Estimates for the mean living expense for all graduate students at the University of Minnesota will vary from sample to sample because of sampling error. In statistical inference this is expected and quantifying the amount of sampling error gives us an indication of how much uncertainty we have in our estimate.\n\n\n\n\n\n\n\n2.2.1.2 Sampling Bias: Does the Sample Represent the Population?\nA second way we consider whether an answer is reasonable is to consider whether our sample of cases is representative of the population as a whole. In our example, we are asking the question of whether the students in your section of EPSY 5261 are representative of all graduate students at the University of Minnesota when it comes to living expenses. This is a really difficult question to answer, but generally (unless you have selected your sample randomly from the population), your sample is not representative. The key here is that the sampling method (how you chose your cases) matters!\n\nWhen a sample is not randomly selected from the population we say that the sampling method is biased.\n\nA biased sampling method leads to systematically wrong answers. For example, again say you were interested in determining the average monthly living expenses for all graduate students at the University of Minnesota. This time, your sampling method is to collect data about the monthly living expenses from all the graduate students who live in a particular apartment building in Downtown Minneapolis. Would these students‚Äô living expenses be representative of all graduate students at the University of Minnesota?\nAgain, probably not. The living expenses in Downtown Minneapolis are different (generally higher) than the living expenses of students who live in Dinkytown or one of the suburbs. Because the cases in your sample all come from the same apartment building in Downtown Minneapolis, their average monthly living expense will be systematically higher than the average monthly living expenses for all graduate students at the University of Minnesota.\nWhat about our original sampling method of collecting data from each of the graduate students in your EPSY 5261 section? While these students might live in different areas, and seem more representative, this sampling method is likely still biased. Even if we have a hard time identifying how, the estimate for the average monthly living expenses based on students in EPSY 5261 is likely systematically different than the average monthly living expenses for all graduate students at the University of Minnesota. (It may be systematically too low, or too high.)\n\nThe only sampling method that is guaranteed to be unbiased (and therefore representative) is to select your sample randomly from the population.\n\n\n\n\n\n2.2.2 Random Sampling\nThere are many methods for randomly selecting a sample from the population. The simplest method that incorporates randomness into the sampling process is Simple Random Sampling. In simple random sampling each case in the population has an equal probability of being selected into the sample.1\n\nIn the discipline of statistics, there are words that we use that have very different meanings from their use in colloquial English. ‚ÄúRandom‚Äù is one of those words. In our everyday language ‚Äúrandom‚Äù might mean happenstance or unexpected. For example: It was so random that I saw Ferris Bueller at the 31 Flavors last night. In statistics, ‚Äúrandom‚Äù does not mean happenstance at all. Random sampling is quite formal in ensuring that cases have a specified probability of being selected into the sample.\n\nOne of the most compelling and useful results in statistics is that a simple random sample is representative of the population, and moreover that even small samples that are selected with this method can be representative of very large populations. This is powerful!\nBut, it can sometimes be very difficult to draw a simple random sample in practice. For one thing, it requires that you have a list of all the cases in the population (called a sampling frame). This allows you to make sure that everyone in the population has the same probability of being selected. While it might be possible to obtain a list of all graduate students enrolled at the University of Minnesota, it is another thing to obtain a list of all people living in Minnesota. Or even a list of people living in Dinkytown. Depending on your population of interest you may not be able to get a simple random sample.2\n\nYour Turn\nWhat is the sampling method for the New York Time best sellers data. Based on this method, are the estimates of the population parameters we compute from these data going to be biased or unbiased?\n\nShow/Hide Solution\n\n\nThe New York Time best sellers data was sampled using simple random sampling. Because the sampling method employed randomness, the estimates we compute from the data are unbiased estimates of the population parameters."
  },
  {
    "objectID": "02-01-data.html#summary",
    "href": "02-01-data.html#summary",
    "title": "2¬† Data",
    "section": "2.3 Summary",
    "text": "2.3 Summary\nEvery time you encounter data, you should identify the cases and attributes in the data. Understanding the cases, especially in relation to the cases you want to draw conclusions about, helps you identify whether you have a sample, or the entire population. Classifying the attributes helps you think about the type of analysis you can undertake. If your data are a sample (rather than a population), you also need to ask how they were collected. Were they collected using randomness in the sampling method? Or is the sampling method used to collect the data biased?"
  },
  {
    "objectID": "02-01-data.html#references",
    "href": "02-01-data.html#references",
    "title": "2¬† Data",
    "section": "2.4 References",
    "text": "2.4 References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmerican Statistical Association. (2023). ASA newsroom. Website. https://www.amstat.org/asa-newsroom"
  },
  {
    "objectID": "02-02-visualization.html",
    "href": "02-02-visualization.html",
    "title": "3¬† Visualization",
    "section": "",
    "text": "4 College Scorecard Data\nThe U.S. Department of Education publishes data on institutions of higher education in their College Scorecard (https://collegescorecard.ed.gov/) to facilitate transparency and provide information for interested stakeholders (e.g., parents, students, educators). A subset of this data is provided in the file College-scorecard-clean.csv. To illustrate some of the common methods statisticians use to visualize data, we will examine admissions rates for 2,019 institutions of higher education.\nBefore we begin the analysis, we will load two packages, the tidyverse package and the ggformula package. These packages include many useful functions that we will use in this chapter.\nThere are many functions in R to import data. We will use the function read_csv() since the data file we are importing (College-scorecard-clean.csv) is a comma separated value (CSV) file..2 CSV files are a common format for storing data. Since they are encoded as text files they generally do not take up a lot of space nor computer memory. They get their name from the fact that in the text file, each data attribute (i.e.¬†column in the data) is separated by a comma within each row. Each row represents a unique case or observation from the data. The syntax to import the college scorecard data is as follows:\nIn this syntax we have passed two arguments to the read_csv() function. The first argument, file=, indicates the path to the data file. The data file here is stored on GitHub, so the path is specified as a URL. The second argument, guess_max=, helps ensure that the data are read in appropriately. This argument will be described in more detail later.\nThe syntax to the left of the read_csv() function, namely colleges <-, takes the output of the function and stores it, or in the language of R, assigns it to an object named colleges. In data analysis, it is often useful to use results in later computations, so rather than continually re-running syntax to obtain these results, we can instead store those results in an object and then compute on the object. Here for example, we would like to use the data that was read by the read_csv() function to explore it. When we want to assign computational results to an object, we use the assignment operator, <- . (Note that the assignment operator looks like a left-pointing arrow; it is taking the computational result produced on the right side and storing it in the object to the left side.)"
  },
  {
    "objectID": "02-02-visualization.html#view-the-data",
    "href": "02-02-visualization.html#view-the-data",
    "title": "3¬† Visualization",
    "section": "4.1 View the Data",
    "text": "4.1 View the Data\nOnce we have imported and assigned the data to an object, it is quite useful to ensure that it was read in appropriately. The head() function will give us a quick snapshot of the data by printing the first six rows of data.\n\nhead(colleges)\n\n\n\n  \n\n\n\nWe can also include an interactive version for viewing the book on the web using the DT package.\n\nDT::datatable(colleges)"
  },
  {
    "objectID": "02-02-visualization.html#exploring-attributes",
    "href": "02-02-visualization.html#exploring-attributes",
    "title": "3¬† Visualization",
    "section": "4.2 Exploring Attributes",
    "text": "4.2 Exploring Attributes\nData scientists and statisticians often start analyses by exploring attributes (i.e., variables) that are of interest to them. For example, suppose we are interested in exploring the admission rates of the institutions in the college scorecard data to determine how selective the different institutions are. We will begin our exploration of admission rates by examining different visualizations of the admissions rate attribute. There is not one perfect visualization for exploring the data. Each visualization has pros and cons; it may highlight some features of the attribute and mask others. It is often necessary to look at many different visualizations of the data in the exploratory phase.\nOne of the primary goals of any data visualization, especially those in this chapter, are to summarize (think, simplify) the data so that it can be more easily processed to understand key components of the attribute being explored. To be more explicit, it would be possible to explore all 2019 of the raw data to see the exact admission rate for each institution. However, if the goal is to know overall trends for the admission rates of institutions, knowing the exact values for each institution from the table would be too unwieldy. Instead, the goal of the data visualization to simplify the attribute to understand better the key components of the admission rate attribute. This is a trade-off, as there is a loss of information, but this loss of information is useful in this context as it allows for the summarization of the attribute.\n\n\n\n4.2.1 Histograms\nThe first visualization we will examine is a histogram. We can create a histogram of the admission rates using the gf_histogram() function. (This function is part of the ggformula package which needs to be loaded prior to using the gf_histogram() function.) This function requires two arguments. The first argument is a formula that identifies the variables to be plotted and the second argument, data =, specifies the data object that was assigned on data import. For example, earlier we used the read_csv() function to import the college scorecard data and we assigned this to the name, colleges. The syntax used to create a histogram of the admission rates is:\n\ngf_histogram(~ adm_rate, data = colleges)\n\n\n\n\n\n\nFigure¬†4.1: Histogram of college admission rates.\n\n\n\nThe formula we provide in the first argument is based on the following general structure:\n~ attribute name\nwhere the attribute name identified to the right of the ~ is the exact name of one of the columns in the colleges data object.\n\n\n\n4.2.2 Interpretting Histograms\nHistograms are created by collapsing the data into bins and then counting the number of observations that fall into each bin. To show this more clearly in the figure created previously, we can color the bin lines to highlight the different bins. To do this we include an additional argument, color =, in the gf_histogram() function. We can also set the color for the bins themselves using the fill = argument. Here we color the bin lines black and set the bin color to yellow.3\n\ngf_histogram(~ adm_rate, data = colleges, color = 'black', fill = 'yellow')\n\n\n\n\nFigure¬†4.2: Histogram of college admission rates. Here the color of the bin lines to black and fill in the bars with yellow.\n\n\n\n\nWhen looking at a single bar, for example the bar that is at 0.50, shows the number of institutions with admissions rates between about 0.48 and 0.52. In this case, there are about 80 institutions that have admissions rates between 0.48 and 0.52 (i.e., 48% to 52% admission rates). Similar interpretations are found for all of the other bars as well.\nOne common assumption made with a histogram is for the width of the bars be the same width on the attribute of interest. For example, the single bar interpreted in the preceding paragraph, the width of the bar was about 4% on the admission rate scale. Therefore, with the assumption that all bars have the same width, this would mean that all of the 30 bars in the histogram would each range by about 4%.\nRather than focusing on any one bin, we typically want to describe the distribution as a whole. For example, it appears as though most institutions admit a high proportion of applicants since the bins to the right of 0.5 have higher counts than the bins that are below 0.5. There are, however, some institutions that are quite selective, only admitting fewer than 25% of the students who apply.\n\n\n4.2.2.1 Adjusting Number of Bins\nInterpretation of the distribution is sometimes influenced by the width or number of bins. It is often useful to change the number of bins to explore the impact this may have on your interpretation. This can be accomplished by either (1) changing the width of the bins via thebinwidth = argument in the gf_histogram() function, or (2) changing the number of bins using the bins = argument.\nThe binwidth in the histogram refers to the range or width of each bin. A larger binwidth would mean there are fewer bins as it would take fewer bins to span the entire range of the attribute of interest. In contrast, a smaller binwidth would require more bins to span the entire range of the attribute of interest.\nIn contrast, the number of bins can be specified directly, for example 10 or 20 bins. The default within the R graphics package used is 30. Within this framework, each bin will have the same width or binwidth.\nThe relationship between the number of bins and binwidth could be shown with the following equation:\n\\[\nbinwidth = \\frac{attribute\\ range}{\\#\\ of\\ bins}\n\\]\nTo be more explicit, suppose that we wanted there to be 25 bins, using algebra we could compute the new binwidth given that we know we want 25 bins and knowing the range of the original attribute. The admission rates attribute have values as small as 0 and as large as 1. Therefore, the total range would be 1 (1 - 0 = 1). The binwidth could then be computed as:\n\\[\nbindwidth = \\frac{1}{25} = .04\n\\]\nIn contrast, if we wanted to specify the binwidth instead of the number of bins, we could do a little bit of algebra in the equation above to compute the number of bins needed to span the range of the attribute given the specified binwidth. For example, if we wanted the binwidth to be .025, 2.5%, we could compute this as follows:\n\\[\n\\#\\ of\\ bins = \\frac{1}{.025} = 40\n\\]\nWe will take the approach of letting the software compute these, but the equations above shows the general process that is used by the software in selecting the binwidth.\nMore bins/smaller binwidth can give a slightly more nuanced interpretation of the attribute of interest, whereas fewer bins/large binwidth will do more summarization. Having too few bins or too many bins can make the figure more difficult to interpret by missing key features of the attribute or including too many unique features of the attribute. For this reason, it is often of interest to adjust the binwidth or number of bins to explore the impact on the interpretation.\nThe code below changes the binwidth to specify it as .01 via the binwidth = .01 argument with the figure shown in Figure¬†4.3.\n\ngf_histogram(~ adm_rate, data = colleges, color = 'black', fill = 'yellow', binwidth = .01)\n\n\n\n\nFigure¬†4.3: Histogram of college admission rates. Here the binwidth has been changed to .01.\n\n\n\n\nThe code below specifies 10 bins via the bins = 10 argument with the figure shown in Figure¬†4.4.\n\ngf_histogram(~ adm_rate, data = colleges, color = 'black', fill = 'yellow', bins = 10)\n\n\n\n\nFigure¬†4.4: Histogram of college admission rates. Here the number of bins has been changed to 10.\n\n\n\n\nIn general, our interpretation remains the same across all the different binwidth/bins combinations, namely that most institutions admit a high proportion of applicants. When we used a bin width of 0.01, however, we were able to see that several institutions admit 100% of applicants. This was obscured in the other histograms we examined. As a data scientist these institutions might warrant a more nuanced examination."
  },
  {
    "objectID": "02-02-visualization.html#plot-customization",
    "href": "02-02-visualization.html#plot-customization",
    "title": "3¬† Visualization",
    "section": "4.3 Plot Customization",
    "text": "4.3 Plot Customization\nThere are many ways to further customize the plot we produced to make it more appealing. For example, you would likely want to change the label on the x-axis from adm_rate to something more informative. Or, you may want to add a descriptive title to your plot. These customizations can be specified using the gf_labs() function. Specific examples are given below.\n\n\n4.3.1 Axes labels\nTo change the labels on the x- and y-axes, we can use the arguments x = and y = in the gf_labs() function. These arguments take the text for the label you want to add to each axis, respectively. Here we change the text on the x-axis to ‚ÄúAdmission Rate‚Äù and the text on the y-axis to ‚ÄúFrequency‚Äù. The gf_labs() function is connected to the histogram by linking the gf_histogram() and gf_labs() functions with the pipe operator (|>).4\n\ngf_histogram(~ adm_rate, data = colleges, color = 'black', fill = 'yellow', bins = 25) |>\n  gf_labs(\n    x = 'Admission Rate',\n    y = 'Frequency'\n    )\n\n\n\n\nFigure¬†4.5: Histogram of college admission rates. Here we add custom x- and y-axis labels.\n\n\n\n\n\n\n\n4.3.2 Plot title and subtitle\nWe can also add a title and subtitle to our plot. Similar to changing the axis labels, these are added using gf_labs(), but using the title = and subtitle = arguments.\n\ngf_histogram(~ adm_rate, data = colleges, color = 'black', fill = 'yellow', bins = 25) |>\n  gf_labs(\n    x = 'Admission Rate',\n    y = 'Frequency',\n    title = 'Distribution of admission rates for 2,019 institutions of higher education.',\n    subtitle = 'Data Source: U.S. Department of Education College Scorecard'\n    )\n\n\n\n\nFigure¬†4.6: Histogram of college admission rates. Here we add a title and subtitle.\n\n\n\n\nPlot titles and subtitles are helpful to used to provide context to the figure and describe the overall purpose for the figure. For example, the subtitle in Figure @ref(fig:subtitle) describes the source for the data plotted.\n\n\n\n4.3.3 Plot theme\nBy default, the plot has a grey background and white grid lines. This can be modified to using the gf_theme() function. For example, in the syntax below we change the plot theme to a white background with no grid lines using theme_classic(). Again, the gf_theme() is linked to the histogram with the pipe operator.\n\ngf_histogram(~ adm_rate, data = colleges, color = 'black', fill = 'yellow', bins = 25) |>\n  gf_labs(\n    x = 'Admission Rate',\n    y = 'Frequency',\n    title = 'Distribution of admission rates for 2,019 institutions of higher education.',\n    subtitle = 'Data Source: U.S. Department of Education College Scorecard'\n    ) |>\n  gf_theme(theme_classic())\n\n\n\n\nFigure¬†4.7: Histogram of college admission rates. Here we change the plot theme."
  },
  {
    "objectID": "02-02-visualization.html#density-plots",
    "href": "02-02-visualization.html#density-plots",
    "title": "3¬† Visualization",
    "section": "4.4 Density plots",
    "text": "4.4 Density plots\nAnother plot that is useful for exploring attributes is the density plot. This plot usually highlights similar distributional features as the histogram, but the visualization does not have the same dependency on the specification of bins. Density plots can be created with the gf_density() function which takes similar arguments as gf_histogram(), namely a formula identifying the attribute to be plotted and the data object.5 If you compare the code specified for the very first histogram, notice that only the function name changed.\n\ngf_density(~ adm_rate, data = colleges)\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\n\nFigure¬†4.8: Density plot of college admission rates.\n\n\n\n\n\n\n4.4.1 Interpreting Density Plots\nDensity plots are interpreted similarly to a histogram in that areas of the density curve that are higher indicate more data in those areas of the attribute of interest. Places where the density curve are lower indicate areas where data occur infrequently. The density metric on the y-axis is not the same as the histogram, but the relative magnitude can be interpreted similarly. That is, higher indicates more data in that region of the attribute.\nJust like the histogram, the attribute being depicted in the density curve is on the x-axis. Therefore, important features for the attribute of interest can be found by looking at the y-axis, but then the place where high or low prevalence occurs are depicted by looking back to the x-axis. For example, when looking at the density curve in Figure¬†4.9, the density curve has a peak on the y-axis density scale of just under 2.0, the peak of this density curve occurs around a 0.75 as shown on the x-axis.\nOur interpretation remains that most institutions admit a high proportion of applicants. In fact, colleges that admit around 75% of their applicants have the highest probability density, indicating this is where most of the institutions are found in the distribution. Additionally, there are just a few institutions that are have an admission rate 25% or less.\nThe axis labels, title, subtitle can be customized with gf_labs() in the same manner as with the histogram. The color = and fill = arguments in gf_density() will color the density curve and area under the density curve, respectively.\n\ngf_density(~ adm_rate, data = colleges, color = 'black', fill = 'yellow') |>\n  gf_labs(\n    x = 'Admission Rate',\n    y = 'Probability density',\n    title = 'Distribution of admission rates for 2,019 institutions of higher education.',\n    subtitle = 'Data Source: U.S. Department of Education College Scorecard'\n    )\n\n\n\n\nFigure¬†4.9: Density plot of college admission rates."
  }
]